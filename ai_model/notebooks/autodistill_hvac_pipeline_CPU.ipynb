{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production-Grade HVAC Auto-Labeling Pipeline - CPU Version\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a comprehensive, production-ready auto-labeling pipeline using **Grounded-SAM-2** from Autodistill, **optimized for CPU-only environments**. It follows official documentation best practices and is adapted for HVAC blueprint symbol detection on systems without GPU acceleration.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- \u2705 **CPU-Optimized**: Configured specifically for CPU-only inference without CUDA dependencies\n",
    "- \u2705 **Grounded-SAM-2**: Uses the latest Florence-2 + SAM 2 architecture\n",
    "- \u2705 **Memory Efficient**: Optimized batch processing and memory management for CPU\n",
    "- \u2705 **Official Best Practices**: Follows autodistill official documentation guidelines\n",
    "- \u2705 **Robust Error Handling**: Comprehensive validation and error recovery\n",
    "- \u2705 **Quality Assurance**: Built-in quality checks and visualization\n",
    "- \u2705 **Optimized Parameters**: Research-backed threshold configurations\n",
    "\n",
    "### Workflow Phases\n",
    "\n",
    "1. **Environment Setup**: Install CPU-compatible dependencies and configure paths\n",
    "2. **Configuration**: Set optimal parameters for HVAC symbol detection on CPU\n",
    "3. **Auto-Labeling**: Generate high-precision annotations with Grounded-SAM-2 (CPU mode)\n",
    "4. **Quality Review**: Visual inspection and approval gate\n",
    "5. **Model Training**: Train YOLOv8 on auto-labeled dataset (CPU)\n",
    "6. **Inference**: Test trained model on new images\n",
    "\n",
    "### CPU Performance Notes\n",
    "\n",
    "\u26a0\ufe0f **Important**: CPU processing is significantly slower than GPU:\n",
    "- Auto-labeling may take 10-20x longer per image compared to GPU\n",
    "- Model training will be substantially slower (hours vs minutes)\n",
    "- Consider processing smaller batches or fewer images for testing\n",
    "- Recommended for development, small datasets, or systems without GPU access\n",
    "\n",
    "### References\n",
    "\n",
    "- [Grounded-SAM-2 Official Docs](https://docs.autodistill.com/base_models/grounded-sam-2/)\n",
    "- [Autodistill GitHub](https://github.com/autodistill/autodistill-grounded-sam-2)\n",
    "- [Autodistill Quickstart](https://docs.autodistill.com/quickstart/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Environment Setup (CPU-Optimized)\n",
    "\n",
    "### Installation Strategy for CPU\n",
    "\n",
    "This section installs all dependencies optimized for CPU-only execution:\n",
    "- PyTorch with CPU support (no CUDA)\n",
    "- Grounded-SAM-2 (will use CPU backend automatically)\n",
    "- YOLOv8 for training target model (CPU mode)\n",
    "- Supporting libraries for visualization and dataset handling\n",
    "\n",
    "### Key Differences from GPU Version\n",
    "\n",
    "- \u274c No CUDA toolkit installation\n",
    "- \u2705 CPU-only PyTorch binaries (smaller, faster to install)\n",
    "- \u2705 No GPU memory management needed\n",
    "- \u26a0\ufe0f Slower inference and training times\n",
    "\n",
    "### Expected Installation Time\n",
    "\n",
    "- CPU version: ~3-5 minutes\n",
    "- Dependencies are smaller without CUDA libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\ude80 HVAC AUTO-LABELING PIPELINE - CPU ENVIRONMENT SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mount Google Drive for data persistence\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"\u2705 Google Drive mounted successfully\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"\u2139\ufe0f  Running in local environment (not Colab)\")\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(f\"\ud83d\udcc2 Working Directory: {HOME}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udce6 INSTALLING CPU-OPTIMIZED DEPENDENCIES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\u26a0\ufe0f  CPU Mode: Processing will be slower than GPU but more compatible\")\n",
    "\n",
    "# Install PyTorch with CPU support only (no CUDA)\n",
    "print(\"\\n[1/6] Installing PyTorch (CPU version)...\")\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Verify PyTorch installation\n",
    "import torch\n",
    "print(f\"   \u2705 PyTorch {torch.__version__} installed (CPU)\")\n",
    "print(f\"   \u2139\ufe0f  CUDA available: {torch.cuda.is_available()} (Expected: False for CPU)\")\n",
    "print(f\"   \u2139\ufe0f  CPU device will be used for all operations\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   \u26a0\ufe0f  Warning: CUDA detected but will NOT be used in this CPU-optimized notebook\")\n",
    "\n",
    "# Install core autodistill framework\n",
    "print(\"\\n[2/6] Installing Autodistill core framework...\")\n",
    "!pip install -q autodistill\n",
    "print(\"   \u2705 Autodistill installed\")\n",
    "\n",
    "# Install Grounded-SAM-2 base model (will use CPU backend)\n",
    "print(\"\\n[3/6] Installing Grounded-SAM-2 base model (CPU mode)...\")\n",
    "!pip install -q autodistill-grounded-sam-2\n",
    "print(\"   \u2705 Grounded-SAM-2 installed (CPU backend)\")\n",
    "print(\"   \u2139\ufe0f  Model will automatically use CPU for inference\")\n",
    "\n",
    "# Install YOLOv8 target model for training\n",
    "print(\"\\n[4/6] Installing YOLOv8 target model (CPU compatible)...\")\n",
    "!pip install -q autodistill-yolov8\n",
    "print(\"   \u2705 YOLOv8 installed (CPU training supported)\")\n",
    "\n",
    "# Install supporting libraries\n",
    "print(\"\\n[5/6] Installing supporting libraries...\")\n",
    "!pip install -q opencv-python-headless matplotlib numpy supervision roboflow\n",
    "print(\"   \u2705 Supporting libraries installed\")\n",
    "\n",
    "# CPU Memory optimization check\n",
    "print(\"\\n[6/6] Checking system resources...\")\n",
    "import psutil\n",
    "import platform\n",
    "\n",
    "cpu_count = psutil.cpu_count(logical=True)\n",
    "memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "\n",
    "print(f\"   \u2139\ufe0f  CPU cores: {cpu_count}\")\n",
    "print(f\"   \u2139\ufe0f  System RAM: {memory_gb:.1f} GB\")\n",
    "print(f\"   \u2139\ufe0f  Platform: {platform.system()} {platform.machine()}\")\n",
    "\n",
    "# Set CPU thread optimization\n",
    "# Use all available CPU cores for parallel processing\n",
    "torch.set_num_threads(cpu_count)\n",
    "print(f\"   \u2705 PyTorch configured to use {cpu_count} CPU threads\")\n",
    "\n",
    "if memory_gb < 8:\n",
    "    print(\"   \u26a0\ufe0f  Warning: Less than 8GB RAM detected. Consider processing fewer images at once.\")\n",
    "elif memory_gb < 16:\n",
    "    print(\"   \u2139\ufe0f  Note: 16GB+ RAM recommended for optimal performance.\")\n",
    "else:\n",
    "    print(\"   \u2705 Sufficient RAM available for processing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 CPU ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\u26a0\ufe0f  Performance Note:\")\n",
    "print(\"   \u2022 Auto-labeling: ~10-60 seconds per image (vs 3-10s on GPU)\")\n",
    "print(\"   \u2022 Training: May take several hours (vs 10-30 min on GPU)\")\n",
    "print(\"   \u2022 Consider starting with a small subset of images for testing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Configuration & Logging Setup (CPU-Optimized)\n",
    "\n",
    "### Configuration Strategy\n",
    "\n",
    "This phase sets up:\n",
    "- Comprehensive logging system with file and console output\n",
    "- Progress tracking for all pipeline phases\n",
    "- Path configuration for templates, images, and outputs\n",
    "- Detection parameters optimized for CPU processing\n",
    "- Training parameters adjusted for CPU training times\n",
    "\n",
    "### CPU-Specific Adjustments\n",
    "\n",
    "- \u2699\ufe0f Same detection thresholds (quality remains consistent)\n",
    "- \u2699\ufe0f Reduced default training epochs for faster experimentation\n",
    "- \u2699\ufe0f Added performance monitoring and time estimates\n",
    "- \u2699\ufe0f Memory-efficient processing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "",
    "import logging",
    "",
    "import sys",
    "",
    "from pathlib import Path",
    "",
    "from datetime import datetime",
    "",
    "",
    "",
    "print(\"=\"*70)",
    "",
    "print(\"\u2699\ufe0f  PIPELINE CONFIGURATION & LOGGING SETUP\")",
    "",
    "print(\"=\"*70)",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# LOGGING CONFIGURATION",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "print(\"\\n\" + \"-\"*70)",
    "",
    "print(\"\ud83d\udcdd SETTING UP LOGGING SYSTEM\")",
    "",
    "print(\"-\"*70)",
    "",
    "",
    "",
    "# Create logs directory",
    "",
    "LOG_DIR = os.path.join(os.getcwd(), \"pipeline_logs\")",
    "",
    "os.makedirs(LOG_DIR, exist_ok=True)",
    "",
    "",
    "",
    "# Create timestamped log file",
    "",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
    "",
    "log_file = os.path.join(LOG_DIR, f\"autodistill_pipeline_{timestamp}.log\")",
    "",
    "",
    "",
    "# Configure logging",
    "",
    "logging.basicConfig(",
    "",
    "    level=logging.INFO,",
    "",
    "    format='%(asctime)s - %(levelname)s - %(message)s',",
    "",
    "    handlers=[",
    "",
    "        logging.FileHandler(log_file),",
    "",
    "        logging.StreamHandler(sys.stdout)",
    "",
    "    ]",
    "",
    ")",
    "",
    "",
    "",
    "logger = logging.getLogger(__name__)",
    "",
    "",
    "",
    "logger.info(\"=\"*70)",
    "",
    "logger.info(\"HVAC AUTO-LABELING PIPELINE - STARTING\")",
    "",
    "logger.info(\"=\"*70)",
    "",
    "logger.info(f\"Log file: {log_file}\")",
    "",
    "logger.info(f\"Timestamp: {timestamp}\")",
    "",
    "",
    "",
    "print(f\"\u2705 Logging system initialized\")",
    "",
    "print(f\"   \u2022 Log file: {log_file}\")",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# PROGRESS TRACKING UTILITIES",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "class ProgressTracker:",
    "",
    "    \"\"\"Track progress and performance metrics throughout the pipeline.\"\"\"",
    "",
    "    ",
    "",
    "    def __init__(self):",
    "",
    "        self.start_time = datetime.now()",
    "",
    "        self.phase_times = {}",
    "",
    "        self.metrics = {}",
    "",
    "        self.current_phase = None",
    "",
    "        self.phase_start = None",
    "",
    "    ",
    "",
    "    def start_phase(self, phase_name):",
    "",
    "        \"\"\"Start tracking a new phase.\"\"\"",
    "",
    "        if self.current_phase:",
    "",
    "            self.end_phase()",
    "",
    "        self.current_phase = phase_name",
    "",
    "        self.phase_start = datetime.now()",
    "",
    "        logger.info(f\"Starting phase: {phase_name}\")",
    "",
    "    ",
    "",
    "    def end_phase(self):",
    "",
    "        \"\"\"End current phase and record time.\"\"\"",
    "",
    "        if self.current_phase and self.phase_start:",
    "",
    "            duration = (datetime.now() - self.phase_start).total_seconds()",
    "",
    "            self.phase_times[self.current_phase] = duration",
    "",
    "            logger.info(f\"Completed phase: {self.current_phase} (Duration: {duration:.2f}s)\")",
    "",
    "            self.current_phase = None",
    "",
    "            self.phase_start = None",
    "",
    "    ",
    "",
    "    def record_metric(self, metric_name, value):",
    "",
    "        \"\"\"Record a metric value.\"\"\"",
    "",
    "        self.metrics[metric_name] = value",
    "",
    "        logger.info(f\"Metric - {metric_name}: {value}\")",
    "",
    "    ",
    "",
    "    def get_total_time(self):",
    "",
    "        \"\"\"Get total elapsed time.\"\"\"",
    "",
    "        return (datetime.now() - self.start_time).total_seconds()",
    "",
    "    ",
    "",
    "    def print_summary(self):",
    "",
    "        \"\"\"Print pipeline execution summary.\"\"\"",
    "",
    "        print(\"\\n\" + \"=\"*70)",
    "",
    "        print(\"\ud83d\udcca PIPELINE EXECUTION SUMMARY\")",
    "",
    "        print(\"=\"*70)",
    "",
    "        print(f\"\\n\u23f1\ufe0f  Total Pipeline Time: {self.get_total_time()/60:.2f} minutes\")",
    "",
    "        ",
    "",
    "        if self.phase_times:",
    "",
    "            print(\"\\n\ud83d\udd04 Phase Breakdown:\")",
    "",
    "            for phase, duration in self.phase_times.items():",
    "",
    "                print(f\"   \u2022 {phase:<30} {duration:>8.2f}s\")",
    "",
    "        ",
    "",
    "        if self.metrics:",
    "",
    "            print(\"\\n\ud83d\udcc8 Key Metrics:\")",
    "",
    "            for metric, value in self.metrics.items():",
    "",
    "                print(f\"   \u2022 {metric:<30} {value}\")",
    "",
    "        ",
    "",
    "        logger.info(\"Pipeline execution summary printed\")",
    "",
    "",
    "",
    "# Initialize global progress tracker",
    "",
    "progress = ProgressTracker()",
    "",
    "logger.info(\"Progress tracker initialized\")",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# PATH CONFIGURATION",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "progress.start_phase(\"Configuration\")",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    # Google Colab paths (using Google Drive for persistence)",
    "",
    "    BASE_DRIVE_PATH = \"/content/drive/MyDrive/HVAC_AutoLabeling/\"",
    "",
    "    TEMPLATE_FOLDER_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_templates/\")",
    "",
    "    UNLABELED_IMAGES_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_example_images/\")",
    "",
    "    TRAINING_OUTPUT_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_yolov8_training/\")",
    "",
    "    INFERENCE_OUTPUT_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_inference_results/\")",
    "",
    "    ",
    "",
    "    # Temporary dataset output (faster local processing)",
    "",
    "    DATASET_OUTPUT_PATH = os.path.join(HOME, \"hvac_autodistill_dataset/\")",
    "",
    "else:",
    "",
    "    # Local environment paths",
    "",
    "    BASE_PATH = Path.cwd()",
    "",
    "    TEMPLATE_FOLDER_PATH = str(BASE_PATH / \"ai_model\" / \"datasets\" / \"hvac_templates\" / \"hvac_templates\")",
    "",
    "    UNLABELED_IMAGES_PATH = str(BASE_PATH / \"ai_model\" / \"datasets\" / \"hvac_example_images\" / \"hvac_example_images\")",
    "",
    "    DATASET_OUTPUT_PATH = str(BASE_PATH / \"ai_model\" / \"outputs\" / \"autodistill_dataset\")",
    "",
    "    TRAINING_OUTPUT_PATH = str(BASE_PATH / \"ai_model\" / \"outputs\" / \"yolov8_training\")",
    "",
    "    INFERENCE_OUTPUT_PATH = str(BASE_PATH / \"ai_model\" / \"outputs\" / \"inference_results\")",
    "",
    "",
    "",
    "# Create all required directories",
    "",
    "for path_name, path_value in [",
    "",
    "    (\"Dataset Output\", DATASET_OUTPUT_PATH),",
    "",
    "    (\"Training Output\", TRAINING_OUTPUT_PATH),",
    "",
    "    (\"Inference Output\", INFERENCE_OUTPUT_PATH)",
    "",
    "]:",
    "",
    "    os.makedirs(path_value, exist_ok=True)",
    "",
    "    print(f\"\ud83d\udcc2 {path_name}: {path_value}\")",
    "",
    "    logger.info(f\"Created directory: {path_value}\")",
    "",
    "",
    "",
    "# Record path configuration",
    "",
    "progress.record_metric(\"Template Path\", TEMPLATE_FOLDER_PATH)",
    "",
    "progress.record_metric(\"Images Path\", UNLABELED_IMAGES_PATH)",
    "",
    "progress.record_metric(\"Output Path\", DATASET_OUTPUT_PATH)",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# DETECTION PARAMETERS (Research-Based Optimal Values)",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "# Box threshold: Confidence threshold for bounding box predictions",
    "",
    "# Range: 0.25-0.30 recommended for technical drawings",
    "",
    "# Lower = higher recall, more false positives",
    "",
    "# Higher = higher precision, may miss objects",
    "",
    "BOX_THRESHOLD = 0.27",
    "",
    "",
    "",
    "# Text threshold: Confidence threshold for text prompt matching",
    "",
    "# Range: 0.20-0.25 recommended for HVAC symbols",
    "",
    "# Lower = more lenient matching",
    "",
    "# Higher = stricter prompt matching",
    "",
    "TEXT_THRESHOLD = 0.22",
    "",
    "",
    "",
    "print(\"\\n\" + \"-\"*70)",
    "",
    "print(\"\ud83c\udfaf DETECTION PARAMETERS\")",
    "",
    "print(\"-\"*70)",
    "",
    "print(f\"Box Threshold:  {BOX_THRESHOLD:.2f} (optimized for technical drawings)\")",
    "",
    "print(f\"Text Threshold: {TEXT_THRESHOLD:.2f} (optimized for HVAC symbols)\")",
    "",
    "",
    "",
    "logger.info(f\"Detection parameters - Box: {BOX_THRESHOLD}, Text: {TEXT_THRESHOLD}\")",
    "",
    "progress.record_metric(\"Box Threshold\", BOX_THRESHOLD)",
    "",
    "progress.record_metric(\"Text Threshold\", TEXT_THRESHOLD)",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# TRAINING PARAMETERS",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "TRAINING_EPOCHS = 50  # Reduced for CPU (100+ recommended for production with GPU)",
    "",
    "YOLO_MODEL_SIZE = \"yolov8n.pt\"  # Nano model - fastest for CPU training",
    "",
    "",
    "",
    "print(\"\\n\" + \"-\"*70)",
    "",
    "print(\"\ud83c\udfcb\ufe0f  TRAINING PARAMETERS\")",
    "",
    "print(\"-\"*70)",
    "",
    "print(f\"Training Epochs: {TRAINING_EPOCHS}\")",
    "",
    "print(f\"YOLOv8 Model:    {YOLO_MODEL_SIZE}\")",
    "",
    "",
    "",
    "logger.info(f\"Training parameters - Epochs: {TRAINING_EPOCHS}, Model: {YOLO_MODEL_SIZE}\")",
    "",
    "progress.record_metric(\"Training Epochs\", TRAINING_EPOCHS)",
    "",
    "progress.record_metric(\"YOLO Model\", YOLO_MODEL_SIZE)",
    "",
    "",
    "",
    "progress.end_phase()",
    "",
    "",
    "",
    "print(\"\\n\" + \"=\"*70)",
    "",
    "print(\"\u2705 CONFIGURATION COMPLETE\")",
    "",
    "print(\"=\"*70)",
    "",
    "logger.info(\"Configuration phase completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Optimized Ontology Generation from HVAC Templates\n\n\n\n### Enhanced Ontology Design\n\n\n\nFollowing autodistill best practices with optimizations:\n\n- **Smart Template Processing**: Extracts and normalizes class names from filenames\n\n- **Prompt Engineering**: Creates descriptive, context-rich prompts for better detection\n\n- **Category Grouping**: Organizes classes by type (valves, instruments, signals, etc.)\n\n- **Validation**: Ensures ontology integrity and completeness\n\n\n\n### Template Processing Pipeline\n\n\n\n1. Scan template directory for all image files\n\n2. Extract and clean class names from filenames\n\n3. Apply intelligent prompt engineering\n\n4. Group classes by category for better organization\n\n5. Validate ontology structure and log statistics\n\n### CPU Compatibility\n\nOntology generation is device-agnostic and runs efficiently on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from autodistill.detection import CaptionOntology\n",
    "\n",
    "progress.start_phase(\"Ontology Generation\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udccb OPTIMIZED HVAC ONTOLOGY GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "logger.info(\"Starting ontology generation from templates\")\n",
    "print(f\"\\n\ud83d\udd0d Scanning template directory: {TEMPLATE_FOLDER_PATH}\")\n",
    "logger.info(f\"Template directory: {TEMPLATE_FOLDER_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEMPLATE DISCOVERY\n",
    "# ============================================================================\n",
    "\n",
    "# Find all template image files\n",
    "template_extensions = ['*.png', '*.PNG', '*.jpg', '*.JPG', '*.jpeg', '*.JPEG']\n",
    "all_template_files = []\n",
    "for ext in template_extensions:\n",
    "    found_files = glob.glob(os.path.join(TEMPLATE_FOLDER_PATH, ext))\n",
    "    all_template_files.extend(found_files)\n",
    "    if found_files:\n",
    "        logger.info(f\"Found {len(found_files)} files with extension {ext}\")\n",
    "\n",
    "if not all_template_files:\n",
    "    logger.error(f\"No template files found in {TEMPLATE_FOLDER_PATH}\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"\u274c FATAL ERROR: No template files found in {TEMPLATE_FOLDER_PATH}\\n\"\n",
    "        f\"   Please ensure template images are present in the directory.\"\n",
    "    )\n",
    "\n",
    "print(f\"\u2705 Found {len(all_template_files)} template files\")\n",
    "logger.info(f\"Total templates discovered: {len(all_template_files)}\")\n",
    "progress.record_metric(\"Template Files Found\", len(all_template_files))\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED PROMPT ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def engineer_prompt(base_name):\n",
    "    \"\"\"Apply intelligent prompt engineering for better detection.\n",
    "    \n",
    "    Creates context-rich prompts that help the model understand\n",
    "    the specific HVAC component being detected.\n",
    "    \"\"\"\n",
    "    # Clean the name\n",
    "    clean = base_name.replace('template_', '').replace('_', ' ').strip()\n",
    "    \n",
    "    # Add context for specific component types\n",
    "    if 'valve' in clean.lower():\n",
    "        prompt = f\"hvac {clean}\"\n",
    "    elif 'instrument' in clean.lower():\n",
    "        prompt = f\"hvac control {clean}\"\n",
    "    elif 'signal' in clean.lower():\n",
    "        prompt = f\"{clean} line\"\n",
    "    else:\n",
    "        prompt = clean\n",
    "    \n",
    "    return prompt, clean\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD ONTOLOGY WITH CATEGORIZATION\n",
    "# ============================================================================\n",
    "\n",
    "ontology_mapping = {}\n",
    "categories = defaultdict(list)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83d\udcdd PROCESSING TEMPLATES WITH PROMPT ENGINEERING\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "logger.info(\"Processing templates and engineering prompts\")\n",
    "\n",
    "for i, template_path in enumerate(sorted(all_template_files), 1):\n",
    "    # Extract filename without extension\n",
    "    filename = os.path.basename(template_path)\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Apply prompt engineering\n",
    "    prompt, class_name = engineer_prompt(base_name)\n",
    "    \n",
    "    ontology_mapping[prompt] = class_name\n",
    "    \n",
    "    # Categorize for organization\n",
    "    if 'valve' in class_name.lower():\n",
    "        categories['Valves'].append(class_name)\n",
    "    elif 'instrument' in class_name.lower():\n",
    "        categories['Instruments'].append(class_name)\n",
    "    elif 'signal' in class_name.lower():\n",
    "        categories['Signals'].append(class_name)\n",
    "    else:\n",
    "        categories['Other'].append(class_name)\n",
    "    \n",
    "    if i <= 10:  # Show first 10 mappings\n",
    "        print(f\"   [{i:2d}] {prompt:<40} -> {class_name}\")\n",
    "        logger.debug(f\"Mapped: {prompt} -> {class_name}\")\n",
    "\n",
    "if len(all_template_files) > 10:\n",
    "    print(f\"   ... and {len(all_template_files) - 10} more classes\")\n",
    "    logger.info(f\"Processed {len(all_template_files) - 10} additional classes\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE ONTOLOGY OBJECT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83c\udfd7\ufe0f  CREATING ONTOLOGY OBJECT\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "logger.info(\"Creating CaptionOntology object\")\n",
    "ontology = CaptionOntology(ontology_mapping)\n",
    "classes = ontology.classes()\n",
    "\n",
    "print(f\"\u2705 Ontology created successfully\")\n",
    "print(f\"\u2705 Total classes in ontology: {len(classes)}\")\n",
    "logger.info(f\"Ontology created with {len(classes)} classes\")\n",
    "progress.record_metric(\"Ontology Classes\", len(classes))\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83d\udcca CATEGORY BREAKDOWN\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for category, items in sorted(categories.items()):\n",
    "    print(f\"\\n\ud83c\udff7\ufe0f  {category} ({len(items)} classes):\")\n",
    "    logger.info(f\"Category '{category}': {len(items)} classes\")\n",
    "    for i, item in enumerate(sorted(items)[:5], 1):  # Show first 5\n",
    "        print(f\"   {i}. {item}\")\n",
    "    if len(items) > 5:\n",
    "        print(f\"   ... and {len(items) - 5} more\")\n",
    "    progress.record_metric(f\"Category: {category}\", len(items))\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\u2705 ONTOLOGY VALIDATION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Validate ontology integrity\n",
    "validation_checks = [\n",
    "    (len(ontology_mapping) > 0, \"Ontology has mappings\"),\n",
    "    (len(classes) == len(ontology_mapping), \"Class count matches mapping count\"),\n",
    "    (len(set(classes)) == len(classes), \"All class names are unique\"),\n",
    "    (all(len(c.strip()) > 0 for c in classes), \"All class names are non-empty\")\n",
    "]\n",
    "\n",
    "all_valid = True\n",
    "for check, description in validation_checks:\n",
    "    status = \"\u2705\" if check else \"\u274c\"\n",
    "    print(f\"   {status} {description}\")\n",
    "    logger.info(f\"Validation - {description}: {check}\")\n",
    "    if not check:\n",
    "        all_valid = False\n",
    "\n",
    "if not all_valid:\n",
    "    logger.error(\"Ontology validation failed\")\n",
    "    raise ValueError(\"Ontology validation failed. Please check the logs.\")\n",
    "\n",
    "progress.end_phase()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 ONTOLOGY GENERATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "logger.info(\"Ontology generation completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Enhanced Auto-Labeling with Per-Class Detection (CPU Mode)\n",
    "\n",
    "### Auto-Labeling Strategy\n",
    "\n",
    "This phase uses Grounded-SAM-2 to automatically generate high-quality annotations:\n",
    "- **Per-Class Detection Mode**: Highest precision, better class separation\n",
    "- **Florence-2 + SAM 2**: State-of-the-art vision-language model + segmentation\n",
    "- **Confidence Filtering**: Using optimized box and text thresholds\n",
    "- **Quality Metrics**: Comprehensive statistics and validation\n",
    "\n",
    "### CPU Performance Expectations\n",
    "\n",
    "\u26a0\ufe0f **Processing Time on CPU**:\n",
    "- **Per image**: ~10-60 seconds (depending on complexity and CPU)\n",
    "- **For 5 images**: ~1-5 minutes total\n",
    "- **For 20 images**: ~5-20 minutes total\n",
    "- **GPU comparison**: 5-10x faster on GPU\n",
    "\n",
    "### Progress Monitoring\n",
    "\n",
    "- Real-time progress updates for each image\n",
    "- Per-class detection counters\n",
    "- Confidence scores and quality metrics\n",
    "- Estimated time remaining\n",
    "\n",
    "### Optimization Tips for CPU\n",
    "\n",
    "- Start with 1-3 test images to gauge performance\n",
    "- Process in batches if you have many images\n",
    "- Close other applications to free up CPU resources\n",
    "- Monitor memory usage during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_grounded_sam_2 import GroundedSAM2",
    "",
    "from autodistill.core.dataset import DetectionDataset",
    "",
    "import glob",
    "",
    "import os",
    "",
    "import time",
    "",
    "import cv2",
    "",
    "import numpy as np",
    "",
    "import supervision as sv",
    "",
    "from collections import Counter",
    "",
    "",
    "",
    "progress.start_phase(\"Auto-Labeling\")",
    "",
    "",
    "",
    "print(\"=\"*70)",
    "",
    "print(\"\ud83e\udd16 ENHANCED AUTO-LABELING WITH PER-CLASS DETECTION\")",
    "",
    "print(\"=\"*70)",
    "",
    "logger.info(\"Starting auto-labeling phase\")",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# CONFIGURATION",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "# Detection mode: 'batch' (faster) or 'per_class' (more accurate)",
    "",
    "DETECTION_MODE = 'per_class'  # Use per-class for higher precision",
    "",
    "",
    "",
    "print(\"\\n\" + \"-\"*70)",
    "",
    "print(\"\u2699\ufe0f  DETECTION CONFIGURATION\")",
    "",
    "print(\"-\"*70)",
    "",
    "print(f\"Detection Mode: {DETECTION_MODE.upper()}\")",
    "",
    "if DETECTION_MODE == 'per_class':",
    "",
    "    print(\"   \u2022 Strategy: Iterative per-class detection\")",
    "",
    "    print(\"   \u2022 Benefits: Higher precision, better class separation\")",
    "",
    "    print(\"   \u2022 Trade-off: Longer processing time\")",
    "",
    "else:",
    "",
    "    print(\"   \u2022 Strategy: Batch detection of all classes\")",
    "",
    "    print(\"   \u2022 Benefits: Faster processing\")",
    "",
    "logger.info(f\"Detection mode: {DETECTION_MODE}\")",
    "",
    "progress.record_metric(\"Detection Mode\", DETECTION_MODE)",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# INITIALIZE GROUNDED-SAM-2 MODEL",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "print(\"\\n\" + \"-\"*70)",
    "",
    "print(\"\ud83c\udfd7\ufe0f  INITIALIZING GROUNDED-SAM-2 BASE MODEL\")",
    "",
    "print(\"-\"*70)",
    "",
    "logger.info(\"Initializing Grounded-SAM-2 model\")",
    "",
    "",
    "",
    "try:",
    "",
    "    base_model = GroundedSAM2(",
    "",
    "        ontology=ontology,",
    "",
    "        grounding_dino_box_threshold=BOX_THRESHOLD,",
    "",
    "        grounding_dino_text_threshold=TEXT_THRESHOLD",
    "",
    "    )",
    "",
    "    print(\"\u2705 Grounded-SAM-2 model initialized successfully\")",
    "",
    "    print(f\"   \u2022 Model type: GroundedSAM2 (Florence-2 + SAM 2)\")",
    "",
    "    print(f\"   \u2022 Box threshold: {BOX_THRESHOLD}\")",
    "",
    "    print(f\"   \u2022 Text threshold: {TEXT_THRESHOLD}\")",
    "",
    "    print(f\"   \u2022 Classes loaded: {len(classes)}\")",
    "",
    "    logger.info(f\"Model initialized: box_threshold={BOX_THRESHOLD}, text_threshold={TEXT_THRESHOLD}\")",
    "",
    "except Exception as e:",
    "",
    "    logger.error(f\"Failed to initialize model: {str(e)}\")",
    "",
    "    raise RuntimeError(",
    "",
    "        f\"\u274c FATAL ERROR: Failed to initialize Grounded-SAM-2 model\\n\"",
    "",
    "        f\"   Error: {str(e)}\\n\"",
    "",
    "        f\"   Please ensure all dependencies are installed correctly.\"",
    "",
    "    )",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# SCAN FOR UNLABELED IMAGES",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "print(\"\\n\" + \"-\"*70)",
    "",
    "print(\"\ud83d\udcc1 SCANNING FOR UNLABELED IMAGES\")",
    "",
    "print(\"-\"*70)",
    "",
    "logger.info(\"Scanning for unlabeled images\")",
    "",
    "",
    "",
    "print(f\"\ud83d\udd0d Scanning directory: {UNLABELED_IMAGES_PATH}\")",
    "",
    "",
    "",
    "# Find all image files",
    "",
    "image_extensions = ['*.png', '*.PNG', '*.jpg', '*.JPG', '*.jpeg', '*.JPEG']",
    "",
    "image_paths = []",
    "",
    "for ext in image_extensions:",
    "",
    "    found = glob.glob(os.path.join(UNLABELED_IMAGES_PATH, ext))",
    "",
    "    image_paths.extend(found)",
    "",
    "    if found:",
    "",
    "        logger.info(f\"Found {len(found)} images with extension {ext}\")",
    "",
    "",
    "",
    "if not image_paths:",
    "",
    "    logger.error(f\"No images found in {UNLABELED_IMAGES_PATH}\")",
    "",
    "    raise FileNotFoundError(",
    "",
    "        f\"\u274c FATAL ERROR: No images found in {UNLABELED_IMAGES_PATH}\\n\"",
    "",
    "        f\"   Please add images to the directory before running auto-labeling.\"",
    "",
    "    )",
    "",
    "",
    "",
    "print(f\"\u2705 Found {len(image_paths)} images to process\")",
    "",
    "logger.info(f\"Total images to process: {len(image_paths)}\")",
    "",
    "progress.record_metric(\"Images to Process\", len(image_paths))",
    "",
    "",
    "",
    "# Display image list",
    "",
    "print(\"\\n\ud83d\udccb Image files:\")",
    "",
    "for i, img_path in enumerate(sorted(image_paths), 1):",
    "",
    "    img_name = os.path.basename(img_path)",
    "",
    "    print(f\"   {i}. {img_name}\")",
    "",
    "    logger.debug(f\"Image {i}: {img_name}\")",
    "",
    "",
    "",
    "# ============================================================================",
    "",
    "# RUN ENHANCED AUTO-LABELING",
    "",
    "# ============================================================================",
    "",
    "",
    "",
    "print(\"\\n\" + \"=\"*70)",
    "",
    "print(\"\ud83d\ude80 STARTING ENHANCED AUTO-LABELING\")",
    "",
    "print(\"=\"*70)",
    "",
    "",
    "",
    "if DETECTION_MODE == 'batch':",
    "",
    "    # ========================================================================",
    "",
    "    # BATCH MODE: Fast processing of all classes together",
    "",
    "    # ========================================================================",
    "",
    "    print(\"\\n\u2139\ufe0f  Using BATCH mode for fast processing\")",
    "",
    "    logger.info(\"Using batch detection mode\")",
    "",
    "    ",
    "",
    "    start_time = time.time()",
    "",
    "    ",
    "",
    "    try:",
    "",
    "        base_model.label(",
    "",
    "            input_folder=UNLABELED_IMAGES_PATH,",
    "",
    "            output_folder=DATASET_OUTPUT_PATH,",
    "",
    "            extension=\".jpg\"",
    "",
    "        )",
    "",
    "        elapsed_time = time.time() - start_time",
    "",
    "        ",
    "",
    "        logger.info(f\"Batch labeling completed in {elapsed_time:.2f}s\")",
    "",
    "        progress.record_metric(\"Labeling Time (s)\", f\"{elapsed_time:.2f}\")",
    "",
    "        progress.record_metric(\"Avg Time per Image (s)\", f\"{elapsed_time/len(image_paths):.2f}\")",
    "",
    "        ",
    "",
    "    except Exception as e:",
    "",
    "        logger.error(f\"Batch labeling failed: {str(e)}\")",
    "",
    "        raise",
    "",
    "",
    "",
    "else:",
    "",
    "    # ========================================================================",
    "",
    "    # PER-CLASS MODE: High-precision iterative detection",
    "",
    "    # ========================================================================",
    "",
    "    print(\"\\n\u2139\ufe0f  Using PER-CLASS mode for maximum precision\")",
    "",
    "    print(\"   This will detect each class individually for better accuracy\\n\")",
    "",
    "    logger.info(\"Using per-class detection mode for maximum precision\")",
    "",
    "    ",
    "",
    "    # Initialize dataset",
    "",
    "    dataset = DetectionDataset(classes=classes, base_dir=DATASET_OUTPUT_PATH)",
    "",
    "    ",
    "",
    "    # Statistics tracking",
    "",
    "    total_detections = 0",
    "",
    "    detections_by_class = Counter()",
    "",
    "    detections_by_image = {}",
    "",
    "    confidence_scores = []",
    "",
    "    ",
    "",
    "    start_time = time.time()",
    "",
    "    ",
    "",
    "    # Process each image",
    "",
    "    for img_idx, img_path in enumerate(sorted(image_paths), 1):",
    "",
    "        img_name = os.path.basename(img_path)",
    "",
    "        print(f\"\\n[{img_idx}/{len(image_paths)}] Processing: {img_name}\")",
    "",
    "        logger.info(f\"Processing image {img_idx}/{len(image_paths)}: {img_name}\")",
    "",
    "        ",
    "",
    "        img_start_time = time.time()",
    "",
    "        ",
    "",
    "        try:",
    "",
    "            # Read image",
    "",
    "            image = cv2.imread(img_path)",
    "",
    "            if image is None:",
    "",
    "                logger.warning(f\"Failed to read image: {img_path}\")",
    "",
    "                print(f\"   \u26a0\ufe0f  Warning: Could not read image, skipping\")",
    "",
    "                continue",
    "",
    "            ",
    "",
    "            # Detect each class individually",
    "",
    "            all_detections = []",
    "",
    "            class_detection_count = 0",
    "",
    "            ",
    "",
    "            for class_idx, class_name in enumerate(classes):",
    "",
    "                # Predict for this specific class",
    "",
    "                detections = base_model.predict(image, prompt=class_name)",
    "",
    "                ",
    "",
    "                if len(detections) > 0:",
    "",
    "                    # Set class IDs",
    "",
    "                    detections.class_id = np.full(len(detections), class_idx)",
    "",
    "                    all_detections.append(detections)",
    "",
    "                    class_detection_count += len(detections)",
    "",
    "                    ",
    "",
    "                    # Track statistics",
    "",
    "                    detections_by_class[class_name] += len(detections)",
    "",
    "                    if hasattr(detections, 'confidence') and detections.confidence is not None:",
    "",
    "                        confidence_scores.extend(detections.confidence.tolist())",
    "",
    "                    ",
    "",
    "                    logger.debug(f\"  Class '{class_name}': {len(detections)} detections\")",
    "",
    "            ",
    "",
    "            # Merge all detections for this image",
    "",
    "            if all_detections:",
    "",
    "                final_detections = sv.Detections.merge(all_detections)",
    "",
    "                dataset.add_detection(image_path=img_path, detections=final_detections)",
    "",
    "                ",
    "",
    "                total_detections += len(final_detections)",
    "",
    "                detections_by_image[img_name] = len(final_detections)",
    "",
    "                ",
    "",
    "                # Calculate statistics",
    "",
    "                if hasattr(final_detections, 'confidence') and final_detections.confidence is not None:",
    "",
    "                    avg_conf = np.mean(final_detections.confidence)",
    "",
    "                    min_conf = np.min(final_detections.confidence)",
    "",
    "                    max_conf = np.max(final_detections.confidence)",
    "",
    "                else:",
    "",
    "                    avg_conf = min_conf = max_conf = 0.0",
    "",
    "                ",
    "",
    "                img_time = time.time() - img_start_time",
    "                avg_time_per_image = (time.time() - start_time) / img_idx",
    "                remaining_images = len(image_paths) - img_idx",
    "                estimated_remaining = avg_time_per_image * remaining_images",
    "",
    "                ",
    "",
    "                print(f\"   \u2705 SUCCESS: {len(final_detections)} symbols detected\")",
    "",
    "                print(f\"      \u2022 Confidence: avg={avg_conf:.3f}, min={min_conf:.3f}, max={max_conf:.3f}\")",
    "",
    "                print(f\"      \u2022 Processing time: {img_time:.2f}s\")",
    "                if remaining_images > 0:",
    "                    print(f\"      \u2022 Est. time remaining: {estimated_remaining/60:.1f} min for {remaining_images} images\")",
    "",
    "                ",
    "",
    "                logger.info(",
    "",
    "                    f\"Image {img_name}: {len(final_detections)} detections, \"",
    "",
    "                    f\"avg_conf={avg_conf:.3f}, time={img_time:.2f}s\"",
    "",
    "                )",
    "",
    "            else:",
    "",
    "                print(f\"   \u2139\ufe0f  No symbols detected\")",
    "",
    "                detections_by_image[img_name] = 0",
    "",
    "                logger.info(f\"Image {img_name}: No detections\")",
    "",
    "        ",
    "",
    "        except Exception as e:",
    "",
    "            print(f\"   \u274c ERROR: {str(e)}\")",
    "",
    "            logger.error(f\"Failed to process {img_path}: {str(e)}\")",
    "",
    "    ",
    "",
    "    elapsed_time = time.time() - start_time",
    "",
    "    ",
    "",
    "    # ========================================================================",
    "",
    "    # QUALITY METRICS & STATISTICS",
    "",
    "    # ========================================================================",
    "",
    "    ",
    "",
    "    print(\"\\n\" + \"=\"*70)",
    "",
    "    print(\"\ud83d\udcca LABELING STATISTICS & QUALITY METRICS\")",
    "",
    "    print(\"=\"*70)",
    "",
    "    ",
    "",
    "    images_with_detections = sum(1 for count in detections_by_image.values() if count > 0)",
    "",
    "    ",
    "",
    "    print(f\"\\n\u23f1\ufe0f  Performance Metrics:\")",
    "",
    "    print(f\"   \u2022 Total processing time: {elapsed_time:.2f}s ({elapsed_time/60:.2f} min)\")",
    "",
    "    print(f\"   \u2022 Average time per image: {elapsed_time/len(image_paths):.2f}s\")",
    "",
    "    print(f\"   \u2022 Processing speed: {len(image_paths)/elapsed_time:.2f} images/second\")",
    "",
    "    ",
    "",
    "    logger.info(f\"Total processing time: {elapsed_time:.2f}s\")",
    "",
    "    progress.record_metric(\"Labeling Time (s)\", f\"{elapsed_time:.2f}\")",
    "",
    "    progress.record_metric(\"Avg Time per Image (s)\", f\"{elapsed_time/len(image_paths):.2f}\")",
    "",
    "    ",
    "",
    "    print(f\"\\n\ud83c\udfaf Detection Metrics:\")",
    "",
    "    print(f\"   \u2022 Total detections: {total_detections}\")",
    "",
    "    print(f\"   \u2022 Images processed: {len(image_paths)}\")",
    "",
    "    print(f\"   \u2022 Images with detections: {images_with_detections} ({images_with_detections/len(image_paths)*100:.1f}%)\")",
    "",
    "    if images_with_detections > 0:",
    "",
    "        print(f\"   \u2022 Average detections per image: {total_detections/images_with_detections:.2f}\")",
    "",
    "    ",
    "",
    "    logger.info(f\"Total detections: {total_detections}\")",
    "",
    "    logger.info(f\"Images with detections: {images_with_detections}/{len(image_paths)}\")",
    "",
    "    progress.record_metric(\"Total Detections\", total_detections)",
    "",
    "    progress.record_metric(\"Images with Detections\", images_with_detections)",
    "",
    "    ",
    "",
    "    if confidence_scores:",
    "",
    "        print(f\"\\n\ud83d\udcc8 Confidence Score Statistics:\")",
    "",
    "        print(f\"   \u2022 Mean: {np.mean(confidence_scores):.3f}\")",
    "",
    "        print(f\"   \u2022 Std Dev: {np.std(confidence_scores):.3f}\")",
    "",
    "        print(f\"   \u2022 Min: {np.min(confidence_scores):.3f}\")",
    "",
    "        print(f\"   \u2022 Max: {np.max(confidence_scores):.3f}\")",
    "",
    "        print(f\"   \u2022 Median: {np.median(confidence_scores):.3f}\")",
    "",
    "        ",
    "",
    "        logger.info(",
    "",
    "            f\"Confidence scores - mean: {np.mean(confidence_scores):.3f}, \"",
    "",
    "            f\"std: {np.std(confidence_scores):.3f}\"",
    "",
    "        )",
    "",
    "        progress.record_metric(\"Avg Confidence\", f\"{np.mean(confidence_scores):.3f}\")",
    "",
    "    ",
    "",
    "    if detections_by_class:",
    "",
    "        print(f\"\\n\ud83c\udff7\ufe0f  Top 10 Detected Classes:\")",
    "",
    "        for i, (class_name, count) in enumerate(detections_by_class.most_common(10), 1):",
    "",
    "            print(f\"   {i:2d}. {class_name:<35} {count:>3} detections\")",
    "",
    "            logger.info(f\"Class '{class_name}': {count} detections\")",
    "",
    "        ",
    "",
    "        if len(detections_by_class) > 10:",
    "",
    "            remaining = sum(count for _, count in list(detections_by_class.items())[10:])",
    "",
    "            print(f\"   ... and {len(detections_by_class)-10} more classes ({remaining} detections)\")",
    "",
    "        ",
    "",
    "        progress.record_metric(\"Unique Classes Detected\", len(detections_by_class))",
    "",
    "    ",
    "",
    "    # Validation warnings",
    "",
    "    print(f\"\\n\u26a0\ufe0f  Quality Validation:\")",
    "",
    "    if images_with_detections == 0:",
    "",
    "        print(f\"   \u26a0\ufe0f  WARNING: No detections in any image\")",
    "",
    "        logger.warning(\"No detections found in any image\")",
    "",
    "    elif images_with_detections < len(image_paths) * 0.5:",
    "",
    "        print(f\"   \u26a0\ufe0f  WARNING: Less than 50% of images have detections\")",
    "",
    "        logger.warning(f\"Only {images_with_detections}/{len(image_paths)} images have detections\")",
    "",
    "    else:",
    "",
    "        print(f\"   \u2705 Good detection coverage\")",
    "",
    "    ",
    "",
    "    if confidence_scores and np.mean(confidence_scores) < 0.3:",
    "",
    "        print(f\"   \u26a0\ufe0f  WARNING: Low average confidence score ({np.mean(confidence_scores):.3f})\")",
    "",
    "        logger.warning(f\"Low confidence score: {np.mean(confidence_scores):.3f}\")",
    "",
    "    elif confidence_scores:",
    "",
    "        print(f\"   \u2705 Good average confidence score\")",
    "",
    "",
    "",
    "print(\"\\n\" + \"=\"*70)",
    "",
    "print(\"\u2705 AUTO-LABELING COMPLETE\")",
    "",
    "print(\"=\"*70)",
    "",
    "print(f\"\ud83d\udcbe Dataset saved to: {DATASET_OUTPUT_PATH}\")",
    "",
    "logger.info(\"Auto-labeling phase completed successfully\")",
    "",
    "progress.end_phase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Enhanced Quality Review & Visualization\n",
    "\n",
    "### Quality Assurance Process\n",
    "\n",
    "This phase provides comprehensive quality metrics and visualization:\n",
    "- Detailed dataset statistics\n",
    "- Class distribution analysis\n",
    "- Bounding box statistics\n",
    "- Visual sample review\n",
    "- Approval gate before training\n",
    "\n",
    "### CPU Note\n",
    "\n",
    "Quality review and visualization run efficiently on CPU - no performance impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\\n",
    "# COMPUTE COMPREHENSIVE DATASET STATISTICS\\n",
    "# ============================================================================\\n",
    "\\n",
    "print(\"\\\\n\" + \"-\"*70)\\n",
    "print(\"\ud83d\udcca COMPREHENSIVE DATASET STATISTICS\")\\n",
    "print(\"-\"*70)\\n",
    "logger.info(\"Computing dataset statistics\")\\n",
    "\\n",
    "total_detections = 0\\n",
    "class_counts = Counter()\\n",
    "images_with_detections = 0\\n",
    "detection_counts_per_image = []\\n",
    "bbox_sizes = []\\n",
    "\\n",
    "for image_path, detections in review_dataset:\\n",
    "    num_detections = len(detections)\\n",
    "    detection_counts_per_image.append(num_detections)\\n",
    "    \\n",
    "    if num_detections > 0:\\n",
    "        images_with_detections += 1\\n",
    "        total_detections += num_detections\\n",
    "        \\n",
    "        # Count classes\\n",
    "        for class_id in detections.class_id:\\n",
    "            class_name = review_dataset.classes[class_id]\\n",
    "            class_counts[class_name] += 1\\n",
    "        \\n",
    "        # Analyze bounding box sizes\\n",
    "        if hasattr(detections, 'xyxy') and detections.xyxy is not None:\\n",
    "            for box in detections.xyxy:\\n",
    "                width = box[2] - box[0]\\n",
    "                height = box[3] - box[1]\\n",
    "                area = width * height\\n",
    "                bbox_sizes.append(area)\\n",
    "\\n",
    "# Basic statistics\\n",
    "print(f\"\\\\n\ud83d\udcc8 Detection Summary:\")\\n",
    "print(f\"   \u2022 Images with detections: {images_with_detections}/{len(review_dataset)} ({images_with_detections/len(review_dataset)*100:.1f}%)\")\\n",
    "print(f\"   \u2022 Total detections: {total_detections}\")\\n",
    "if images_with_detections > 0:\\n",
    "    print(f\"   \u2022 Average detections per image: {total_detections/images_with_detections:.2f}\")\\n",
    "    print(f\"   \u2022 Min detections in an image: {min([c for c in detection_counts_per_image if c > 0])}\")\\n",
    "    print(f\"   \u2022 Max detections in an image: {max(detection_counts_per_image)}\")\\n",
    "\\n",
    "logger.info(f\"Dataset stats: {total_detections} detections across {images_with_detections} images\")\\n",
    "progress.record_metric(\"Total Dataset Detections\", total_detections)\\n",
    "progress.record_metric(\"Images with Detections\", f\"{images_with_detections}/{len(review_dataset)}\")\\n",
    "\\n",
    "# Bounding box statistics\\n",
    "if bbox_sizes:\\n",
    "    print(f\"\\\\n\ud83d\udccf Bounding Box Statistics:\")\\n",
    "    print(f\"   \u2022 Average area: {np.mean(bbox_sizes):.1f} px\u00b2\")\\n",
    "    print(f\"   \u2022 Median area: {np.median(bbox_sizes):.1f} px\u00b2\")\\n",
    "    print(f\"   \u2022 Std deviation: {np.std(bbox_sizes):.1f} px\u00b2\")\\n",
    "    logger.info(f\"Avg bbox area: {np.mean(bbox_sizes):.1f} px\u00b2\")\\n",
    "\\n",
    "# Class distribution\\n",
    "if class_counts:\\n",
    "    print(f\"\\\\n\ud83c\udff7\ufe0f  Class Distribution (Top 15):\")\\n",
    "    for i, (class_name, count) in enumerate(class_counts.most_common(15), 1):\\n",
    "        percentage = (count / total_detections) * 100\\n",
    "        print(f\"   {i:2d}. {class_name:<35} {count:>3} ({percentage:>5.1f}%)\")\\n",
    "        logger.debug(f\"Class {class_name}: {count} detections ({percentage:.1f}%)\")\\n",
    "    \\n",
    "    if len(class_counts) > 15:\\n",
    "        remaining = len(class_counts) - 15\\n",
    "        remaining_detections = sum(count for _, count in list(class_counts.items())[15:])\\n",
    "        print(f\"   ... and {remaining} more classes ({remaining_detections} detections)\")\\n",
    "    \\n",
    "    progress.record_metric(\"Classes with Detections\", len(class_counts))\\n",
    "    \\n",
    "    # Class balance analysis\\n",
    "    print(f\"\\\\n\u2696\ufe0f  Class Balance Analysis:\")\\n",
    "    most_common_count = class_counts.most_common(1)[0][1]\\n",
    "    least_common_count = class_counts.most_common()[-1][1]\\n",
    "    imbalance_ratio = most_common_count / least_common_count if least_common_count > 0 else 0\\n",
    "    print(f\"   \u2022 Most common class: {most_common_count} detections\")\\n",
    "    print(f\"   \u2022 Least common class: {least_common_count} detections\")\\n",
    "    print(f\"   \u2022 Imbalance ratio: {imbalance_ratio:.1f}:1\")\\n",
    "    \\n",
    "    if imbalance_ratio > 10:\\n",
    "        print(f\"   \u26a0\ufe0f  WARNING: High class imbalance detected\")\\n",
    "        logger.warning(f\"High class imbalance: {imbalance_ratio:.1f}:1\")\\n",
    "    else:\\n",
    "        print(f\"   \u2705 Reasonable class balance\")\\n",
    "    \\n",
    "    logger.info(f\"Class balance ratio: {imbalance_ratio:.1f}:1\")\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Train YOLOv8 Model (CPU Mode)\n",
    "\n",
    "### Training Strategy\n",
    "\n",
    "This phase trains a YOLOv8 model on the auto-labeled dataset:\n",
    "- Uses YOLOv8 Nano (fastest variant)\n",
    "- Trains on CPU (slower but functional)\n",
    "- Saves best and last checkpoints\n",
    "- Generates training curves and metrics\n",
    "\n",
    "### \u26a0\ufe0f CPU Training Performance\n",
    "\n",
    "**Expected Training Time on CPU**:\n",
    "- **Per epoch**: ~2-10 minutes (depending on dataset size and CPU)\n",
    "- **50 epochs**: ~2-8 hours\n",
    "- **100 epochs**: ~4-16 hours\n",
    "- **GPU comparison**: 10-30 minutes for 100 epochs on GPU\n",
    "\n",
    "### Recommendations for CPU Training\n",
    "\n",
    "1. **Start with fewer epochs** (10-20) to test the pipeline\n",
    "2. **Use smaller dataset** for initial experiments\n",
    "3. **Run overnight** for production training\n",
    "4. **Monitor progress** through training logs\n",
    "5. **Consider cloud GPU** for final production training\n",
    "\n",
    "### Training Control\n",
    "\n",
    "Set `PROCEED_TO_TRAINING = True` in the approval gate above to start training.\n",
    "For CPU systems, you may want to set `TRAINING_EPOCHS` to a lower value (e.g., 10-20) for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCEED_TO_TRAINING:",
    "",
    "    from autodistill_yolov8 import YOLOv8",
    "",
    "    import torch",
    "",
    "    import locale",
    "",
    "    import time",
    "",
    "    ",
    "",
    "    print(\"=\"*70)",
    "",
    "    print(\"\ud83c\udfcb\ufe0f  TRAINING YOLOV8 MODEL\")",
    "",
    "    print(\"=\"*70)",
    "",
    "    ",
    "",
    "    # Set locale for proper encoding (prevents some training warnings)",
    "",
    "    locale.getpreferredencoding = lambda: \"UTF-8\"",
    "",
    "    ",
    "",
    "    TRAIN_DATASET_PATH = os.path.join(DATASET_OUTPUT_PATH, \"data.yaml\")",
    "",
    "    ",
    "",
    "    print(f\"\\n\ud83d\udccb Training Configuration:\")",
    "",
    "    print(f\"   \u2022 Dataset: {TRAIN_DATASET_PATH}\")",
    "",
    "    print(f\"   \u2022 Model: {YOLO_MODEL_SIZE}\")",
    "",
    "    print(f\"   \u2022 Epochs: {TRAINING_EPOCHS}\")",
    "",
    "    print(f\"   \u2022 Output: {TRAINING_OUTPUT_PATH}\")",
    "    print(f\"   \u2022 Device: CPU (expect {TRAINING_EPOCHS * 2}-{TRAINING_EPOCHS * 10} minutes total)\")",
    "",
    "    ",
    "",
    "    # ========================================================================",
    "",
    "    # INITIALIZE YOLOV8 MODEL WITH SECURITY CONTEXT",
    "",
    "    # ========================================================================",
    "",
    "    ",
    "",
    "    print(\"\\n\" + \"-\"*70)",
    "",
    "    print(\"\ud83c\udfd7\ufe0f  INITIALIZING YOLOV8 MODEL\")",
    "",
    "    print(\"-\"*70)",
    "",
    "    ",
    "",
    "    # Import required model components for safe loading",
    "",
    "    from ultralytics.nn.modules import (",
    "",
    "        C2f, Detect, Bottleneck, Conv, ConvTranspose, DFL",
    "",
    "    )",
    "",
    "    ",
    "",
    "    SAFE_GLOBALS = [",
    "",
    "        C2f, Detect, Bottleneck, Conv, ConvTranspose, DFL,",
    "",
    "        torch.nn.ModuleList",
    "",
    "    ]",
    "",
    "    ",
    "",
    "    try:",
    "",
    "        # Use security context for safe model loading",
    "",
    "        with torch.serialization.safe_globals(SAFE_GLOBALS):",
    "",
    "            target_model = YOLOv8(YOLO_MODEL_SIZE)",
    "",
    "        ",
    "",
    "        print(f\"\u2705 YOLOv8 model initialized successfully\")",
    "",
    "        print(f\"   \u2022 Architecture: {YOLO_MODEL_SIZE}\")",
    "",
    "        print(f\"   \u2022 Secure loading: Enabled\")",
    "",
    "        ",
    "",
    "    except Exception as e:",
    "",
    "        raise RuntimeError(",
    "",
    "            f\"\u274c FATAL ERROR: Failed to initialize YOLOv8 model\\n\"",
    "",
    "            f\"   Error: {str(e)}\\n\"",
    "",
    "            f\"   Please ensure YOLOv8 is installed correctly.\"",
    "",
    "        )",
    "",
    "    ",
    "",
    "    # ========================================================================",
    "",
    "    # START TRAINING",
    "",
    "    # ========================================================================",
    "",
    "    ",
    "",
    "    print(\"\\n\" + \"=\"*70)",
    "",
    "    print(\"\ud83d\ude80 STARTING TRAINING\")",
    "",
    "    print(\"=\"*70)",
    "",
    "    print(\"\\n\u26a0\ufe0f  CPU Training Performance:\")",
    "    print(f\"   \u2022 Expected time: ~{TRAINING_EPOCHS * 2}-{TRAINING_EPOCHS * 5} minutes for {TRAINING_EPOCHS} epochs\")",
    "    print(\"   \u2022 Consider reducing TRAINING_EPOCHS if this is too long\")",
    "    print(\"   \u2022 Training progress will be displayed below...\")",
    "",
    "    print(\"   Training progress will be displayed below...\\n\")",
    "",
    "    ",
    "",
    "    start_time = time.time()",
    "",
    "    ",
    "",
    "    try:",
    "",
    "        target_model.train(",
    "",
    "            data_path=TRAIN_DATASET_PATH,",
    "",
    "            epochs=TRAINING_EPOCHS,",
    "",
    "            project=TRAINING_OUTPUT_PATH",
    "",
    "        )",
    "",
    "        ",
    "",
    "        elapsed_time = time.time() - start_time",
    "",
    "        ",
    "",
    "        print(\"\\n\" + \"=\"*70)",
    "",
    "        print(\"\u2705 TRAINING COMPLETE\")",
    "",
    "        print(\"=\"*70)",
    "",
    "        print(f\"\u23f1\ufe0f  Total training time: {elapsed_time/60:.2f} minutes\")",
    "",
    "        print(f\"\ud83d\udcbe Model saved to: {TRAINING_OUTPUT_PATH}\")",
    "",
    "        print(\"\\n\ud83d\udcca Check the training output directory for:\")",
    "",
    "        print(\"   \u2022 weights/best.pt - Best model checkpoint\")",
    "",
    "        print(\"   \u2022 weights/last.pt - Last epoch checkpoint\")",
    "",
    "        print(\"   \u2022 Training curves and metrics\")",
    "",
    "        ",
    "",
    "    except Exception as e:",
    "",
    "        print(\"\\n\" + \"=\"*70)",
    "",
    "        print(\"\u274c TRAINING FAILED\")",
    "",
    "        print(\"=\"*70)",
    "",
    "        print(f\"Error: {str(e)}\")",
    "",
    "        print(\"\\nPossible causes:\")",
    "",
    "        print(\"  \u2022 Insufficient GPU memory\")",
    "",
    "        print(\"  \u2022 Invalid dataset format\")",
    "",
    "        print(\"  \u2022 Corrupted data.yaml file\")",
    "",
    "        print(\"  \u2022 No training images in dataset\")",
    "",
    "        PROCEED_TO_TRAINING = False",
    "",
    "        raise",
    "",
    "",
    "",
    "else:",
    "",
    "    print(\"\\n\" + \"=\"*70)",
    "",
    "    print(\"\u23ed\ufe0f  TRAINING SKIPPED\")",
    "",
    "    print(\"=\"*70)",
    "",
    "    print(\"\\nReason: Dataset was not approved for training.\")",
    "",
    "    print(\"To train the model, re-run the quality review cell and approve the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Inference with Trained Model (CPU Mode)\n",
    "\n",
    "### Inference Strategy\n",
    "\n",
    "This phase tests the trained YOLOv8 model:\n",
    "- Loads the best checkpoint from training\n",
    "- Runs inference on test images\n",
    "- Visualizes detected symbols\n",
    "- Displays results with bounding boxes\n",
    "\n",
    "### CPU Inference Performance\n",
    "\n",
    "- **Per image**: ~1-5 seconds on CPU\n",
    "- **Faster than auto-labeling** (model is optimized)\n",
    "- **Acceptable for production** use on CPU\n",
    "\n",
    "### Note\n",
    "\n",
    "This section only runs if training was completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCEED_TO_TRAINING:",
    "",
    "    from ultralytics import YOLO",
    "",
    "    import glob",
    "",
    "    import os",
    "",
    "    import cv2",
    "",
    "    from IPython.display import Image, display",
    "",
    "    import time",
    "",
    "    ",
    "",
    "    print(\"=\"*70)",
    "",
    "    print(\"\ud83d\udd2e INFERENCE WITH TRAINED MODEL\")",
    "",
    "    print(\"=\"*70)",
    "",
    "    ",
    "",
    "    # ========================================================================",
    "",
    "    # LOCATE TRAINED MODEL",
    "",
    "    # ========================================================================",
    "",
    "    ",
    "",
    "    print(\"\\n\" + \"-\"*70)",
    "",
    "    print(\"\ud83d\udd0d LOCATING TRAINED MODEL\")",
    "",
    "    print(\"-\"*70)",
    "",
    "    ",
    "",
    "    # Find the latest training run",
    "",
    "    run_folders = sorted(glob.glob(os.path.join(TRAINING_OUTPUT_PATH, 'train*')))",
    "",
    "    ",
    "",
    "    if not run_folders:",
    "",
    "        raise FileNotFoundError(",
    "",
    "            f\"\u274c FATAL ERROR: No training runs found in {TRAINING_OUTPUT_PATH}\\n\"",
    "",
    "            f\"   Please ensure training completed successfully.\"",
    "",
    "        )",
    "",
    "    ",
    "",
    "    latest_run_folder = run_folders[-1]",
    "",
    "    TRAINED_MODEL_PATH = os.path.join(latest_run_folder, 'weights/best.pt')",
    "",
    "    ",
    "",
    "    if not os.path.exists(TRAINED_MODEL_PATH):",
    "",
    "        raise FileNotFoundError(",
    "",
    "            f\"\u274c FATAL ERROR: Model checkpoint not found at {TRAINED_MODEL_PATH}\\n\"",
    "",
    "            f\"   Please verify training completed successfully.\"",
    "",
    "        )",
    "",
    "    ",
    "",
    "    print(f\"\u2705 Found trained model: {TRAINED_MODEL_PATH}\")",
    "",
    "    print(f\"   \u2022 Run folder: {os.path.basename(latest_run_folder)}\")",
    "",
    "    ",
    "",
    "    # ========================================================================",
    "",
    "    # LOAD MODEL",
    "",
    "    # ========================================================================",
    "",
    "    ",
    "",
    "    print(\"\\n\" + \"-\"*70)",
    "",
    "    print(\"\ud83d\udce5 LOADING TRAINED MODEL\")",
    "",
    "    print(\"-\"*70)",
    "",
    "    ",
    "",
    "    try:",
    "",
    "        model = YOLO(TRAINED_MODEL_PATH)",
    "",
    "        print(\"\u2705 Model loaded successfully\")",
    "",
    "    except Exception as e:",
    "",
    "        raise RuntimeError(",
    "",
    "            f\"\u274c FATAL ERROR: Failed to load model\\n\"",
    "",
    "            f\"   Error: {str(e)}\\n\"",
    "",
    "            f\"   The checkpoint file may be corrupted.\"",
    "",
    "        )",
    "",
    "    ",
    "",
    "    # ========================================================================",
    "",
    "    # SELECT TEST IMAGE",
    "",
    "    # ========================================================================",
    "",
    "    ",
    "",
    "    print(\"\\n\" + \"-\"*70)",
    "",
    "    print(\"\ud83d\uddbc\ufe0f  SELECTING TEST IMAGE\")",
    "",
    "    print(\"-\"*70)",
    "",
    "    ",
    "",
    "    # Get all image paths again",
    "",
    "    test_image_paths = []",
    "",
    "    for ext in image_extensions:",
    "",
    "        test_image_paths.extend(glob.glob(os.path.join(UNLABELED_IMAGES_PATH, ext)))",
    "",
    "    ",
    "",
    "    if not test_image_paths:",
    "",
    "        print(\"\u26a0\ufe0f  No images found for inference\")",
    "",
    "    else:",
    "",
    "        # Use the first image for demonstration",
    "",
    "        inference_image_path = test_image_paths[0]",
    "",
    "        print(f\"\ud83d\udcf8 Test image: {os.path.basename(inference_image_path)}\")",
    "",
    "        ",
    "",
    "        # ====================================================================",
    "",
    "        # RUN INFERENCE",
    "",
    "        # ====================================================================",
    "",
    "        ",
    "",
    "        print(\"\\n\" + \"-\"*70)",
    "",
    "        print(\"\ud83d\ude80 RUNNING INFERENCE\")",
    "",
    "        print(\"-\"*70)",
    "",
    "        ",
    "",
    "        start_time = time.time()",
    "",
    "        ",
    "",
    "        try:",
    "",
    "            results = model(inference_image_path)",
    "",
    "            inference_time = time.time() - start_time",
    "",
    "            ",
    "",
    "            # Get annotated frame",
    "",
    "            annotated_frame = results[0].plot()",
    "",
    "            ",
    "",
    "            # Create output directory",
    "",
    "            os.makedirs(INFERENCE_OUTPUT_PATH, exist_ok=True)",
    "",
    "            ",
    "",
    "            # Save result",
    "",
    "            output_filename = f\"inference_result_{os.path.basename(inference_image_path)}\"",
    "",
    "            output_path = os.path.join(INFERENCE_OUTPUT_PATH, output_filename)",
    "",
    "            cv2.imwrite(output_path, annotated_frame)",
    "",
    "            ",
    "",
    "            # Display results",
    "",
    "            num_detections = len(results[0].boxes)",
    "",
    "            ",
    "",
    "            print(\"\\n\u2705 Inference complete\")",
    "",
    "            print(f\"   \u2022 Detections found: {num_detections}\")",
    "",
    "            print(f\"   \u2022 Inference time: {inference_time:.3f} seconds\")",
    "",
    "            print(f\"   \u2022 Result saved to: {output_path}\")",
    "",
    "            ",
    "",
    "            # Show detection details",
    "",
    "            if num_detections > 0:",
    "",
    "                print(\"\\n\ud83d\udccb Detection Details:\")",
    "",
    "                for i, box in enumerate(results[0].boxes[:10], 1):  # Show first 10",
    "",
    "                    class_id = int(box.cls[0])",
    "",
    "                    confidence = float(box.conf[0])",
    "",
    "                    class_name = model.names[class_id]",
    "",
    "                    print(f\"   {i:2d}. {class_name:<30} (confidence: {confidence:.3f})\")",
    "",
    "                ",
    "",
    "                if num_detections > 10:",
    "",
    "                    print(f\"   ... and {num_detections - 10} more detections\")",
    "",
    "            ",
    "",
    "            # Display image in notebook",
    "",
    "            print(\"\\n\ud83d\uddbc\ufe0f  Displaying annotated result...\\n\")",
    "",
    "            display(Image(filename=output_path, width=800))",
    "",
    "            ",
    "",
    "        except Exception as e:",
    "",
    "            print(f\"\\n\u274c Inference failed: {str(e)}\")",
    "",
    "            raise",
    "",
    "    ",
    "",
    "    print(\"\\n\" + \"=\"*70)",
    "",
    "    print(\"\u2705 INFERENCE COMPLETE\")",
    "",
    "    print(\"=\"*70)",
    "",
    "",
    "",
    "else:",
    "",
    "    print(\"\\n\" + \"=\"*70)",
    "",
    "    print(\"\u23ed\ufe0f  INFERENCE SKIPPED\")",
    "",
    "    print(\"=\"*70)",
    "",
    "    print(\"\\nReason: Training was not completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Complete! \ud83c\udf89\n",
    "\n",
    "### Summary\n",
    "\n",
    "You have successfully completed the CPU-optimized HVAC auto-labeling pipeline:\n",
    "\n",
    "1. \u2705 Environment setup (CPU mode)\n",
    "2. \u2705 Configuration and logging\n",
    "3. \u2705 Ontology generation from templates\n",
    "4. \u2705 Auto-labeling with Grounded-SAM-2 (CPU)\n",
    "5. \u2705 Quality review and validation\n",
    "6. \u2705 YOLOv8 model training (CPU)\n",
    "7. \u2705 Inference testing\n",
    "\n",
    "### Output Files\n",
    "\n",
    "Check your output directories for:\n",
    "- **Auto-labeled Dataset**: YOLO format annotations and images\n",
    "- **Training Checkpoints**: `weights/best.pt` and `weights/last.pt`\n",
    "- **Training Metrics**: Loss curves, precision, recall, mAP\n",
    "- **Inference Results**: Annotated images with predictions\n",
    "- **Pipeline Logs**: Detailed execution logs with timestamps\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Review Quality**: Inspect auto-labeled annotations for accuracy\n",
    "2. **Evaluate Model**: Check training metrics and validation results\n",
    "3. **Test Inference**: Run model on additional blueprint images\n",
    "4. **Iterate**: Adjust parameters and retrain if needed\n",
    "5. **Production**: Deploy model for HVAC symbol detection\n",
    "\n",
    "### CPU vs GPU Performance Comparison\n",
    "\n",
    "| Phase | CPU Time | GPU Time | Ratio |\n",
    "|-------|----------|----------|-------|\n",
    "| Auto-labeling (5 images) | 1-5 min | 15-30 sec | 5-10x |\n",
    "| Training (50 epochs) | 2-4 hours | 5-15 min | 10-20x |\n",
    "| Inference (per image) | 1-5 sec | 0.1-0.5 sec | 5-10x |\n",
    "\n",
    "### When to Use CPU vs GPU\n",
    "\n",
    "**Use CPU Version When**:\n",
    "- No GPU available\n",
    "- Small datasets (< 10 images)\n",
    "- Development and testing\n",
    "- Budget constraints\n",
    "- Learning the pipeline\n",
    "\n",
    "**Use GPU Version When**:\n",
    "- Large datasets (50+ images)\n",
    "- Production training\n",
    "- Fast iteration needed\n",
    "- Multiple training runs\n",
    "- Time is critical\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Autodistill Documentation](https://docs.autodistill.com/)\n",
    "- [YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [Grounded-SAM-2 GitHub](https://github.com/autodistill/autodistill-grounded-sam-2)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Auto-Labeling! \ud83d\ude80**\n",
    "\n",
    "*For GPU-accelerated processing, use the GPU version of this notebook: `autodistill_hvac_grounded_sam2.ipynb`*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "CPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}