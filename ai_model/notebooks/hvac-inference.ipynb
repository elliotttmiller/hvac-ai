{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "# üöÄ HVAC AI - Remote Inference Server (v3 - Production Ready)\n",
        "\n",
        "This notebook acts as a GPU-powered backend server for the `hvac-ai` platform. It clones the repository, installs dependencies, and uses `ngrok` to create a public URL for your local frontend. This version is fully automated and uses a `.env` file for secure configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1_header"
      },
      "source": [
        "### Phase 1: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo_cell"
      },
      "outputs": [],
      "source": [
        "# 1. Clone the GitHub repository\n",
        "!git clone https://github.com/elliotttmiller/hvac-ai.git\n",
        "\n",
        "# 2. Change the current working directory to the cloned repo\n",
        "%cd hvac-ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps_cell"
      },
      "outputs": [],
      "source": [
        "# 3. Install all required Python packages\n",
        "\n",
        "# Web server, tunneling, and configuration dependencies\n",
        "!pip install flask flask-cors pyngrok python-dotenv --quiet\n",
        "\n",
        "# Machine learning and SAM dependencies\n",
        "!pip install torch torchvision --quiet\n",
        "!pip install opencv-python pycocotools matplotlib onnxruntime onnx --quiet\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git --quiet\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2_header"
      },
      "source": [
        "### Phase 2: Create and Configure `.env` File\n",
        "\n",
        "This is the **only configuration step** you need to perform.\n",
        "\n",
        "1.  **Get your `ngrok` authtoken** from your dashboard: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "2.  **Run the cell below**, pasting your `ngrok` token and the correct Google Drive path to your fine-tuned model (`.pth` file) where indicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_env_cell"
      },
      "outputs": [],
      "source": [
        "%%writefile .env\n",
        "# .env file for HVAC AI Inference Server\n",
        "\n",
        "# 1. Paste your secret token from the ngrok dashboard below\n",
        "NGROK_AUTHTOKEN=\"YOUR_NGROK_AUTHTOKEN_HERE\"\n",
        "\n",
        "# 2. Update with the full path to your fine-tuned model in Google Drive\n",
        "MODEL_PATH=\"/content/drive/MyDrive/sam_finetuning_results/best_model_expert_v1.pth\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase3_header"
      },
      "source": [
        "### Phase 3: Authenticate `ngrok`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngrok_auth_cell"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load the environment variables from the .env file we just created\n",
        "load_dotenv()\n",
        "\n",
        "# Get the token\n",
        "ngrok_token = os.getenv(\"NGROK_AUTHTOKEN\")\n",
        "\n",
        "if ngrok_token and \"YOUR_NGROK_AUTHTOKEN_HERE\" not in ngrok_token:\n",
        "    print(\"NGROK_AUTHTOKEN loaded successfully. Authenticating...\")\n",
        "    !ngrok authtoken {ngrok_token}\n",
        "else:\n",
        "    print(\"‚ùå ERROR: NGROK_AUTHTOKEN not found or not updated in the .env file.\")\n",
        "    print(\"üëâ Please edit the cell in Phase 2 with your real token and run it again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase4_header"
      },
      "source": [
        "### Phase 4: Launch the Inference Server\n",
        "\n",
        "This final cell will mount your Google Drive, create the public URL, and start your backend server. The `app.py` script in the repository should already be configured to read the `.env` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_server_cell"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# --- 1. Mount Google Drive to access the model file ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive mounted.\")\n",
        "\n",
        "# --- 2. Set up the ngrok tunnel ---\n",
        "PORT = 5000\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(PORT).public_url\n",
        "print(\"-\"*50)\n",
        "print(f\"‚úÖ Your inference server is live at: {public_url}\")\n",
        "print(\"üëâ Use this URL as the API endpoint in your local frontend application.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# --- 3. Run the Flask Application from the CORRECT PATH ---\n",
        "# This cell will run continuously and display live logs from your server.\n",
        "print(\"\\nüöÄ Starting the backend server... (Logs will appear below)\")\n",
        "!python python-services/hvac_analysis_service/app.py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}