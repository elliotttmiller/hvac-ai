{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ HVAC AI ‚Äî Production-Ready YOLOv11-OBB Pipeline\n",
        "**End-to-End Oriented Bounding Box (OBB) Analysis & Inference Server**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Overview\n",
        "This notebook deploys a high-performance API for HVAC blueprint analysis.\n",
        "\n",
        "### üåü Configuration\n",
        "- **Endpoint**: `/api/v1/analyze/stream` (POST)\n",
        "- **Format**: YOLOv11-OBB (Rotated Bounding Boxes)\n",
        "- **Optimization**: FP16 (Half-Precision) & CUDA\n",
        "- **Security**: CORS enabled for Frontend access\n",
        "\n",
        "## üéØ Instructions\n",
        "1. Set Runtime to **GPU**.\n",
        "2. Run all cells from top to bottom.\n",
        "3. Copy the **Ngrok Public URL** into your Frontend `.env` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "print(\"üîß Environment Setup\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Install Dependencies\n",
        "print(\"\\nüì¶ Installing libraries...\")\n",
        "!pip install -q ultralytics>=8.3.0 fastapi>=0.115.0 uvicorn[standard]>=0.34.0\n",
        "!pip install -q python-multipart pyngrok>=7.0.0 python-dotenv opencv-python-headless\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "print(f\"\\n‚úÖ Python: {sys.version.split()[0]}\")\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Running on CPU (Slow)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "print(\"‚öôÔ∏è  Pipeline Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- USER CONFIGURATION START ---\n",
        "# Path to your YOLOv11-OBB model\n",
        "MODEL_PATH = \"/content/drive/Shareddrives/HVAC/DECEMBER 24 OUTPUT WEIGHTS {dataset2}/hvac_obb_l_20251224_214011/weights/best.pt\"\n",
        "\n",
        "# Inference Settings\n",
        "CONF_THRESHOLD = 0.50      # Confidence threshold\n",
        "IOU_THRESHOLD = 0.45       # NMS IoU threshold\n",
        "IMG_SIZE = 1024            # Inference image size\n",
        "HALF_PRECISION = torch.cuda.is_available() # Use FP16 if GPU is available\n",
        "\n",
        "# Server Settings\n",
        "PORT = 8000\n",
        "NGROK_AUTHTOKEN = \"36hBoLt4A3L8yOYt96wKiCxxrwp_5wFbj1Frv6GoHARRQ6H6t\" # <--- PASTE TOKEN HERE\n",
        "# --- USER CONFIGURATION END ---\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚ùå ERROR: Model not found at {MODEL_PATH}\")\n",
        "    print(\"   Please update MODEL_PATH to point to your .pt file.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model Path: {MODEL_PATH}\")\n",
        "    print(f\"‚úÖ Config: Conf={CONF_THRESHOLD}, IoU={IOU_THRESHOLD}, FP16={HALF_PRECISION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen_server"
      },
      "outputs": [],
      "source": [
        "print(\"üìù Generating Production Server Code (app.py)...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Updates:\n",
        "# 1. STRICT OBB MODE: Extracts 'xywhr' (Center X, Center Y, Width, Height, Rotation).\n",
        "# 2. Flattens the response: Puts x, y, w, h, r at the top level of the detection object.\n",
        "# 3. Removes generic 'bbox' or 'polygon' keys to prevent frontend confusion.\n",
        "\n",
        "server_code = f\"\"\"\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException, Request\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_PATH = r'{MODEL_PATH}'\n",
        "CONF_THRES = {CONF_THRESHOLD}\n",
        "IOU_THRES = {IOU_THRESHOLD}\n",
        "IMG_SIZE = {IMG_SIZE}\n",
        "HALF = {HALF_PRECISION}\n",
        "\n",
        "# --- LOGGING ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(\"HVAC-Service\")\n",
        "\n",
        "app = FastAPI(title=\"HVAC YOLOv11-OBB Inference API\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "model = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def load_model():\n",
        "    global model\n",
        "    logger.info(f\"Loading OBB model from {{MODEL_PATH}}...\")\n",
        "    try:\n",
        "        model = YOLO(MODEL_PATH)\n",
        "        if torch.cuda.is_available():\n",
        "            model.to('cuda')\n",
        "            logger.info(\"Model loaded on GPU\")\n",
        "        else:\n",
        "            logger.info(\"Model loaded on CPU\")\n",
        "        # Warmup\n",
        "        model.predict(np.zeros((640,640,3), dtype=np.uint8), verbose=False, half=HALF)\n",
        "        logger.info(\"Model Warmup Complete\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model: {{e}}\")\n",
        "        raise RuntimeError(\"Model loading failed\")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {{\"message\": \"HVAC OBB Inference Server Online\", \"docs\": \"/docs\"}}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    if model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Model initializing\")\n",
        "    return {{\"status\": \"healthy\", \"device\": str(model.device), \"task\": \"obb\"}}\n",
        "\n",
        "@app.get(\"/api/v1/analyze/stream\")\n",
        "def analyze_help():\n",
        "    return JSONResponse(status_code=200, content={{\"message\": \"Ready for OBB Inference\"}})\n",
        "\n",
        "@app.post(\"/api/v1/analyze/stream\")\n",
        "async def analyze_image(\n",
        "    request: Request,\n",
        "    file: UploadFile = File(None),\n",
        "    image: UploadFile = File(None)\n",
        "):\n",
        "    if not model:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
        "\n",
        "    target_file = file or image\n",
        "    if not target_file:\n",
        "        raise HTTPException(status_code=422, detail=\"No file uploaded. Use key 'file' or 'image'.\")\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"üì∏ Processing OBB Request: {{target_file.filename}}\")\n",
        "        contents = await target_file.read()\n",
        "        nparr = np.frombuffer(contents, np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if img is None:\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid image format\")\n",
        "\n",
        "        # Inference\n",
        "        results = model.predict(\n",
        "            img,\n",
        "            conf=CONF_THRES,\n",
        "            iou=IOU_THRES,\n",
        "            imgsz=IMG_SIZE,\n",
        "            half=HALF\n",
        "        )\n",
        "\n",
        "        result = results[0]\n",
        "        detections = []\n",
        "\n",
        "        # --- STRICT OBB FORMATTING ---\n",
        "        if hasattr(result, 'obb') and result.obb is not None:\n",
        "            for box in result.obb:\n",
        "                # Extract OBB Data: [x_center, y_center, width, height, rotation]\n",
        "                xywhr = box.xywhr[0].cpu().numpy().tolist()\n",
        "                cls_id = int(box.cls[0].item())\n",
        "                conf = float(box.conf[0].item())\n",
        "                label = result.names[cls_id]\n",
        "\n",
        "                detections.append({{\n",
        "                    \"label\": label,          # Common Frontend Key\n",
        "                    \"class\": label,          # Common Frontend Key\n",
        "                    \"score\": conf,           # Common Frontend Key\n",
        "                    \"confidence\": conf,\n",
        "                    \"type\": \"OBB\",\n",
        "\n",
        "                    # Flattened OBB Data (Easiest for Frontend to read)\n",
        "                    \"x\": xywhr[0],           # Center X\n",
        "                    \"y\": xywhr[1],           # Center Y\n",
        "                    \"w\": xywhr[2],           # Width\n",
        "                    \"h\": xywhr[3],           # Height\n",
        "                    \"rotation\": xywhr[4],    # Rotation (Radians)\n",
        "                    \"r\": xywhr[4],           # Alias for rotation\n",
        "\n",
        "                    # Explicit Object for strict types\n",
        "                    \"obb\": {{\n",
        "                        \"x_center\": xywhr[0],\n",
        "                        \"y_center\": xywhr[1],\n",
        "                        \"width\": xywhr[2],\n",
        "                        \"height\": xywhr[3],\n",
        "                        \"rotation\": xywhr[4]\n",
        "                    }}\n",
        "                }})\n",
        "        else:\n",
        "            logger.warning(\"‚ö†Ô∏è No OBB detections found (or model is not OBB)\")\n",
        "\n",
        "        logger.info(f\"‚úÖ Success: {{len(detections)}} OBB components detected\")\n",
        "\n",
        "        return JSONResponse(content={{\n",
        "            \"success\": True,\n",
        "            \"count\": len(detections),\n",
        "            \"detections\": detections\n",
        "        }})\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Inference error: {{e}}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"‚úÖ Generated app.py successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_server"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import sys\n",
        "from pyngrok import ngrok\n",
        "\n",
        "print(\"üöÄ Launching & Validating Server\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Cleanup\n",
        "ngrok.kill()\n",
        "\n",
        "# 2. Start Uvicorn\n",
        "print(\"‚è≥ Starting Uvicorn process...\")\n",
        "process = subprocess.Popen(\n",
        "    [sys.executable, \"-m\", \"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", str(PORT)],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    encoding='utf-8',\n",
        "    bufsize=1\n",
        ")\n",
        "\n",
        "# 3. Health Check\n",
        "print(\"üè• Checking server health (timeout: 60s)...\\n\")\n",
        "server_ready = False\n",
        "health_url = f\"http://localhost:{PORT}/health\"\n",
        "\n",
        "start_time = time.time()\n",
        "while time.time() - start_time < 60:\n",
        "    try:\n",
        "        response = requests.get(health_url, timeout=1)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"\\n‚úÖ Server is HEALTHY!\")\n",
        "            print(f\"   Status: {data['status']}\")\n",
        "            print(f\"   Device: {data['device']}\")\n",
        "            server_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(2)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è Unexpected error: {e}\")\n",
        "\n",
        "if not server_ready:\n",
        "    print(\"\\n‚ùå Server failed to start.\")\n",
        "    print(process.stdout.read())\n",
        "    process.terminate()\n",
        "    raise RuntimeError(\"Server startup failed\")\n",
        "\n",
        "# 4. Ngrok Tunnel\n",
        "print(\"\\nüåê Initializing Public Tunnel...\")\n",
        "if NGROK_AUTHTOKEN and NGROK_AUTHTOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    try:\n",
        "        ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "        tunnel = ngrok.connect(PORT)\n",
        "        public_url = tunnel.public_url\n",
        "        print(f\"\\nüéâ API IS LIVE at: {public_url}\")\n",
        "        print(f\"   üìÑ Docs: {public_url}/docs\")\n",
        "        print(f\"   üîó Endpoint: {public_url}/api/v1/analyze/stream\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Ngrok Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No Ngrok token. Server local only.\")\n",
        "\n",
        "print(\"\\nüìú Streaming Logs (Press STOP to exit)...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 5. Log Loop\n",
        "try:\n",
        "    while True:\n",
        "        line = process.stdout.readline()\n",
        "        if line:\n",
        "            print(line.strip())\n",
        "        if process.poll() is not None:\n",
        "            print(\"‚ùå Server process terminated.\")\n",
        "            break\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Server stopped by user.\")\n",
        "    process.terminate()\n",
        "    ngrok.kill()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}