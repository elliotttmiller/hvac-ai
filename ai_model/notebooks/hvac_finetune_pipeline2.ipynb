{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbJThT5Ol_lr"
      },
      "source": [
        "# üü£ HVAC-Specific SAM Fine-Tuning Pipeline (Optimized)\n",
        "## üîß Complete Production-Ready Implementation\n",
        "\n",
        "This notebook has been refactored to provide a clean, linear, and robust workflow for fine-tuning the Segment Anything Model (SAM) on your HVAC dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1_header"
      },
      "source": [
        "### Phase 1: Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BiBpFHzYU4Je",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3fd4195-6b79-442a-8e5e-80d9a910c2c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r0oru8hAn6q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0f98dc-2e00-49e4-86ef-b0bd20cba64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "!pip install torch torchvision --quiet\n",
        "!pip install opencv-python pycocotools matplotlib onnxruntime onnx --quiet\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lwmQm0C3n_3D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools import mask as mask_utils\n",
        "\n",
        "# SAM imports\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2_header"
      },
      "source": [
        "### Phase 2: Configuration and Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OdTD9CTxKena",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e22ac48-464f-4045-9d16-b29021652558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Configuration loaded. Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "CONFIG = {\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'model_path': '/content/drive/MyDrive/sam_finetuning_results/best_model.pth',\n",
        "    'dataset_zip_path': '/content/drive/MyDrive/hvac_dataset_final.zip',\n",
        "    'unzip_path': '/content/drive/MyDrive/hvac_dataset_final',\n",
        "    'annotations_file_name': '_annotations.coco.json',\n",
        "    'output_dir': '/content/drive/MyDrive/sam_finetuning_results',\n",
        "    'model_type': 'vit_h',\n",
        "    'image_size': 1024,\n",
        "    'batch_size': 1,\n",
        "    'num_workers': 0,\n",
        "    'num_epochs': 1,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0,\n",
        "    'early_stopping_patience': 10,\n",
        "    'checkpoint_interval': 10, # For end-of-epoch saving\n",
        "    'checkpoint_batch_interval': 300, # ADDED: Save every 200 batches\n",
        "    'min_mask_area': 100,\n",
        "}\n",
        "\n",
        "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úì Configuration loaded. Using device: {CONFIG['device']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset if it hasn't been already\n",
        "if not os.path.exists(CONFIG['unzip_path']):\n",
        "    print(f\"üìÅ Unzipping dataset from {CONFIG['dataset_zip_path']}...\")\n",
        "    with zipfile.ZipFile(CONFIG['dataset_zip_path'], 'r') as zip_ref:\n",
        "        zip_ref.extractall(CONFIG['unzip_path'])\n",
        "    print(\"‚úÖ Unzipping complete.\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset already unzipped.\")"
      ],
      "metadata": {
        "id": "unzip_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe015fa7-394a-4ae3-b7cc-cd461208fa73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset already unzipped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase3_header"
      },
      "source": [
        "### Phase 3: Dataset Loading and DataLoader Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "data_loading_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5f8c27-2c36-41f9-b04e-a7dc050496f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading 'train' annotations from: /content/drive/MyDrive/hvac_dataset_final/train/_annotations.coco.json\n",
            "loading annotations into memory...\n",
            "Done (t=1.41s)\n",
            "creating index...\n",
            "index created!\n",
            "üìä Found 2604 images in 'train'.\n",
            "üîÑ Loading 'valid' annotations from: /content/drive/MyDrive/hvac_dataset_final/valid/_annotations.coco.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.54s)\n",
            "creating index...\n",
            "index created!\n",
            "üìä Found 351 images in 'valid'.\n",
            "\n",
            "‚úÖ Training dataset initialized with 2604 samples.\n",
            "‚úÖ Validation dataset initialized with 351 samples.\n"
          ]
        }
      ],
      "source": [
        "def load_coco_split(dataset_root_path: str, split_name: str, annotations_file: str) -> Tuple[COCO, str]:\n",
        "    \"\"\"Loads a specific split of a COCO dataset.\"\"\"\n",
        "    split_path = os.path.join(dataset_root_path, split_name)\n",
        "    annotations_path = os.path.join(split_path, annotations_file)\n",
        "    if not os.path.exists(annotations_path):\n",
        "        raise FileNotFoundError(f\"Annotations file not found for '{split_name}' at: {annotations_path}\")\n",
        "    print(f\"üîÑ Loading '{split_name}' annotations from: {annotations_path}\")\n",
        "    coco = COCO(annotations_path)\n",
        "    print(f\"üìä Found {len(coco.getImgIds())} images in '{split_name}'.\")\n",
        "    return coco, split_path\n",
        "\n",
        "def get_image_path(split_path: str, img_info: dict) -> str:\n",
        "    \"\"\"Constructs the full path to an image file.\"\"\"\n",
        "    full_path = os.path.join(split_path, img_info['file_name'])\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Image file not found: {img_info['file_name']} in {split_path}\")\n",
        "    return full_path\n",
        "\n",
        "class HvacSamDataset(Dataset):\n",
        "    def __init__(self, coco: COCO, image_ids: List[int], split_path: str):\n",
        "        self.coco = coco\n",
        "        self.image_ids = image_ids\n",
        "        self.split_path = split_path\n",
        "        self.resize_transform = ResizeLongestSide(CONFIG['image_size'])\n",
        "        self.pixel_mean = torch.tensor([123.675, 116.28, 103.53]).view(-1, 1, 1)\n",
        "        self.pixel_std = torch.tensor([58.395, 57.12, 57.375]).view(-1, 1, 1)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_info = self.coco.loadImgs([img_id])[0]\n",
        "        image_path = get_image_path(self.split_path, img_info)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
        "        annotations = self.coco.loadAnns(ann_ids)\n",
        "        masks, bboxes = [], []\n",
        "\n",
        "        for ann in annotations:\n",
        "            if 'segmentation' not in ann or ann.get('iscrowd', 0) == 1: continue\n",
        "            mask = self.coco.annToMask(ann)\n",
        "            if mask.sum() < CONFIG['min_mask_area']: continue\n",
        "            masks.append(mask.astype(bool))\n",
        "            bboxes.append(ann['bbox'])\n",
        "\n",
        "        original_size = image.shape[:2]\n",
        "        resized_image = self.resize_transform.apply_image(image)\n",
        "        input_image_torch = torch.as_tensor(resized_image, dtype=torch.float32).permute(2, 0, 1).contiguous()\n",
        "        input_image_torch = (input_image_torch - self.pixel_mean) / self.pixel_std\n",
        "\n",
        "        h, w = input_image_torch.shape[-2:]\n",
        "        padh, padw = CONFIG['image_size'] - h, CONFIG['image_size'] - w\n",
        "        input_image_padded = torch.nn.functional.pad(input_image_torch, (0, padw, 0, padh))\n",
        "\n",
        "        return {\n",
        "            'image': input_image_padded,\n",
        "            'masks': masks,\n",
        "            'bboxes': bboxes,\n",
        "            'original_size': original_size,\n",
        "            'input_size': (h, w)\n",
        "        }\n",
        "\n",
        "def custom_collate_fn(batch: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"Handles batches with variable numbers of masks/bboxes per image.\"\"\"\n",
        "    return {\n",
        "        'image': torch.stack([item['image'] for item in batch]),\n",
        "        'masks': [item['masks'] for item in batch],\n",
        "        'bboxes': [item['bboxes'] for item in batch],\n",
        "        'original_size': [item['original_size'] for item in batch],\n",
        "        'input_size': [item['input_size'] for item in batch]\n",
        "    }\n",
        "\n",
        "# Load datasets\n",
        "train_coco, train_path = load_coco_split(CONFIG['unzip_path'], 'train', CONFIG['annotations_file_name'])\n",
        "val_coco, val_path = load_coco_split(CONFIG['unzip_path'], 'valid', CONFIG['annotations_file_name'])\n",
        "\n",
        "train_ids = train_coco.getImgIds()\n",
        "val_ids = val_coco.getImgIds()\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = HvacSamDataset(train_coco, train_ids, train_path)\n",
        "val_dataset = HvacSamDataset(val_coco, val_ids, val_path)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True,\n",
        "                        num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
        "                      num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
        "\n",
        "print(f\"\\n‚úÖ Training dataset initialized with {len(train_dataset)} samples.\")\n",
        "print(f\"‚úÖ Validation dataset initialized with {len(val_dataset)} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Define URL for the official ViT-H model\n",
        "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
        "CHECKPOINT_PATH = \"/content/sam_vit_h_4b8939.pth\"\n",
        "\n",
        "print(f\"‚¨áÔ∏è Downloading official SAM ViT-H weights to {CHECKPOINT_PATH}...\")\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    urllib.request.urlretrieve(CHECKPOINT_URL, CHECKPOINT_PATH)\n",
        "    print(\"‚úÖ Download complete.\")\n",
        "else:\n",
        "    print(\"‚úÖ File already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du1oZDKE6GLF",
        "outputId": "069b94ef-f8c9-4001-bb50-3ce42ab09189"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading official SAM ViT-H weights to /content/sam_vit_h_4b8939.pth...\n",
            "‚úÖ Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase4_header"
      },
      "source": [
        "### Phase 4: Model Preparation and Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HjTIJtLxP8ZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a032eb4-39a4-45b8-d6e8-27b9e927bd65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading SAM model from: /content/sam_vit_h_4b8939.pth\n",
            "‚úÖ Successfully loaded 'vit_h' model.\n",
            "‚úÖ SAM model configured for fine-tuning. Trainable parameters: 4,058,340\n"
          ]
        }
      ],
      "source": [
        "# --- UPDATE CONFIG PATH ---\n",
        "CONFIG['model_path'] = \"/content/sam_vit_h_4b8939.pth\"  # Point to the fresh download\n",
        "\n",
        "def load_sam_model(model_path: str, model_type: str) -> nn.Module:\n",
        "    \"\"\"Loads a SAM model from a checkpoint.\"\"\"\n",
        "    print(f\"üîÑ Loading SAM model from: {model_path}\")\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"‚ùå Model checkpoint not found at: {model_path}. Did you run the download cell?\")\n",
        "\n",
        "    try:\n",
        "        # Load the model\n",
        "        sam = sam_model_registry[model_type](checkpoint=model_path)\n",
        "        print(f\"‚úÖ Successfully loaded '{model_type}' model.\")\n",
        "        return sam.to(device=CONFIG['device'])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL ERROR: The model file is corrupted: {e}\")\n",
        "        print(\"üëâ Please re-run the 'Download Official Weights' cell above.\")\n",
        "        raise e\n",
        "\n",
        "# Load the model\n",
        "sam_model = load_sam_model(CONFIG['model_path'], CONFIG['model_type'])\n",
        "\n",
        "# Configure model for fine-tuning (freeze encoders)\n",
        "sam_model.train()\n",
        "for name, param in sam_model.named_parameters():\n",
        "    if name.startswith(\"image_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "        param.requires_grad = False\n",
        "\n",
        "trainable_params = sum(p.numel() for p in sam_model.parameters() if p.requires_grad)\n",
        "print(f\"‚úÖ SAM model configured for fine-tuning. Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtPYpirbK3Wi"
      },
      "outputs": [],
      "source": [
        "# Setup optimizer, scheduler, and loss function\n",
        "optimizer = torch.optim.Adam(\n",
        "    sam_model.mask_decoder.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay']\n",
        ")\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "def combined_loss(pred_masks: torch.Tensor, true_masks: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Combines BCE and Dice loss for better segmentation performance.\"\"\"\n",
        "    bce_loss = nn.BCEWithLogitsLoss()(pred_masks, true_masks.float())\n",
        "\n",
        "    pred_flat = torch.sigmoid(pred_masks).reshape(-1)\n",
        "    true_flat = true_masks.reshape(-1)\n",
        "    intersection = (pred_flat * true_flat).sum()\n",
        "    dice_loss = 1 - (2. * intersection + 1e-8) / (pred_flat.sum() + true_flat.sum() + 1e-8)\n",
        "\n",
        "    return 0.8 * bce_loss + 0.2 * dice_loss\n",
        "\n",
        "print(\"‚úÖ Optimizer and loss function configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase5_header"
      },
      "source": [
        "### Phase 5: Training and Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRQ6yd_PM_B9"
      },
      "outputs": [],
      "source": [
        "def run_epoch(model, dataloader, optimizer, is_training, device, epoch):\n",
        "    model.train(is_training)\n",
        "    epoch_losses = []\n",
        "    iou_scores, dice_scores = [], []\n",
        "\n",
        "    desc = \"Training\" if is_training else \"Validation\"\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=desc)):\n",
        "        images = batch['image'].to(device, non_blocking=True)\n",
        "        all_gt_masks_list = batch['masks']\n",
        "        all_bboxes_list = batch['bboxes']\n",
        "\n",
        "        batch_loss = 0\n",
        "\n",
        "        with torch.set_grad_enabled(is_training):\n",
        "            with torch.no_grad():\n",
        "                image_embeddings = model.image_encoder(images)\n",
        "\n",
        "            for i in range(len(all_gt_masks_list)):\n",
        "                if not all_gt_masks_list[i]: continue\n",
        "\n",
        "                gt_mask_np = all_gt_masks_list[i][0]\n",
        "                bbox_np = all_bboxes_list[i][0]\n",
        "\n",
        "                gt_mask_torch = torch.from_numpy(gt_mask_np).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "                transform = ResizeLongestSide(CONFIG['image_size'])\n",
        "                box_torch = torch.as_tensor(transform.apply_boxes(np.array(bbox_np).reshape(1, 4), batch['original_size'][i]),\n",
        "                                          dtype=torch.float, device=device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    sparse_embeddings, dense_embeddings = model.prompt_encoder(points=None, boxes=box_torch, masks=None)\n",
        "\n",
        "                low_res_masks, iou_predictions = model.mask_decoder(\n",
        "                    image_embeddings=image_embeddings[i].unsqueeze(0),\n",
        "                    image_pe=model.prompt_encoder.get_dense_pe(),\n",
        "                    sparse_prompt_embeddings=sparse_embeddings,\n",
        "                    dense_prompt_embeddings=dense_embeddings,\n",
        "                    multimask_output=False,\n",
        "                )\n",
        "\n",
        "                upscaled_masks = model.postprocess_masks(low_res_masks, batch['input_size'][i], batch['original_size'][i])\n",
        "\n",
        "                loss = combined_loss(upscaled_masks, gt_mask_torch)\n",
        "                batch_loss += loss\n",
        "\n",
        "                if not is_training:\n",
        "                    pred_mask = torch.sigmoid(upscaled_masks) > 0.5\n",
        "                    pred_mask_np = pred_mask.cpu().numpy().squeeze().astype(np.uint8)\n",
        "                    gt_mask_np_uint8 = gt_mask_np.astype(np.uint8)\n",
        "                    rle_pred = mask_utils.encode(np.asfortranarray(pred_mask_np))\n",
        "                    rle_gt = mask_utils.encode(np.asfortranarray(gt_mask_np_uint8))\n",
        "                    h, w = batch['original_size'][i]\n",
        "                    empty_rle = {'size': [h, w], 'counts': ''}\n",
        "                    if rle_pred is None: rle_pred = empty_rle\n",
        "                    if rle_gt is None: rle_gt = empty_rle\n",
        "                    iou_scores.append(mask_utils.iou([rle_pred], [rle_gt], [0])[0][0])\n",
        "                    intersection = torch.logical_and(pred_mask.squeeze(), gt_mask_torch.squeeze()).sum().item()\n",
        "                    total = pred_mask.sum().item() + gt_mask_torch.sum().item()\n",
        "                    dice_scores.append((2 * intersection) / total if total > 0 else 0)\n",
        "\n",
        "        if is_training and batch_loss > 0:\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(batch_loss.item() / len(all_gt_masks_list))\n",
        "        elif not is_training and batch_loss > 0:\n",
        "            epoch_losses.append(batch_loss.item() / len(all_gt_masks_list))\n",
        "\n",
        "        # --- ADDED: Intra-epoch checkpoint saving logic ---\n",
        "        if is_training and (batch_idx + 1) % CONFIG['checkpoint_batch_interval'] == 0:\n",
        "            chk_path = os.path.join(CONFIG['output_dir'], f'checkpoint_epoch_{epoch+1}_batch_{batch_idx+1}.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'batch_idx': batch_idx,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, chk_path)\n",
        "            # Use .write to avoid interfering with tqdm progress bar\n",
        "            tqdm.write(f\"\\nüíæ Intra-epoch checkpoint saved: {chk_path}\")\n",
        "\n",
        "    return {\n",
        "        'loss': mean(epoch_losses) if epoch_losses else 0,\n",
        "        'iou': mean(iou_scores) if iou_scores else 0,\n",
        "        'dice': mean(dice_scores) if dice_scores else 0\n",
        "    }\n",
        "\n",
        "best_val_iou = 0\n",
        "patience_counter = 0\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_iou': [], 'val_dice': []}\n",
        "\n",
        "print(f\"\\nüöÄ Starting training for {CONFIG['num_epochs']} epochs...\")\n",
        "for epoch in range(CONFIG['num_epochs']):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{CONFIG['num_epochs']} ---\")\n",
        "\n",
        "    train_metrics = run_epoch(sam_model, train_loader, optimizer, is_training=True, device=CONFIG['device'], epoch=epoch)\n",
        "    history['train_loss'].append(train_metrics['loss'])\n",
        "\n",
        "    val_metrics = run_epoch(sam_model, val_loader, None, is_training=False, device=CONFIG['device'], epoch=epoch)\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['val_iou'].append(val_metrics['iou'])\n",
        "    history['val_dice'].append(val_metrics['dice'])\n",
        "\n",
        "    print(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
        "    print(f\"Val Loss: {val_metrics['loss']:.4f} | Val IoU: {val_metrics['iou']:.4f} | Val Dice: {val_metrics['dice']:.4f}\")\n",
        "\n",
        "    scheduler.step(val_metrics['loss'])\n",
        "\n",
        "    if val_metrics['iou'] > best_val_iou:\n",
        "        best_val_iou = val_metrics['iou']\n",
        "        patience_counter = 0\n",
        "        best_model_path = os.path.join(CONFIG['output_dir'], 'best_model.pth')\n",
        "        torch.save(sam_model.state_dict(), best_model_path)\n",
        "        print(f\"üèÜ New best model saved with IoU: {best_val_iou:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= CONFIG['early_stopping_patience']:\n",
        "        print(f\"üõë Early stopping triggered after {patience_counter} epochs with no improvement.\")\n",
        "        break\n",
        "\n",
        "    if (epoch + 1) % CONFIG['checkpoint_interval'] == 0:\n",
        "        chk_path = os.path.join(CONFIG['output_dir'], f'checkpoint_epoch_{epoch+1}.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': sam_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': val_metrics['loss'],\n",
        "        }, chk_path)\n",
        "        print(f\"üíæ End-of-epoch checkpoint saved: {chk_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase6_header"
      },
      "source": [
        "### Phase 6: Results and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKqIxUgAOTzp_1"
      },
      "outputs": [],
      "source": [
        "# Plot training metrics\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax1.plot(history['train_loss'], label='Training Loss', color='blue')\n",
        "ax1.plot(history['val_loss'], label='Validation Loss', color='red')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history['val_iou'], label='Validation IoU', color='green')\n",
        "ax2.plot(history['val_dice'], label='Validation Dice', color='purple')\n",
        "ax2.set_title('Validation Metrics (IoU & Dice)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Score')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CONFIG['output_dir'], 'training_metrics.png'))\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä FINAL METRICS:\")\n",
        "print(f\"Best Validation IoU: {max(history['val_iou']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKqIxUgAOTzp_4"
      },
      "outputs": [],
      "source": [
        "# Save the final trained model (or the best one)\n",
        "# Note: The 'best_model.pth' is already saved during training.\n",
        "# You can also save the final epoch's model if desired.\n",
        "final_model_path = os.path.join(CONFIG['output_dir'], 'final_model.pth')\n",
        "torch.save(sam_model.state_dict(), final_model_path)\n",
        "print(f\"‚úÖ Final model state saved to: {final_model_path}\")\n",
        "print(f\"‚úÖ Best performing model (by IoU) saved to: {os.path.join(CONFIG['output_dir'], 'best_model.pth')}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Phase 7: Final, Unbiased Evaluation on the Test Set\n",
        "\n",
        "print(\"\\n--- Final Model Evaluation on Unseen Test Data ---\")\n",
        "\n",
        "# 1. Load the best performing model that was saved during training\n",
        "best_model_path = os.path.join(CONFIG['output_dir'], 'best_model.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"üîÑ Loading best model from: {best_model_path}\")\n",
        "    # We need to re-initialize the model structure before loading the state dict\n",
        "    sam_model = sam_model_registry[CONFIG['model_type']]()\n",
        "    sam_model.load_state_dict(torch.load(best_model_path))\n",
        "    sam_model.to(CONFIG['device'])\n",
        "else:\n",
        "    print(\"‚ùå Best model file not found. Cannot perform final evaluation.\")\n",
        "    # You might want to handle this case, but for now we'll assume it exists\n",
        "\n",
        "# 2. Load the test dataset\n",
        "try:\n",
        "    test_coco, test_path = load_coco_split(CONFIG['unzip_path'], 'test', CONFIG['annotations_file_name'])\n",
        "    test_ids = test_coco.getImgIds()\n",
        "\n",
        "    test_dataset = HvacSamDataset(test_coco, test_ids, test_path)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
        "                             num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
        "\n",
        "    print(f\"\\n‚úÖ Test dataset loaded with {len(test_dataset)} samples.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Test split not found: {e}. Skipping final evaluation.\")\n",
        "\n",
        "# 3. Run a single evaluation pass on the test data\n",
        "if 'test_loader' in locals():\n",
        "    print(\"\\nüöÄ Running final evaluation on the test set...\")\n",
        "    final_test_metrics = run_epoch(sam_model, test_loader, optimizer=None, is_training=False, device=CONFIG['device'], epoch=0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"      üéâ FINAL UNBIASED PERFORMANCE METRICS üéâ\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Final Test IoU:   {final_test_metrics['iou']:.4f}\")\n",
        "    print(f\"Final Test Dice:  {final_test_metrics['dice']:.4f}\")\n",
        "    print(f\"Final Test Loss:  {final_test_metrics['loss']:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nThis is the true expected performance of your model on new data.\")"
      ],
      "metadata": {
        "id": "anl9NMkohmRj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}