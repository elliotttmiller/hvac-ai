{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITIVE & CORRECT - HVAC Dataset Optimization Pipeline\n",
    "\n",
    "**Objective**: This notebook uses a direct, case-sensitive mapping built from the successful diagnostic to finally and correctly process the **original** `hvac-dataset.zip` into 6 universal categories."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install -q pycocotools tqdm"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, json, shutil, zipfile, logging\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Any, Optional\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration (Using Your Exact Category List)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# This MUST point to your ORIGINAL, multi-class dataset zip file.\n",
    "INPUT_ZIP_PATH = Path(\"/content/drive/MyDrive/hvac-dataset.zip\") \n",
    "RAW_DATA_DIR = Path(\"/content/hvac-dataset-raw\")\n",
    "OPTIMIZED_DATA_DIR = Path(\"/content/drive/MyDrive/hvac-dataset-optimized\")\n",
    "MIN_ANNOTATION_AREA = 25\n",
    "\n",
    "# *** FINAL MAPPING: Built from your exact, case-sensitive diagnostic output ***\n",
    "CATEGORY_MAPPING = {\n",
    "    \"Equipment\": [\"Coil\", \"Fan\", \"e_motor_3p\", \"centrifugal-pump\", \"scroll_compressosr\", \"tank\", \"finned_tubes_HE\"],\n",
    "    \"Ductwork\": [\"duct\", \"bend\", \"reducer\"],\n",
    "    \"Piping\": [\"Pipe-Insulated\", \"u_trap\"],\n",
    "    \"Valves\": [\"BDV\", \"Plug Valve\", \"SDV\", \"Valve-Ball\", \"Valve-Butterfly\", \"Valve-Check\", \"Valve-Control\", \"Valve-Gate\", \"Valve-Globe\", \"Valve-Needle\", \"Valve-SafetyRelief\", \"Valve-ThreeWay\", \"ball w insulate\", \"ball_valve_with_schrader\", \"ex_valve\"],\n",
    "    \"Air Devices\": [\"Damper\", \"Filter\", \"fire-damper\", \"fire-detector\", \"smoke detector\"],\n",
    "    \"Controls\": [\"Sensor-Temperature\", \"filter_drier\", \"maintainance-switch\", \"pressure sensor\", \"schraeder_vavle\", \"sight_glass\", \"solenoid_valve\", \"vd\"]\n",
    "}\n",
    "\n",
    "# --- Build a Direct, Case-Sensitive Reverse Map --- \n",
    "REVERSE_CATEGORY_MAP = {}\n",
    "for simp_cat, orig_cats in CATEGORY_MAPPING.items():\n",
    "    for orig_cat in orig_cats:\n",
    "        REVERSE_CATEGORY_MAP[orig_cat] = simp_cat # Direct 1-to-1 mapping\n",
    "\n",
    "SIMPLIFIED_CATEGORIES = list(CATEGORY_MAPPING.keys())\n",
    "SIMP_CAT_NAME_TO_ID = {name: i for i, name in enumerate(SIMPLIFIED_CATEGORIES)}\n",
    "\n",
    "logger.info(\"Configuration loaded with DIRECT, case-sensitive mapping.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Extraction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if RAW_DATA_DIR.exists(): shutil.rmtree(RAW_DATA_DIR)\n",
    "with zipfile.ZipFile(INPUT_ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(RAW_DATA_DIR)\n",
    "logger.info(f\"Successfully extracted ORIGINAL dataset to {RAW_DATA_DIR}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Core Optimization Logic (Simplified & Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DatasetOptimizer:\n",
    "    def __init__(self, raw_dir: Path, optimized_dir: Path):\n",
    "        self.raw_dir = raw_dir\n",
    "        self.optimized_dir = optimized_dir\n",
    "        self.stats = defaultdict(lambda: defaultdict(int))\n",
    "        if self.optimized_dir.exists(): shutil.rmtree(self.optimized_dir)\n",
    "        self.optimized_dir.mkdir(parents=True)\n",
    "\n",
    "    def run(self):\n",
    "        for item in self.raw_dir.iterdir():\n",
    "            if item.is_dir() and item.name in ['train', 'valid', 'test']:\n",
    "                logger.info(f\"--- Processing split: {item.name} ---\")\n",
    "                self._process_split(item)\n",
    "        self._print_summary_stats()\n",
    "\n",
    "    def _process_split(self, raw_split_dir: Path):\n",
    "        split_name = raw_split_dir.name\n",
    "        optimized_split_dir = self.optimized_dir / split_name\n",
    "        optimized_split_dir.mkdir()\n",
    "        ann_file = next(raw_split_dir.glob('*_annotations.coco.json'), None)\n",
    "        if not ann_file: return\n",
    "\n",
    "        with open(ann_file) as f: coco_data = json.load(f)\n",
    "        original_images = {img['id']: img for img in coco_data['images']}\n",
    "        original_anns = defaultdict(list)\n",
    "        for ann in coco_data['annotations']: original_anns[ann['image_id']].append(ann)\n",
    "        original_cats = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "        optimized_coco = {\"images\": [], \"annotations\": [], \"categories\": self._build_simplified_categories()}\n",
    "        ann_id_counter = 1\n",
    "\n",
    "        for img_id, img_info in tqdm(original_images.items(), desc=f\"Optimizing {split_name}\"):\n",
    "            refined_annotations = []\n",
    "            for ann in original_anns.get(img_id, []):\n",
    "                refined_ann = self._refine_annotation(ann, img_info, original_cats, split_name)\n",
    "                if refined_ann:\n",
    "                    refined_ann['id'] = ann_id_counter\n",
    "                    refined_annotations.append(refined_ann)\n",
    "                    ann_id_counter += 1\n",
    "            \n",
    "            if refined_annotations:\n",
    "                optimized_coco[\"images\"].append(img_info)\n",
    "                optimized_coco[\"annotations\"].extend(refined_annotations)\n",
    "                shutil.copy(raw_split_dir / img_info['file_name'], optimized_split_dir)\n",
    "\n",
    "        with open(optimized_split_dir / \"_annotations.coco.json\", 'w') as f:\n",
    "            json.dump(optimized_coco, f, indent=2)\n",
    "\n",
    "    def _refine_annotation(self, ann: Dict, img_info: Dict, original_cats: Dict, split: str) -> Optional[Dict]:\n",
    "        # *** FINAL FIX: Direct, case-sensitive lookup with a strip() for safety ***\n",
    "        orig_cat_name = original_cats.get(ann['category_id'])\n",
    "        if not orig_cat_name: return None\n",
    "        \n",
    "        simp_cat_name = REVERSE_CATEGORY_MAP.get(orig_cat_name.strip())\n",
    "        if not simp_cat_name: return None\n",
    "\n",
    "        if 'segmentation' not in ann or not ann['segmentation']: return None\n",
    "        h, w = img_info['height'], img_info['width']\n",
    "        rle = mask_utils.frPyObjects(ann['segmentation'], h, w)\n",
    "        mask = mask_utils.decode(rle)\n",
    "        if mask.ndim == 3: mask = np.any(mask, axis=2)\n",
    "        rle = mask_utils.encode(np.asfortranarray(mask))\n",
    "        area = mask_utils.area(rle).sum()\n",
    "        if area < MIN_ANNOTATION_AREA: return None\n",
    "        bbox = mask_utils.toBbox(rle).flatten().tolist()\n",
    "        self.stats[split][simp_cat_name] += 1\n",
    "        return {\n",
    "            \"image_id\": ann['image_id'], \"category_id\": SIMP_CAT_NAME_TO_ID[simp_cat_name],\n",
    "            \"segmentation\": ann['segmentation'], \"bbox\": bbox, \"area\": float(area), \"iscrowd\": 0\n",
    "        }\n",
    "\n",
    "    def _build_simplified_categories(self) -> List[Dict]:\n",
    "        return [{\"id\": i, \"name\": name, \"supercategory\": \"hvac\"} for i, name in enumerate(SIMPLIFIED_CATEGORIES)]\n",
    "\n",
    "    def _print_summary_stats(self):\n",
    "        logger.info(\"\\n\" + \"=\"*25 + \" FINAL OPTIMIZATION SUMMARY \" + \"=\"*25)\n",
    "        grand_total = Counter()\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            if split in self.stats:\n",
    "                print(f\"\\n--- Split: {split} ---\")\n",
    "                counts = self.stats[split]\n",
    "                for cat, count in sorted(counts.items()): print(f\"{cat:<20}: {count} annotations\")\n",
    "                grand_total.update(counts)\n",
    "        print(\"\\n--- Grand Total Across All Splits ---\")\n",
    "        for cat, count in sorted(grand_total.items()): print(f\"{cat:<20}: {count} annotations\")\n",
    "        logger.info(\"=\"*76 + \"\\n\")\n",
    "\n",
    "print(\"✓ Final optimizer logic defined.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Execute Pipeline & Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optimizer = DatasetOptimizer(RAW_DATA_DIR, OPTIMIZED_DATA_DIR)\n",
    "optimizer.run()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Final Export & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "output_zip_file = OPTIMIZED_DATA_DIR.with_suffix('.zip')\n",
    "logger.info(f\"Creating final CORRECTED ZIP archive at: {output_zip_file}\")\n",
    "if output_zip_file.exists():\n",
    "    os.remove(output_zip_file)\n",
    "shutil.make_archive(str(OPTIMIZED_DATA_DIR), 'zip', str(OPTIMIZED_DATA_DIR))\n",
    "logger.info(\"✓ Archiving complete!\")\n",
    "print(\"\\n---\")\n",
    "print(\"✅ PIPELINE FINISHED SUCCESSFULLY\")\n",
    "print(f\"A new, correct ZIP file has been created at: {output_zip_file}\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}