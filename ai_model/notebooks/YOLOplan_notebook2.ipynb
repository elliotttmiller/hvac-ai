{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "MQzkg4QzqsvU",
      "metadata": {
        "id": "MQzkg4QzqsvU"
      },
      "source": [
        "# YOLOplan Pipeline with Centralized Configuration\n",
        "\n",
        "This notebook provides a complete and easy-to-manage pipeline to train a YOLOplan model. All major settings are controlled from the **Master Configuration** cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "smCllyRvqsvW",
      "metadata": {
        "id": "smCllyRvqsvW"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "This first step clones the YOLOplan repository, installs all dependencies, and verifies the GPU. You only need to run this section once per session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0i8jYaNgtiNL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i8jYaNgtiNL",
        "outputId": "ccef69ca-1063-4e1e-894c-8a9b22c8e129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "hK6eMggqqsvW",
      "metadata": {
        "collapsed": true,
        "id": "hK6eMggqqsvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f83998-6858-4cdb-a9ee-28e997f7fc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YOLOplan'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 70 (delta 21), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (70/70), 63.47 KiB | 9.07 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.241)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Ultralytics Version: 8.3.241 (Should be 8.3.0 or higher for YOLO11)\n",
            "Tue Dec 23 00:23:00 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Clone the YOLOplan repository and navigate into the directory\n",
        "import os\n",
        "if not os.path.exists('YOLOplan'):\n",
        "    !git clone https://github.com/DynMEP/YOLOplan.git\n",
        "    os.chdir('YOLOplan')\n",
        "else:\n",
        "    os.chdir('YOLOplan')\n",
        "\n",
        "# 1. Install standard dependencies\n",
        "!pip install -r requirements.txt roboflow -q\n",
        "\n",
        "# 2. FORCE UPDATE Ultralytics (CRITICAL for YOLO11 support)\n",
        "!pip install -U ultralytics\n",
        "\n",
        "# Verify GPU and Version\n",
        "import ultralytics\n",
        "print(f\"Ultralytics Version: {ultralytics.__version__} (Should be 8.3.0 or higher for YOLO11)\")\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q_AXbHz-qsvX",
      "metadata": {
        "id": "Q_AXbHz-qsvX"
      },
      "source": [
        "## 2. Master Configuration\n",
        "\n",
        "**Action Required:** Configure your entire pipeline here. Update your Roboflow details, choose your model, and decide whether to enable or disable augmentations. The settings you define in this cell will be used by all subsequent steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Dw13lzOaqsvX",
      "metadata": {
        "id": "Dw13lzOaqsvX"
      },
      "outputs": [],
      "source": [
        "#@title ‚öôÔ∏è Master Configuration Panel (Final Version)\n",
        "#@markdown ---\n",
        "#@markdown ### üîë Roboflow Project Details\n",
        "ROBOFLOW_WORKSPACE_ID = \"elliotttmiller\"  #@param {type:\"string\"}\n",
        "ROBOFLOW_PROJECT_ID = \"hvacai-s3kda\"      #@param {type:\"string\"}\n",
        "ROBOFLOW_VERSIONS = [24, 27, 28]          #@param {type:\"raw\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üíæ Storage & Naming\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/hvac_training\" #@param {type:\"string\"}\n",
        "RUN_NAME = \"yolo11m_run_v8_TEXT_FINETUNE\"   #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üß† Progressive Learning\n",
        "USE_PREVIOUS_WEIGHTS = True     #@param {type:\"boolean\"}\n",
        "PREVIOUS_WEIGHTS_PATH = \"/content/drive/MyDrive/hvac_training/yolo11m_run_v8/weights/best.pt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ‚öñÔ∏è Dataset Splitting & Curation\n",
        "TRAIN_RATIO = 0.8   #@param {type:\"number\"}\n",
        "VAL_RATIO   = 0.15  #@param {type:\"number\"}\n",
        "TEST_RATIO  = 0.05  #@param {type:\"number\"}\n",
        "REQUIRE_SPATIAL_CONTAINMENT = True     #@param {type:\"boolean\"}\n",
        "CONTAINER_CLASSES = [\"instrument_shared_primary\", \"instrument_discrete_primary\", \"instrument_discrete_field\", \"instrument_discrete_aux\"] #@param {type:\"raw\"}\n",
        "REQUIRED_CONTENTS = [\"id_letters\", \"tag_number\"] #@param {type:\"raw\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üöÄ Training Settings\n",
        "MODEL = 'yolo11m.pt'\n",
        "EPOCHS = 20\n",
        "IMG_SIZE = 1280\n",
        "BATCH_SIZE = 4\n",
        "CLS_GAIN = 1.0\n",
        "ENABLE_AUGMENTATIONS = False\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üìä Live Monitoring Suite\n",
        "LOGGER = \"TensorBoard\" #@param [\"TensorBoard\", \"Weights & Biases\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚úÖ Pre-flight Configuration Check\n",
        "import os\n",
        "\n",
        "print(\"--- Checking Required Variables for Training ---\")\n",
        "required_vars = [\n",
        "    'DRIVE_ROOT', 'RUN_NAME', 'MODEL', 'USE_PREVIOUS_WEIGHTS',\n",
        "    'PREVIOUS_WEIGHTS_PATH', 'EPOCHS', 'IMG_SIZE', 'BATCH_SIZE',\n",
        "    'ENABLE_AUGMENTATIONS'\n",
        "]\n",
        "\n",
        "all_vars_exist = True\n",
        "for var in required_vars:\n",
        "    if var not in locals():\n",
        "        print(f\"‚ùå FAILED: The variable '{var}' is missing from memory.\")\n",
        "        all_vars_exist = False\n",
        "    else:\n",
        "        # Optional: Print the value to confirm it's correct\n",
        "        # print(f\"  - {var}: {locals()[var]}\")\n",
        "        pass\n",
        "\n",
        "if all_vars_exist:\n",
        "    print(\"\\n‚úÖ SUCCESS: All configuration variables are loaded correctly.\")\n",
        "    print(\"   You are clear to proceed with Data Unification and Training.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ERROR: Please re-run the 'Master Configuration Panel' (Cell 2) above to fix.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te-YbMKZ2jvI",
        "outputId": "54822fa0-4709-4044-92f5-0a0666722756"
      },
      "id": "Te-YbMKZ2jvI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Checking Required Variables for Training ---\n",
            "\n",
            "‚úÖ SUCCESS: All configuration variables are loaded correctly.\n",
            "   You are clear to proceed with Data Unification and Training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6jnE3n7tqsvY",
      "metadata": {
        "id": "6jnE3n7tqsvY"
      },
      "source": [
        "## 3. Download Dataset from Roboflow\n",
        "\n",
        "This cell uses the configuration you provided above to download the correct dataset from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "P_uXuxmdqsvY",
      "metadata": {
        "id": "P_uXuxmdqsvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8237c23b-2165-40a8-89c1-9a4869f9f86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "\n",
            "‚¨áÔ∏è Downloading V24...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in hvacai-24 to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1035642/1035642 [01:05<00:00, 15696.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to hvacai-24 in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15227/15227 [00:06<00:00, 2398.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚¨áÔ∏è Downloading V27...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in hvacai-27 to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22723/22723 [00:02<00:00, 8400.74it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to hvacai-27 in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:00<00:00, 5379.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚¨áÔ∏è Downloading V28...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in hvacai-28 to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1627005/1627005 [01:30<00:00, 17908.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to hvacai-28 in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14873/14873 [00:08<00:00, 1712.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Pooling 3 versions...\n",
            "   üßπ Sanitizing labels in temp_pool...\n",
            "\n",
            "üìê Running Spatial Filter...\n",
            "   Rule: EVERY instrument must contain AT LEAST ONE of ['id_letters', 'tag_number']\n",
            "‚úÖ Spatial Filter Complete. Kept: 7231, Deleted: 8044\n",
            "\n",
            "‚úÇÔ∏è Splitting Dataset (Train: 5784, Val: 1084, Test: 363)\n",
            "\n",
            "‚úÖ Dataset Ready!\n",
            "   Config: /content/YOLOplan/merged_dataset/data.yaml\n"
          ]
        }
      ],
      "source": [
        "import roboflow\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import shutil\n",
        "import yaml\n",
        "import random\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "def to_xyxy(parts):\n",
        "    xc, yc, w, h = parts[1], parts[2], parts[3], parts[4]\n",
        "    return [parts[0], xc - w/2, yc - h/2, xc + w/2, yc + h/2]\n",
        "\n",
        "def get_center(box):\n",
        "    return ((box[1] + box[3]) / 2, (box[2] + box[4]) / 2)\n",
        "\n",
        "def is_inside_center_point(inner_box, outer_box):\n",
        "    inner_center = get_center(inner_box)\n",
        "    return (outer_box[1] <= inner_center[0] <= outer_box[3] and\n",
        "            outer_box[2] <= inner_center[1] <= outer_box[4])\n",
        "\n",
        "def sanitize_labels(path):\n",
        "    print(f\"   üßπ Sanitizing labels in {os.path.basename(path)}...\")\n",
        "    for fp in glob.glob(os.path.join(path, \"**\", \"*.txt\"), recursive=True):\n",
        "        if 'classes' in fp or 'README' in fp: continue\n",
        "        try:\n",
        "            with open(fp, 'r') as f: lines = f.readlines()\n",
        "            new_lines = []\n",
        "            changed = False\n",
        "            for l in lines:\n",
        "                parts = list(map(float, l.strip().split()))\n",
        "                if len(parts) == 5: new_lines.append(l)\n",
        "                elif len(parts) > 5:\n",
        "                    changed = True; cls, coords = int(parts[0]), parts[1:]\n",
        "                    xs, ys = coords[0::2], coords[1::2]\n",
        "                    new_lines.append(f\"{cls} {(min(xs)+max(xs))/2:.6f} {(min(ys)+max(ys))/2:.6f} {max(xs)-min(xs):.6f} {max(ys)-min(ys):.6f}\\n\")\n",
        "            if changed:\n",
        "                with open(fp, 'w') as f: f.writelines(new_lines)\n",
        "        except: continue\n",
        "\n",
        "def get_class_map(dataset_path):\n",
        "    yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "    with open(yaml_path, 'r') as f: config = yaml.safe_load(f)\n",
        "    names = config.get('names', []); return {n: i for i, n in enumerate(names)}\n",
        "\n",
        "# --- CORRECTED \"INTELLIGENT\" SPATIAL FILTER ---\n",
        "def filter_by_spatial_containment(pool_path, containers, contents):\n",
        "    print(f\"\\nüìê Running Spatial Filter...\")\n",
        "    print(f\"   Rule: EVERY instrument must contain AT LEAST ONE of {contents}\")\n",
        "\n",
        "    class_map = get_class_map(pool_path)\n",
        "    container_ids = {class_map[n] for n in containers if n in class_map}\n",
        "    content_ids = {class_map[n] for n in contents if n in class_map}\n",
        "\n",
        "    if not container_ids or not content_ids:\n",
        "        print(\"‚ö†Ô∏è Missing classes for filter. Skipping.\"); return\n",
        "\n",
        "    deleted_count, kept_count = 0, 0\n",
        "\n",
        "    for file_path in glob.glob(os.path.join(pool_path, \"labels\", \"*.txt\")):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f: lines = f.readlines()\n",
        "\n",
        "            all_boxes = [to_xyxy(list(map(float, l.strip().split()))) for l in lines if len(l.strip().split()) == 5]\n",
        "            container_boxes = [b for b in all_boxes if int(b[0]) in container_ids]\n",
        "            content_boxes = [b for b in all_boxes if int(b[0]) in content_ids]\n",
        "\n",
        "            if not container_boxes:\n",
        "                # No instruments on this image, so it doesn't violate the rule. Keep it.\n",
        "                image_is_valid = True\n",
        "            else:\n",
        "                image_is_valid = True\n",
        "                for container in container_boxes:\n",
        "                    has_any_content = False\n",
        "                    for content in content_boxes:\n",
        "                        if is_inside_center_point(content, container):\n",
        "                            has_any_content = True\n",
        "                            break\n",
        "\n",
        "                    if not has_any_content:\n",
        "                        image_is_valid = False\n",
        "                        break\n",
        "\n",
        "            if image_is_valid:\n",
        "                kept_count += 1\n",
        "            else:\n",
        "                deleted_count += 1\n",
        "                os.remove(file_path)\n",
        "                base = os.path.splitext(os.path.basename(file_path))[0]\n",
        "                for ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "                    p = os.path.join(pool_path, \"images\", base + ext)\n",
        "                    if os.path.exists(p): os.remove(p)\n",
        "        except: continue\n",
        "\n",
        "    print(f\"‚úÖ Spatial Filter Complete. Kept: {kept_count}, Deleted: {deleted_count}\")\n",
        "\n",
        "def unify_and_split_datasets(versions, output_path, ratios):\n",
        "    # This function is unchanged\n",
        "    pool_dir = \"temp_pool\"\n",
        "    if os.path.exists(pool_dir): shutil.rmtree(pool_dir)\n",
        "    os.makedirs(os.path.join(pool_dir, \"images\")); os.makedirs(os.path.join(pool_dir, \"labels\"))\n",
        "    if os.path.exists(output_path): shutil.rmtree(output_path)\n",
        "    for s in ['train', 'valid', 'test']: os.makedirs(os.path.join(output_path, s, 'images')); os.makedirs(os.path.join(output_path, s, 'labels'))\n",
        "    print(f\"\\nüîÑ Pooling {len(versions)} versions...\")\n",
        "    for i, v_path in enumerate(versions):\n",
        "        if i == 0:\n",
        "            yp = os.path.join(v_path, 'data.yaml')\n",
        "            if os.path.exists(yp): shutil.copy(yp, os.path.join(pool_dir, 'data.yaml'))\n",
        "        for sub in ['train', 'valid', 'test']:\n",
        "            src_img, src_lbl = os.path.join(v_path, sub, 'images'), os.path.join(v_path, sub, 'labels')\n",
        "            if not os.path.exists(src_img): continue\n",
        "            for f in os.listdir(src_img):\n",
        "                if f.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "                    new_name = f\"v{i}_{sub}_{f}\"; shutil.copy(os.path.join(src_img, f), os.path.join(pool_dir, \"images\", new_name))\n",
        "                    lbl_name = os.path.splitext(f)[0] + \".txt\"\n",
        "                    if os.path.exists(os.path.join(src_lbl, lbl_name)):\n",
        "                        shutil.copy(os.path.join(src_lbl, lbl_name), os.path.join(pool_dir, \"labels\", os.path.splitext(new_name)[0] + \".txt\"))\n",
        "    sanitize_labels(pool_dir)\n",
        "    if REQUIRE_SPATIAL_CONTAINMENT:\n",
        "        filter_by_spatial_containment(pool_dir, CONTAINER_CLASSES, REQUIRED_CONTENTS)\n",
        "    all_images = glob.glob(os.path.join(pool_dir, \"images\", \"*\")); random.shuffle(all_images)\n",
        "    count = len(all_images); n_train, n_val = int(count * ratios[0]), int(count * ratios[1])\n",
        "    train_set, val_set, test_set = all_images[:n_train], all_images[n_train:n_train+n_val], all_images[n_train+n_val:]\n",
        "    print(f\"\\n‚úÇÔ∏è Splitting Dataset (Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)})\")\n",
        "    def move_set(file_list, split_name):\n",
        "        for img_path in file_list:\n",
        "            base = os.path.basename(img_path); shutil.move(img_path, os.path.join(output_path, split_name, \"images\", base))\n",
        "            lbl_name = os.path.splitext(base)[0] + \".txt\"\n",
        "            if os.path.exists(os.path.join(pool_dir, \"labels\", lbl_name)):\n",
        "                shutil.move(os.path.join(pool_dir, \"labels\", lbl_name), os.path.join(output_path, split_name, \"labels\", lbl_name))\n",
        "    move_set(train_set, \"train\"); move_set(val_set, \"valid\"); move_set(test_set, \"test\")\n",
        "    yp = os.path.join(pool_dir, \"data.yaml\")\n",
        "    if os.path.exists(yp):\n",
        "        with open(yp, 'r') as f: y = yaml.safe_load(f)\n",
        "        y['path'], y['train'], y['val'], y['test'] = os.path.abspath(output_path), \"train/images\", \"valid/images\", \"test/images\"\n",
        "        with open(os.path.join(output_path, \"data.yaml\"), 'w') as f: yaml.dump(y, f)\n",
        "        return os.path.join(output_path, \"data.yaml\")\n",
        "    return None\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "try:\n",
        "    ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
        "    rf = roboflow.Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "    project = rf.workspace(ROBOFLOW_WORKSPACE_ID).project(ROBOFLOW_PROJECT_ID)\n",
        "    if isinstance(ROBOFLOW_VERSIONS, int): ROBOFLOW_VERSIONS = [ROBOFLOW_VERSIONS]\n",
        "    d_paths = []\n",
        "    for v in ROBOFLOW_VERSIONS:\n",
        "        print(f\"\\n‚¨áÔ∏è Downloading V{v}...\"); ds = project.version(v).download(\"yolov11\")\n",
        "        loc = ds.location;\n",
        "        if loc.endswith('.zip'):\n",
        "            with zipfile.ZipFile(loc, 'r') as z: z.extractall(os.path.splitext(loc)[0]); loc = os.path.splitext(loc)[0]\n",
        "        d_paths.append(loc)\n",
        "    FINAL_DIR = os.path.join(os.getcwd(), \"merged_dataset\")\n",
        "    DATA_YAML_PATH = unify_and_split_datasets(d_paths, FINAL_DIR, [TRAIN_RATIO, VAL_RATIO, TEST_RATIO])\n",
        "    if DATA_YAML_PATH and os.path.exists(DATA_YAML_PATH):\n",
        "        print(f\"\\n‚úÖ Dataset Ready!\"); print(f\"   Config: {DATA_YAML_PATH}\")\n",
        "    else:\n",
        "        print(\"‚ùå Error generating dataset.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0VBq9XAxqsvZ",
      "metadata": {
        "id": "0VBq9XAxqsvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d6c82e-d57e-4a3d-ee3f-2215bb36bafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Updated 'tensorboard=False'\n",
            "‚úÖ Updated 'wandb=False'\n",
            "‚úÖ Updated 'mlflow=False'\n",
            "JSONDict(\"/root/.config/Ultralytics/settings.json\"):\n",
            "{\n",
            "  \"settings_version\": \"0.0.6\",\n",
            "  \"datasets_dir\": \"/content/YOLOplan/datasets\",\n",
            "  \"weights_dir\": \"weights\",\n",
            "  \"runs_dir\": \"runs\",\n",
            "  \"uuid\": \"569f3ba64b326db489132663f79cd37279811de477381b83ac131e6cdd129cbb\",\n",
            "  \"sync\": true,\n",
            "  \"api_key\": \"\",\n",
            "  \"openai_api_key\": \"\",\n",
            "  \"clearml\": true,\n",
            "  \"comet\": true,\n",
            "  \"dvc\": true,\n",
            "  \"hub\": true,\n",
            "  \"mlflow\": false,\n",
            "  \"neptune\": true,\n",
            "  \"raytune\": true,\n",
            "  \"tensorboard\": false,\n",
            "  \"wandb\": false,\n",
            "  \"vscode_msg\": true,\n",
            "  \"openvino_msg\": true\n",
            "}\n",
            "üí° Learn more about Ultralytics Settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\n",
            "üß† PROGRESSIVE MODE: Loading intelligence from '/content/drive/MyDrive/hvac_training/yolo11m_run_v8/weights/best.pt'\n",
            "\n",
            "üöÄ COMMAND: yolo task=detect mode=train model='/content/drive/MyDrive/hvac_training/yolo11m_run_v8/weights/best.pt' data='/content/YOLOplan/merged_dataset/data.yaml' project='/content/drive/MyDrive/hvac_training' name='yolo11m_run_v8_TEXT_FINETUNE' exist_ok=True epochs=20 imgsz=1280 batch=4 cls=1.0 mosaic=0.0 degrees=0.0 translate=0.0 scale=0.0 shear=0.0 perspective=0.0 flipud=0.0 fliplr=0.0 mixup=0.0 copy_paste=0.0 hsv_h=0.0 hsv_s=0.0 hsv_v=0.0\n",
            "\n",
            "Ultralytics 8.3.241 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=1.0, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/YOLOplan/merged_dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=True, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/hvac_training/yolo11m_run_v8/weights/best.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=yolo11m_run_v8_TEXT_FINETUNE, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/hvac_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/hvac_training/yolo11m_run_v8_TEXT_FINETUNE, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.0, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.0, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 118.1MB/s 0.0s\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
            "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
            "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
            " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
            " 23        [16, 19, 22]  1   1420276  ultralytics.nn.modules.head.Detect           [12, [256, 512, 512]]         \n",
            "YOLO11m summary: 231 layers, 20,062,260 parameters, 20,062,244 gradients, 68.2 GFLOPs\n",
            "\n",
            "Transferred 649/649 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 363.5MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2658.6¬±1078.3 MB/s, size: 168.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/YOLOplan/merged_dataset/train/labels... 5784 images, 513 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5784/5784 1.9Kit/s 3.1s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/YOLOplan/merged_dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 677.1¬±450.8 MB/s, size: 127.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/YOLOplan/merged_dataset/valid/labels... 1084 images, 95 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1084/1084 968.9it/s 1.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/YOLOplan/merged_dataset/valid/labels.cache\n",
            "Plotting labels to /content/drive/MyDrive/hvac_training/yolo11m_run_v8_TEXT_FINETUNE/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000625, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
            "Image sizes 1280 train, 1280 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/hvac_training/yolo11m_run_v8_TEXT_FINETUNE\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/20      9.79G      1.349      1.404      1.183         60       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.7it/s 13:47\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 1.4it/s 1:40\n",
            "                   all       1084      16330      0.878      0.747      0.834      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/20      9.78G      1.329      1.467      1.169         91       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:26\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.3s\n",
            "                   all       1084      16330       0.84      0.785      0.833      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/20      9.65G      1.322      1.454      1.162        103       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.2s\n",
            "                   all       1084      16330      0.864      0.749      0.814      0.532\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/20      9.86G      1.299      1.424      1.154         84       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.2s\n",
            "                   all       1084      16330      0.837      0.805      0.842      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/20      10.1G      1.262      1.337      1.135         41       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.2s\n",
            "                   all       1084      16330       0.83      0.804      0.827      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/20      9.31G      1.219       1.27      1.115         38       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.2s\n",
            "                   all       1084      16330      0.811      0.801      0.819      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/20      9.29G      1.177      1.221      1.096        103       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:21\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.5s\n",
            "                   all       1084      16330      0.869      0.813      0.861       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/20      9.87G      1.129      1.142      1.073         44       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.5s\n",
            "                   all       1084      16330      0.873       0.82      0.855      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/20      9.71G      1.082      1.101      1.053         20       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.4s\n",
            "                   all       1084      16330      0.886      0.822      0.861      0.583\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/20      11.1G      1.026      1.046      1.027         33       1280: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1446/1446 1.8it/s 13:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 136/136 2.8it/s 48.1s\n",
            "                   all       1084      16330      0.903      0.819      0.867      0.596\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/20      9.88G     0.9796      1.015      1.007         66       1280: 65% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏‚îÄ‚îÄ‚îÄ‚îÄ 940/1446 1.8it/s 8:43<4:47"
          ]
        }
      ],
      "source": [
        "#@title üöÄ Train The Model\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# --- 2. DISABLE EXTERNAL LOGGERS ---\n",
        "# Ensures a clean, local-only run.\n",
        "!yolo settings tensorboard=False wandb=False mlflow=False\n",
        "\n",
        "# --- 3. DEFINE PATHS & DETERMINE STARTING WEIGHTS ---\n",
        "current_run_dir = os.path.join(DRIVE_ROOT, RUN_NAME)\n",
        "crash_ckpt = os.path.join(current_run_dir, \"weights\", \"last.pt\")\n",
        "weights_to_load, should_resume = MODEL, False\n",
        "\n",
        "if os.path.exists(crash_ckpt):\n",
        "    print(f\"üö® CRASH DETECTED: Resuming from '{crash_ckpt}'\")\n",
        "    weights_to_load, should_resume = crash_ckpt, True\n",
        "elif USE_PREVIOUS_WEIGHTS and os.path.exists(PREVIOUS_WEIGHTS_PATH):\n",
        "    print(f\"üß† PROGRESSIVE MODE: Loading intelligence from '{PREVIOUS_WEIGHTS_PATH}'\")\n",
        "    weights_to_load = PREVIOUS_WEIGHTS_PATH\n",
        "else:\n",
        "    print(f\"üÜï COLD START: Training from standard {MODEL}\")\n",
        "\n",
        "# --- 4. ENSURE DATA & BUILD COMMAND ---\n",
        "if 'DATA_YAML_PATH' not in locals(): raise FileNotFoundError(\"‚ùå data.yaml not found. Run Cell 3 first.\")\n",
        "cmd_args = [\n",
        "    f\"task=detect\",\n",
        "    f\"mode=train\",\n",
        "    f\"model='{weights_to_load}'\",\n",
        "    f\"data='{DATA_YAML_PATH}'\",\n",
        "    f\"project='{DRIVE_ROOT}'\",\n",
        "    f\"name='{RUN_NAME}'\",\n",
        "    f\"exist_ok=True\"\n",
        "]\n",
        "\n",
        "if not should_resume:\n",
        "    cmd_args.extend([\n",
        "        f\"epochs={EPOCHS}\",\n",
        "        f\"imgsz={IMG_SIZE}\",\n",
        "        f\"batch={BATCH_SIZE}\"\n",
        "    ])\n",
        "    if 'CLS_GAIN' in locals(): cmd_args.append(f\"cls={CLS_GAIN}\")\n",
        "\n",
        "    if not ENABLE_AUGMENTATIONS:\n",
        "        cmd_args.extend([\"mosaic=0.0\", \"degrees=0.0\", \"translate=0.0\", \"scale=0.0\", \"shear=0.0\",\n",
        "                         \"perspective=0.0\", \"flipud=0.0\", \"fliplr=0.0\", \"mixup=0.0\",\n",
        "                         \"copy_paste=0.0\", \"hsv_h=0.0\", \"hsv_s=0.0\", \"hsv_v=0.0\"])\n",
        "else:\n",
        "    cmd_args.append(\"resume=True\")\n",
        "\n",
        "final_cmd = \"yolo \" + \" \".join(cmd_args)\n",
        "print(f\"\\nüöÄ COMMAND: {final_cmd}\\n\")\n",
        "\n",
        "# --- 5. EXECUTE TRAINING ---\n",
        "!{final_cmd}\n",
        "\n",
        "print(\"\\n\\n‚úÖ Training Complete!\")\n",
        "print(f\"   Results, weights, and charts saved to: {current_run_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PCk8on6QqsvZ",
      "metadata": {
        "id": "PCk8on6QqsvZ"
      },
      "source": [
        "## 5. Validate and Test\n",
        "\n",
        "After training, these cells will help you validate the model's performance on the test set and run predictions on new images. The results will be saved in the project folder you defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-UMdjuOQqsvZ",
      "metadata": {
        "id": "-UMdjuOQqsvZ"
      },
      "outputs": [],
      "source": [
        "#@title üîé Find Best Model from Last Run\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Construct the expected path using the variables from your Master Config\n",
        "# This points to the results of your last training run.\n",
        "model_path = os.path.join(DRIVE_ROOT, RUN_NAME, \"weights\", \"best.pt\")\n",
        "\n",
        "print(f\"Searching for trained model at: {model_path}\")\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    BEST_WEIGHTS_PATH = model_path\n",
        "    print(f\"‚úÖ Model Found: {BEST_WEIGHTS_PATH}\")\n",
        "else:\n",
        "    BEST_WEIGHTS_PATH = None\n",
        "    print(f\"‚ùå Model Not Found! Please ensure a training run has completed and saved to the correct Drive folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uaiZNpX1qsvZ",
      "metadata": {
        "id": "uaiZNpX1qsvZ"
      },
      "outputs": [],
      "source": [
        "#@title üîÆ Run Prediction & Visualize Results\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# 1. CHECK IF MODEL WAS FOUND\n",
        "if 'BEST_WEIGHTS_PATH' in locals() and BEST_WEIGHTS_PATH:\n",
        "\n",
        "    # 2. FIND VALIDATION IMAGES\n",
        "    if 'DATA_YAML_PATH' in locals() and os.path.exists(DATA_YAML_PATH):\n",
        "        dataset_root = os.path.dirname(DATA_YAML_PATH)\n",
        "        valid_images_path = os.path.join(dataset_root, \"valid\", \"images\")\n",
        "\n",
        "        if os.path.exists(valid_images_path):\n",
        "\n",
        "            # 3. RUN PREDICTION\n",
        "            print(f\"üîÆ Running prediction with model: {BEST_WEIGHTS_PATH}\")\n",
        "\n",
        "            # Use the new Drive path for saving results\n",
        "            output_dir = os.path.join(DRIVE_ROOT, f\"{RUN_NAME}_predictions\")\n",
        "\n",
        "            # Run YOLO inference on the entire validation set\n",
        "            !yolo task=detect mode=predict \\\n",
        "                model=\"{BEST_WEIGHTS_PATH}\" \\\n",
        "                source=\"{valid_images_path}\" \\\n",
        "                imgsz={IMG_SIZE} \\\n",
        "                conf=0.25 \\\n",
        "                project=\"{DRIVE_ROOT}\" \\\n",
        "                name=\"{RUN_NAME}_predictions\" \\\n",
        "                save=True \\\n",
        "                max_det=100 \\\n",
        "                exist_ok=True\n",
        "\n",
        "            # 4. VISUALIZE RESULTS\n",
        "            predicted_images = glob.glob(os.path.join(output_dir, '*.jpg'))\n",
        "\n",
        "            if predicted_images:\n",
        "                print(f\"\\nüëÄ Displaying 3 Random Sample Results:\")\n",
        "                display_samples = random.sample(predicted_images, min(len(predicted_images), 3))\n",
        "\n",
        "                for img_path in display_samples:\n",
        "                    print(f\"--- Result: {os.path.basename(img_path)} ---\")\n",
        "                    display(Image(filename=img_path, width=800))\n",
        "                    print(\"\\n\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Prediction ran, but no output images were found.\")\n",
        "        else:\n",
        "            print(f\"‚ùå Validation image folder not found at: {valid_images_path}\")\n",
        "    else:\n",
        "        print(\"‚ùå DATA_YAML_PATH not found. Ensure dataset is downloaded.\")\n",
        "else:\n",
        "    print(\"‚ùå BEST_WEIGHTS_PATH not found. Run the 'Find Best Model' cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lr3LcaKR5pMT",
      "metadata": {
        "id": "lr3LcaKR5pMT"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the source folder containing all your runs\n",
        "source_folder = '/content/drive/MyDrive/hvac_training/yolo11m_run_v4'\n",
        "\n",
        "# Check if the folder exists before trying to zip\n",
        "if os.path.exists(source_folder):\n",
        "    # Create a unique filename with a timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
        "    zip_filename = f\"YOLO11_Training_Results_{timestamp}\"\n",
        "\n",
        "    print(f\"üì¶ Compressing '{source_folder}'...\")\n",
        "\n",
        "    # Create the zip file (shutil.make_archive adds .zip automatically)\n",
        "    shutil.make_archive(zip_filename, 'zip', source_folder)\n",
        "\n",
        "    output_file = f\"{zip_filename}.zip\"\n",
        "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "\n",
        "    print(f\"‚úÖ Compression complete. File size: {file_size_mb:.2f} MB\")\n",
        "    print(f\"‚¨áÔ∏è Starting download of: {output_file}\")\n",
        "\n",
        "    # Trigger the browser download\n",
        "    files.download(output_file)\n",
        "else:\n",
        "    print(f\"‚ùå Folder not found: {source_folder}\")\n",
        "    print(\"Did you run the training cell successfully?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ItK-y1AQHTZ1",
      "metadata": {
        "id": "ItK-y1AQHTZ1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}