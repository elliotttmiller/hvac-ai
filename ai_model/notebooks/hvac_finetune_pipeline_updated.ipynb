{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbJThT5Ol_lr"
      },
      "source": [
        "# üü£ HVAC-Specific SAM Fine-Tuning Pipeline (v2 - Advanced)\n",
        "## üîß Enhanced with Multi-Prompt Training & Resumability\n",
        "\n",
        "This notebook has been upgraded to include advanced features for creating a more robust and flexible model, inspired by recent research (e.g., SAM-PAR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1_header"
      },
      "source": [
        "### Phase 1: Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BiBpFHzYU4Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0oru8hAn6q2"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install torch torchvision --quiet\n",
        "!pip install opencv-python pycocotools matplotlib onnxruntime onnx --quiet\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwmQm0C3n_3D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools import mask as mask_utils\n",
        "import random\n",
        "\n",
        "# SAM imports\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2_header"
      },
      "source": [
        "### Phase 2: Configuration and Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdTD9CTxKena"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "CONFIG = {\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "    # --- STARTING MODEL --- \n",
        "    # Path to the official pre-trained SAM weights. We start fresh for this new expert dataset.\n",
        "    'model_path': '/content/sam_vit_h_4b8939.pth',\n",
        "    \n",
        "    # --- DATASET --- \n",
        "    'dataset_zip_path': '/content/hvac_dataset_golden.zip',\n",
        "    'unzip_path': '/content/hvac_dataset_golden',\n",
        "    'annotations_file_name': '_annotations.coco.json',\n",
        "    \n",
        "    # --- OUTPUT & CHECKPOINTING --- \n",
        "    'output_dir': '/content/drive/MyDrive/sam_finetuning_results',\n",
        "    'best_model_save_path': '/content/drive/MyDrive/sam_finetuning_results/best_model_multiprompt_v1.pth',\n",
        "    'latest_checkpoint_save_path': '/content/drive/MyDrive/sam_finetuning_results/latest_checkpoint_multiprompt_v1.pth',\n",
        "    'resume_training': False, # SET TO TRUE TO RESUME FROM 'latest_checkpoint_save_path'\n",
        "    \n",
        "    # --- PROMPT ENGINEERING STRATEGY --- \n",
        "    # Options: 'perfect_box', 'noisy_box', 'multi_prompt' (inspired by SAM-PAR paper)\n",
        "    'prompt_strategy': 'multi_prompt',\n",
        "    'bbox_noise_factor': 0.1, # How much to 'jiggle' the box in noisy_box or multi_prompt mode\n",
        "    \n",
        "    # --- MODEL & TRAINING HYPERPARAMETERS ---\n",
        "    'model_type': 'vit_h',\n",
        "    'image_size': 1024,\n",
        "    'batch_size': 1,\n",
        "    'num_workers': 0,\n",
        "    'num_epochs': 25,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0,\n",
        "    'early_stopping_patience': 10,\n",
        "    'checkpoint_batch_interval': 300,\n",
        "    'min_mask_area': 100,\n",
        "}\n",
        "\n",
        "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úì Configuration loaded. Using device: {CONFIG['device']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset if it hasn't been already\n",
        "if not os.path.exists(CONFIG['unzip_path']):\n",
        "    print(f\"üìÅ Unzipping dataset from {CONFIG['dataset_zip_path']}...\")\n",
        "    with zipfile.ZipFile(CONFIG['dataset_zip_path'], 'r') as zip_ref:\n",
        "        zip_ref.extractall(CONFIG['unzip_path'])\n",
        "    print(\"‚úÖ Unzipping complete.\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset already unzipped.\")"
      ],
      "metadata": {
        "id": "unzip_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase3_header"
      },
      "source": [
        "### Phase 3: Dataset Loading and DataLoader Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loading_cell"
      },
      "outputs": [],
      "source": [
        "def load_coco_split(dataset_root_path: str, split_name: str, annotations_file: str) -> Tuple[COCO, str]:\n",
        "    split_path = os.path.join(dataset_root_path, split_name)\n",
        "    annotations_path = os.path.join(split_path, annotations_file)\n",
        "    if not os.path.exists(annotations_path):\n",
        "        raise FileNotFoundError(f\"Annotations file not found for '{split_name}' at: {annotations_path}\")\n",
        "    print(f\"üîÑ Loading '{split_name}' annotations from: {annotations_path}\")\n",
        "    coco = COCO(annotations_path)\n",
        "    print(f\"üìä Found {len(coco.getImgIds())} images in '{split_name}'.\")\n",
        "    return coco, split_path\n",
        "\n",
        "def get_image_path(split_path: str, img_info: dict) -> str:\n",
        "    full_path = os.path.join(split_path, img_info['file_name'])\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Image file not found: {img_info['file_name']} in {split_path}\")\n",
        "    return full_path\n",
        "\n",
        "class HvacSamDataset(Dataset):\n",
        "    def __init__(self, coco: COCO, image_ids: List[int], split_path: str, is_training: bool = True):\n",
        "        self.coco = coco\n",
        "        self.image_ids = image_ids\n",
        "        self.split_path = split_path\n",
        "        self.is_training = is_training\n",
        "        self.resize_transform = ResizeLongestSide(CONFIG['image_size'])\n",
        "        self.pixel_mean = torch.tensor([123.675, 116.28, 103.53]).view(-1, 1, 1)\n",
        "        self.pixel_std = torch.tensor([58.395, 57.12, 57.375]).view(-1, 1, 1)\n",
        "        self.prompt_strategy = CONFIG.get('prompt_strategy', 'perfect_box')\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def _generate_prompt(self, mask: np.ndarray, bbox: List[float]):\n",
        "        # For validation/testing, always use the perfect box for consistent evaluation\n",
        "        if not self.is_training:\n",
        "            return {'box': np.array(bbox)}\n",
        "        \n",
        "        # Determine the prompt type for this training item\n",
        "        if self.prompt_strategy == 'multi_prompt':\n",
        "            prompt_type = random.choice(['box', 'point', 'scribble'])\n",
        "        elif self.prompt_strategy == 'point':\n",
        "            prompt_type = 'point'\n",
        "        elif self.prompt_strategy == 'noisy_box':\n",
        "            prompt_type = 'box'\n",
        "        else: # 'perfect_box'\n",
        "            prompt_type = 'box'\n",
        "        \n",
        "        if prompt_type == 'box':\n",
        "            noise_factor = CONFIG.get('bbox_noise_factor', 0) if self.prompt_strategy == 'noisy_box' or self.prompt_strategy == 'multi_prompt' else 0\n",
        "            x, y, w, h = bbox\n",
        "            x_noise = w * noise_factor * (random.random() - 0.5) * 2\n",
        "            y_noise = h * noise_factor * (random.random() - 0.5) * 2\n",
        "            w_noise = w * noise_factor * (random.random() - 0.5) * 2\n",
        "            h_noise = h * noise_factor * (random.random() - 0.5) * 2\n",
        "            box = np.array([x + x_noise, y + y_noise, w + w_noise, h + h_noise])\n",
        "            return {'box': box}\n",
        "        \n",
        "        points = np.argwhere(mask)\n",
        "        if len(points) == 0: return None\n",
        "\n",
        "        if prompt_type == 'point':\n",
        "            point = points[random.randint(0, len(points) - 1)]\n",
        "            point_coords = np.array([[point[1], point[0]]]) # (x, y)\n",
        "            point_labels = np.array([1])\n",
        "            return {'point_coords': point_coords, 'point_labels': point_labels}\n",
        "            \n",
        "        elif prompt_type == 'scribble':\n",
        "            num_points = min(5, len(points))\n",
        "            point_indices = np.random.choice(len(points), num_points, replace=False)\n",
        "            scribble_points = points[point_indices]\n",
        "            point_coords = scribble_points[:, ::-1] # (row, col) -> (x, y)\n",
        "            point_labels = np.ones(num_points)\n",
        "            return {'point_coords': point_coords, 'point_labels': point_labels}\n",
        "            \n",
        "        return None\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_info = self.coco.loadImgs([img_id])[0]\n",
        "        image_path = get_image_path(self.split_path, img_info)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
        "        annotations = self.coco.loadAnns(ann_ids)\n",
        "        \n",
        "        masks, prompts = [], []\n",
        "        for ann in annotations:\n",
        "            if 'segmentation' not in ann or ann.get('iscrowd', 0) == 1: continue\n",
        "            mask = self.coco.annToMask(ann)\n",
        "            if mask.sum() < CONFIG['min_mask_area']: continue\n",
        "            \n",
        "            prompt = self._generate_prompt(mask, ann['bbox'])\n",
        "            if prompt:\n",
        "                masks.append(mask.astype(bool))\n",
        "                prompts.append(prompt)\n",
        "\n",
        "        original_size = image.shape[:2]\n",
        "        resized_image = self.resize_transform.apply_image(image)\n",
        "        input_image_torch = torch.as_tensor(resized_image, dtype=torch.float32).permute(2, 0, 1).contiguous()\n",
        "        input_image_torch = (input_image_torch - self.pixel_mean) / self.pixel_std\n",
        "        h, w = input_image_torch.shape[-2:]\n",
        "        padh, padw = CONFIG['image_size'] - h, CONFIG['image_size'] - w\n",
        "        input_image_padded = torch.nn.functional.pad(input_image_torch, (0, padw, 0, padh))\n",
        "\n",
        "        return {\n",
        "            'image': input_image_padded,\n",
        "            'masks': masks,\n",
        "            'prompts': prompts,\n",
        "            'original_size': original_size,\n",
        "            'input_size': (h, w)\n",
        "        }\n",
        "\n",
        "def custom_collate_fn(batch: List[Dict]) -> Dict[str, Any]:\n",
        "    return {\n",
        "        'image': torch.stack([item['image'] for item in batch]),\n",
        "        'masks': [item['masks'] for item in batch],\n",
        "        'prompts': [item['prompts'] for item in batch],\n",
        "        'original_size': [item['original_size'] for item in batch],\n",
        "        'input_size': [item['input_size'] for item in batch]\n",
        "    }\n",
        "\n",
        "# Load datasets\n",
        "train_coco, train_path = load_coco_split(CONFIG['unzip_path'], 'train', CONFIG['annotations_file_name'])\n",
        "val_coco, val_path = load_coco_split(CONFIG['unzip_path'], 'valid', CONFIG['annotations_file_name'])\n",
        "train_ids, val_ids = train_coco.getImgIds(), val_coco.getImgIds()\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = HvacSamDataset(train_coco, train_ids, train_path, is_training=True)\n",
        "val_dataset = HvacSamDataset(val_coco, val_ids, val_path, is_training=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
        "\n",
        "print(f\"\\n‚úÖ Training dataset initialized with {len(train_dataset)} samples.\")\n",
        "print(f\"‚úÖ Validation dataset initialized with {len(val_dataset)} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Define URL for the official ViT-H model\n",
        "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
        "CHECKPOINT_PATH = \"/content/sam_vit_h_4b8939.pth\"\n",
        "\n",
        "print(f\"‚¨áÔ∏è Downloading official SAM ViT-H weights to {CHECKPOINT_PATH}...\")\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    urllib.request.urlretrieve(CHECKPOINT_URL, CHECKPOINT_PATH)\n",
        "    print(\"‚úÖ Download complete.\")\n",
        "else:\n",
        "    print(\"‚úÖ File already exists.\")"
      ],
      "metadata": {
        "id": "du1oZDKE6GLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase4_header"
      },
      "source": [
        "### Phase 4: Model Preparation and Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjTIJtLxP8ZG"
      },
      "outputs": [],
      "source": [
        "# Initialize model and optimizer first to allow for state loading\n",
        "sam_model = sam_model_registry[CONFIG['model_type']]()\n",
        "sam_model.to(CONFIG['device'])\n",
        "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# --- NEW: RESUME TRAINING LOGIC ---\n",
        "if CONFIG['resume_training']:\n",
        "    print(f\"üîÑ Attempting to resume training from {CONFIG['latest_checkpoint_save_path']}\")\n",
        "    if os.path.exists(CONFIG['latest_checkpoint_save_path']):\n",
        "        checkpoint = torch.load(CONFIG['latest_checkpoint_save_path'])\n",
        "        sam_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"‚úÖ Resumed successfully. Starting from epoch {start_epoch}.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Resume checkpoint not found. Starting training from scratch with pre-trained SAM.\")\n",
        "        # Load the base pre-trained model if resume fails\n",
        "        sam_model.load_state_dict(torch.load(CONFIG['model_path']))\n",
        "else:\n",
        "    print(f\"üîÑ Starting new training session from pre-trained model: {CONFIG['model_path']}\")\n",
        "    sam_model.load_state_dict(torch.load(CONFIG['model_path']))\n",
        "\n",
        "# Configure model for fine-tuning (freeze encoders)\n",
        "sam_model.train()\n",
        "for name, param in sam_model.named_parameters():\n",
        "    if name.startswith(\"image_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "        param.requires_grad = False\n",
        "\n",
        "trainable_params = sum(p.numel() for p in sam_model.parameters() if p.requires_grad)\n",
        "print(f\"‚úÖ SAM model configured for fine-tuning. Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtPYpirbK3Wi"
      },
      "outputs": [],
      "source": [
        "# Setup scheduler and loss function\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "def combined_loss(pred_masks: torch.Tensor, true_masks: torch.Tensor) -> torch.Tensor:\n",
        "    bce_loss = nn.BCEWithLogitsLoss()(pred_masks, true_masks.float())\n",
        "    pred_flat = torch.sigmoid(pred_masks).view(-1)\n",
        "    true_flat = true_masks.view(-1)\n",
        "    intersection = (pred_flat * true_flat).sum()\n",
        "    dice_loss = 1 - (2. * intersection + 1e-8) / (pred_flat.sum() + true_flat.sum() + 1e-8)\n",
        "    return 0.8 * bce_loss + 0.2 * dice_loss\n",
        "\n",
        "print(\"‚úÖ Scheduler and loss function configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase5_header"
      },
      "source": [
        "### Phase 5: Training and Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRQ6yd_PM_B9"
      },
      "outputs": [],
      "source": [
        "def run_epoch(model, dataloader, optimizer, is_training, device, epoch):\n",
        "    model.train(is_training)\n",
        "    epoch_losses, iou_scores = [], []\n",
        "\n",
        "    desc = \"Training\" if is_training else \"Validation\"\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=desc)):\n",
        "        images = batch['image'].to(device, non_blocking=True)\n",
        "        all_gt_masks_list = batch['masks']\n",
        "        all_prompts_list = batch['prompts']\n",
        "\n",
        "        batch_loss = 0\n",
        "        num_annotations_in_batch = 0\n",
        "\n",
        "        with torch.set_grad_enabled(is_training):\n",
        "            with torch.no_grad():\n",
        "                image_embeddings = model.image_encoder(images)\n",
        "\n",
        "            # --- CRITICAL BUG FIX & REVISION --- \n",
        "            # Loop through each image in the batch (batch size is 1, but this is robust)\n",
        "            for i in range(len(all_gt_masks_list)):\n",
        "                # Now, loop through every annotation for the current image\n",
        "                for j in range(len(all_gt_masks_list[i])):\n",
        "                    gt_mask_np = all_gt_masks_list[i][j]\n",
        "                    prompt = all_prompts_list[i][j]\n",
        "                    num_annotations_in_batch += 1\n",
        "\n",
        "                    gt_mask_torch = torch.from_numpy(gt_mask_np).unsqueeze(0).unsqueeze(0).to(device)\n",
        "                    transform = ResizeLongestSide(CONFIG['image_size'])\n",
        "                    \n",
        "                    # --- DYNAMIC PROMPT HANDLING ---\n",
        "                    box_torch, points_torch, points_label_torch = None, None, None\n",
        "                    \n",
        "                    if 'box' in prompt:\n",
        "                        box_np = prompt['box']\n",
        "                        box_torch = torch.as_tensor(transform.apply_boxes(box_np.reshape(1, 4), batch['original_size'][i]), dtype=torch.float, device=device)\n",
        "                    elif 'point_coords' in prompt:\n",
        "                        point_coords, point_labels = prompt['point_coords'], prompt['point_labels']\n",
        "                        points_torch = torch.as_tensor(transform.apply_coords(point_coords, batch['original_size'][i]), dtype=torch.float, device=device).unsqueeze(0)\n",
        "                        points_label_torch = torch.as_tensor(point_labels, dtype=torch.float, device=device).unsqueeze(0)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
        "                            points=(points_torch, points_label_torch) if points_torch is not None else None,\n",
        "                            boxes=box_torch,\n",
        "                            masks=None\n",
        "                        )\n",
        "\n",
        "                    low_res_masks, iou_predictions = model.mask_decoder(\n",
        "                        image_embeddings=image_embeddings[i].unsqueeze(0),\n",
        "                        image_pe=model.prompt_encoder.get_dense_pe(),\n",
        "                        sparse_prompt_embeddings=sparse_embeddings,\n",
        "                        dense_prompt_embeddings=dense_embeddings,\n",
        "                        multimask_output=False,\n",
        "                    )\n",
        "\n",
        "                    upscaled_masks = model.postprocess_masks(low_res_masks, batch['input_size'][i], batch['original_size'][i])\n",
        "                    loss = combined_loss(upscaled_masks, gt_mask_torch)\n",
        "                    batch_loss += loss\n",
        "\n",
        "                    if not is_training:\n",
        "                        pred_mask = (torch.sigmoid(upscaled_masks) > 0.5).squeeze().cpu().numpy().astype(np.uint8)\n",
        "                        iou = mask_utils.iou([mask_utils.encode(np.asfortranarray(pred_mask))], [mask_utils.encode(np.asfortranarray(gt_mask_np.astype(np.uint8)))], [0])[0][0]\n",
        "                        iou_scores.append(iou)\n",
        "\n",
        "        if num_annotations_in_batch > 0:\n",
        "            normalized_batch_loss = batch_loss / num_annotations_in_batch\n",
        "            if is_training:\n",
        "                optimizer.zero_grad()\n",
        "                normalized_batch_loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_losses.append(normalized_batch_loss.item())\n",
        "            else:\n",
        "                epoch_losses.append(normalized_batch_loss.item())\n",
        "\n",
        "        # Checkpoint saving logic\n",
        "        if is_training and (batch_idx + 1) % CONFIG['checkpoint_batch_interval'] == 0:\n",
        "            chk_path = CONFIG['latest_checkpoint_save_path']\n",
        "            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, chk_path)\n",
        "            tqdm.write(f\"\\nüíæ Overwrote latest checkpoint (for crash recovery): {os.path.basename(chk_path)}\")\n",
        "\n",
        "    return {'loss': mean(epoch_losses) if epoch_losses else 0, 'iou': mean(iou_scores) if iou_scores else 0}\n",
        "\n",
        "\n",
        "best_val_iou = 0\n",
        "patience_counter = 0\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_iou': []}\n",
        "\n",
        "print(f\"\\nüöÄ Starting training from epoch {start_epoch} for {CONFIG['num_epochs']} total epochs...\")\n",
        "for epoch in range(start_epoch, CONFIG['num_epochs']):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{CONFIG['num_epochs']} ---\")\n",
        "\n",
        "    train_metrics = run_epoch(sam_model, train_loader, optimizer, is_training=True, device=CONFIG['device'], epoch=epoch)\n",
        "    history['train_loss'].append(train_metrics['loss'])\n",
        "\n",
        "    val_metrics = run_epoch(sam_model, val_loader, None, is_training=False, device=CONFIG['device'], epoch=epoch)\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['val_iou'].append(val_metrics['iou'])\n",
        "\n",
        "    print(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
        "    print(f\"Val Loss: {val_metrics['loss']:.4f} | Val IoU: {val_metrics['iou']:.4f}\")\n",
        "\n",
        "    scheduler.step(val_metrics['loss'])\n",
        "\n",
        "    if val_metrics['iou'] > best_val_iou:\n",
        "        best_val_iou = val_metrics['iou']\n",
        "        patience_counter = 0\n",
        "        best_model_path = CONFIG['best_model_save_path']\n",
        "        torch.save(sam_model.state_dict(), best_model_path)\n",
        "        print(f\"üèÜ New best model saved with IoU: {best_val_iou:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= CONFIG['early_stopping_patience']:\n",
        "        print(f\"üõë Early stopping triggered after {patience_counter} epochs with no improvement.\")\n",
        "        break\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase6_header"
      },
      "source": [
        "### Phase 6: Results and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKqIxUgAOTzp_1"
      },
      "outputs": [],
      "source": [
        "# Plot training metrics\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax1.plot(history['train_loss'], label='Training Loss', color='blue')\n",
        "ax1.plot(history['val_loss'], label='Validation Loss', color='red')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history['val_iou'], label='Validation IoU', color='green')\n",
        "ax2.set_title('Validation Metrics (IoU)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Score')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CONFIG['output_dir'], 'training_metrics.png'))\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä FINAL METRICS:\")\n",
        "if history['val_iou']:\n",
        "    print(f\"Best Validation IoU: {max(history['val_iou']):.4f}\")\n",
        "else:\n",
        "    print(\"No validation metrics recorded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKqIxUgAOTzp_4"
      },
      "outputs": [],
      "source": [
        "# Save the final trained model (or the best one)\n",
        "final_model_path = os.path.join(CONFIG['output_dir'], 'final_model.pth')\n",
        "torch.save(sam_model.state_dict(), final_model_path)\n",
        "print(f\"‚úÖ Final model state saved to: {final_model_path}\")\n",
        "print(f\"‚úÖ Best performing model (by IoU) saved to: {CONFIG['best_model_save_path']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Phase 7: Final, Unbiased Evaluation on the Test Set\n",
        "\n",
        "print(\"\\n--- Final Model Evaluation on Unseen Test Data ---\")\n",
        "\n",
        "# 1. Load the best performing model that was saved during training\n",
        "best_model_path = CONFIG['best_model_save_path']\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"üîÑ Loading best model from: {best_model_path}\")\n",
        "    # We need to re-initialize the model structure before loading the state dict\n",
        "    eval_model = sam_model_registry[CONFIG['model_type']]()\n",
        "    eval_model.load_state_dict(torch.load(best_model_path))\n",
        "    eval_model.to(CONFIG['device'])\n",
        "else:\n",
        "    print(\"‚ùå Best model file not found. Cannot perform final evaluation.\")\n",
        "    # You might want to handle this case, but for now we'll assume it exists\n",
        "\n",
        "# 2. Load the test dataset\n",
        "try:\n",
        "    test_coco, test_path = load_coco_split(CONFIG['unzip_path'], 'test', CONFIG['annotations_file_name'])\n",
        "    test_ids = test_coco.getImgIds()\n",
        "\n",
        "    # Use is_training=False to ensure validation uses perfect boxes\n",
        "    test_dataset = HvacSamDataset(test_coco, test_ids, test_path, is_training=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
        "                             num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
        "\n",
        "    print(f\"\\n‚úÖ Test dataset loaded with {len(test_dataset)} samples.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Test split not found: {e}. Skipping final evaluation.\")\n",
        "\n",
        "# 3. Run a single evaluation pass on the test data\n",
        "if 'test_loader' in locals() and 'eval_model' in locals():\n",
        "    print(\"\\nüöÄ Running final evaluation on the test set...\")\n",
        "    final_test_metrics = run_epoch(eval_model, test_loader, optimizer=None, is_training=False, device=CONFIG['device'], epoch=0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"      üéâ FINAL UNBIASED PERFORMANCE METRICS üéâ\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Final Test IoU:   {final_test_metrics['iou']:.4f}\")\n",
        "    print(f\"Final Test Loss:  {final_test_metrics['loss']:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nThis is the true expected performance of your model on new data.\")"
      ],
      "metadata": {
        "id": "anl9NMkohmRj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}