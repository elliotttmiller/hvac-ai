{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 HVAC AI \u2014 Production-Ready YOLO11 Inference Server\n",
        "**Optimized Turn-Key Backend/Inference Notebook**\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udccb Overview\n",
        "Production-ready YOLO11 inference deployment with:\n",
        "- \u2705 Comprehensive GPU & dependency validation\n",
        "- \u2705 Optimized configuration management\n",
        "- \u2705 Error handling & monitoring\n",
        "- \u2705 Testing & benchmarking\n",
        "- \u2705 Security best practices\n",
        "- \u2705 Turn-key deployment\n",
        "\n",
        "## \ud83c\udfaf Prerequisites\n",
        "1. **GPU Runtime**: T4 or better (Runtime \u2192 Change runtime type \u2192 GPU)\n",
        "2. **Trained Model**: YOLO11 `.pt` file in Google Drive\n",
        "3. **Ngrok Token**: Free token from [ngrok.com](https://ngrok.com/)\n",
        "4. **Test Image**: Sample HVAC blueprint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udd27 Environment Setup & Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Clone repository\n",
        "print(\"\\n\ud83d\udce6 Cloning repository...\")\n",
        "!git clone https://github.com/elliotttmiller/hvac-ai.git 2>/dev/null || echo \"Repository exists\"\n",
        "%cd hvac-ai\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\n\ud83d\udcda Installing dependencies (2-3 minutes)...\")\n",
        "!pip install -q ultralytics>=8.0.0 fastapi>=0.115.0 uvicorn[standard]>=0.34.0\n",
        "!pip install -q python-multipart>=0.0.9 pyngrok>=7.0.0 python-dotenv>=1.0.0\n",
        "!pip install -q Pillow>=10.0.0 numpy>=1.24.0 tqdm>=4.65.0\n",
        "\n",
        "# Validate environment\n",
        "print(\"\\n\ud83d\udd0d System Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import torch\n",
        "print(f\"\ud83d\udc0d Python: {sys.version.split()[0]}\")\n",
        "print(f\"\ud83d\udd25 PyTorch: {torch.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n\u2705 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"   CUDA: {torch.version.cuda}\")\n",
        "    # Test GPU\n",
        "    test_tensor = torch.rand(1000, 1000).cuda()\n",
        "    _ = torch.matmul(test_tensor, test_tensor)\n",
        "    print(f\"   Test: \u2705 PASSED\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f  WARNING: No GPU! Set Runtime > GPU. Inference will be SLOW.\")\n",
        "\n",
        "print(\"\\n\u2705 Environment Ready!\")\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive for model access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"\u2705 Drive mounted at: /content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"\u2699\ufe0f  Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- UPDATE THESE VALUES ---\n",
        "MODEL_PATH = \"/content/drive/MyDrive/hvac_detection_project/runs/segment/hvac_yolo11_segmentation_sota/weights/best.pt\"\n",
        "NGROK_AUTHTOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # Get from ngrok.com\n",
        "\n",
        "# Server settings\n",
        "PORT = 8000\n",
        "DEFAULT_CONF_THRESHOLD = 0.50\n",
        "DEFAULT_IOU_THRESHOLD = 0.45\n",
        "MAX_IMAGE_SIZE = 1024\n",
        "\n",
        "# Validation\n",
        "errors = []\n",
        "if not MODEL_PATH or not os.path.exists(MODEL_PATH):\n",
        "    errors.append(\"\u274c MODEL_PATH invalid or not found\")\n",
        "else:\n",
        "    print(f\"\u2705 Model: {MODEL_PATH}\")\n",
        "    print(f\"   Size: {os.path.getsize(MODEL_PATH) / 1e6:.1f} MB\")\n",
        "\n",
        "if not NGROK_AUTHTOKEN or NGROK_AUTHTOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"\u26a0\ufe0f  Ngrok token not set (optional, for public URL)\")\n",
        "else:\n",
        "    print(f\"\u2705 Ngrok: {'*' * 20}{NGROK_AUTHTOKEN[-8:]}\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Inference: conf={DEFAULT_CONF_THRESHOLD}, iou={DEFAULT_IOU_THRESHOLD}, size={MAX_IMAGE_SIZE}\")\n",
        "\n",
        "# Write .env\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"MODEL_PATH={MODEL_PATH}\\nNGROK_AUTHTOKEN={NGROK_AUTHTOKEN}\\nPORT={PORT}\\n\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\n\u274c Errors:\", \"\\n\".join(errors))\n",
        "else:\n",
        "    print(\"\\n\u2705 Configuration valid\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"\ud83e\udd16 Model Loading & Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n\ud83d\udce5 Loading model (10-30s)...\")\n",
        "start = time.time()\n",
        "model = YOLO(MODEL_PATH)\n",
        "print(f\"\u2705 Loaded in {time.time() - start:.2f}s\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Model Info:\")\n",
        "print(f\"   Device: {model.device}\")\n",
        "print(f\"   Classes: {len(model.names)}\")\n",
        "for idx, name in model.names.items():\n",
        "    print(f\"   [{idx}] {name}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')\n",
        "    print(f\"\\n\ud83d\ude80 Model on GPU\")\n",
        "\n",
        "# Warm-up\n",
        "print(f\"\\n\ud83d\udd25 Warm-up inference...\")\n",
        "dummy = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
        "start = time.time()\n",
        "_ = model.predict(dummy, verbose=False, conf=0.25)\n",
        "first_time = time.time() - start\n",
        "start = time.time()\n",
        "_ = model.predict(dummy, verbose=False, conf=0.25)\n",
        "second_time = time.time() - start\n",
        "\n",
        "print(f\"   First: {first_time*1000:.1f}ms\")\n",
        "print(f\"   Subsequent: {second_time*1000:.1f}ms (~{1.0/second_time:.0f} FPS)\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n\ud83d\udcbe GPU Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "\n",
        "print(\"\\n\u2705 Model ready!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "print(\"\ud83e\uddea Test Inference\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\ud83d\udce4 Upload test image...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    img_path = list(uploaded.keys())[0]\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_array = np.array(img)\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Image: {img.size[0]}x{img.size[1]}\")\n",
        "    \n",
        "    print(f\"\\n\ud83d\udd04 Running inference...\")\n",
        "    start = time.time()\n",
        "    results = model.predict(img_array, conf=DEFAULT_CONF_THRESHOLD, \n",
        "                           iou=DEFAULT_IOU_THRESHOLD, imgsz=MAX_IMAGE_SIZE, verbose=False)\n",
        "    inf_time = (time.time() - start) * 1000\n",
        "    \n",
        "    result = results[0]\n",
        "    boxes = result.boxes\n",
        "    \n",
        "    print(f\"\\n\u2705 Complete: {inf_time:.1f}ms ({1000.0/inf_time:.1f} FPS)\")\n",
        "    print(f\"   Detections: {len(boxes)}\")\n",
        "    \n",
        "    if len(boxes) > 0:\n",
        "        class_counts = {}\n",
        "        for box in boxes:\n",
        "            cls_id = int(box.cls[0])\n",
        "            name = model.names[cls_id]\n",
        "            class_counts[name] = class_counts.get(name, 0) + 1\n",
        "        print(f\"\\n\ud83d\udcca By Class:\")\n",
        "        for name, count in sorted(class_counts.items()):\n",
        "            print(f\"   {name}: {count}\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "    ax1.imshow(img_array)\n",
        "    ax1.set_title('Original')\n",
        "    ax1.axis('off')\n",
        "    ax2.imshow(result.plot())\n",
        "    ax2.set_title(f'{len(boxes)} detections ({inf_time:.0f}ms)')\n",
        "    ax2.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\u274c No image uploaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "print(\"\ud83d\ude80 Deploying API Server\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Validate configuration\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"\\n\u274c ERROR: MODEL_PATH not found. Check configuration.\")\n",
        "    raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
        "\n",
        "print(f\"\u2705 Model found: {MODEL_PATH}\")\n",
        "\n",
        "# Setup ngrok tunnel\n",
        "if NGROK_AUTHTOKEN and NGROK_AUTHTOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"\\n\ud83c\udf10 Setting up ngrok tunnel...\")\n",
        "    ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "    public_url = ngrok.connect(PORT)\n",
        "    print(f\"\\n\u2705 API LIVE!\")\n",
        "    print(f\"   Public URL: {public_url.public_url}\")\n",
        "    print(f\"   API Docs: {public_url.public_url}/docs\")\n",
        "    print(f\"   Health: {public_url.public_url}/health\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f  No ngrok token - server will be local only\")\n",
        "    print(f\"   Local URL: http://localhost:{PORT}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83c\udfac Starting server (Press STOP button to shutdown)...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Validate python-services directory exists\n",
        "if not os.path.exists('python-services'):\n",
        "    print(\"\u274c ERROR: python-services directory not found\")\n",
        "    print(f\"   Current directory: {os.getcwd()}\")\n",
        "    print(\"   Please ensure you're in the hvac-ai repository root\")\n",
        "    raise FileNotFoundError(\"python-services directory not found\")\n",
        "\n",
        "%cd python-services\n",
        "# Use PORT variable via Python string formatting\n",
        "import subprocess\n",
        "subprocess.run([\"uvicorn\", \"hvac_analysis_service:app\", \"--host\", \"0.0.0.0\", \"--port\", str(PORT), \"--reload\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}