{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbJThT5Ol_lr"
      },
      "source": [
        "# üü£ HVAC-Specific SAM Fine-Tuning Pipeline (v2 - Advanced)\n",
        "## üîß Enhanced with Multi-Prompt Training, Resumability, and T4 Optimizations\n",
        "\n",
        "This notebook has been upgraded to include advanced features for creating a more robust and flexible model, inspired by recent research (e.g., SAM-PAR) and optimized for T4 GPU runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1_header"
      },
      "source": [
        "### Phase 1: Initial Setup and T4 Optimizations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BiBpFHzYU4Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0oru8hAn6q2"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install torch torchvision --quiet\n",
        "!pip install opencv-python pycocotools matplotlib onnxruntime onnx --quiet\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwmQm0C3n_3D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools import mask as mask_utils\n",
        "import random\n",
        "import gc\n",
        "import contextlib\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# SAM imports\n",
        "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Clear memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LayerNorm2d_def"
      },
      "outputs": [],
      "source": [
        "# LayerNorm2d from SAM repository\n",
        "class LayerNorm2d(nn.Module):\n",
        "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        u = x.mean(1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.eps)\n",
        "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvacOptimizedDecoder_def"
      },
      "outputs": [],
      "source": [
        "# Custom HVAC-Optimized Decoder (Blog #2 Inspired)\n",
        "class HvacOptimizedDecoder(nn.Module):\n",
        "    \"\"\"Memory-efficient decoder inspired by Blog #2, optimized for HVAC components\"\"\"\n",
        "    def __init__(self, sam_encoder):\n",
        "        super().__init__()\n",
        "        self.sam_encoder = sam_encoder\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        \n",
        "        # Transposed convolutions for upsampling (more efficient than SAM's decoder)\n",
        "        self.conv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.norm1 = LayerNorm2d(128)\n",
        "        self.conv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.norm2 = LayerNorm2d(64)\n",
        "        self.conv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.norm3 = LayerNorm2d(32)\n",
        "        self.conv4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
        "        self.norm4 = LayerNorm2d(16)\n",
        "        self.conv5 = nn.Conv2d(16, 1, kernel_size=1)  # Final 1x1 conv for binary mask\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.sam_encoder(x)\n",
        "        x = torch.nn.functional.relu(self.norm1(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.nn.functional.relu(self.norm2(self.conv2(x)))\n",
        "        x = torch.nn.functional.relu(self.norm3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.nn.functional.relu(self.norm4(self.conv4(x)))\n",
        "        x = torch.sigmoid(self.conv5(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2_header"
      },
      "source": [
        "### Phase 2: Configuration and Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdTD9CTxKena"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "CONFIG = {\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # --- STARTING MODEL ---\n",
        "    'model_path': '/content/sam_vit_h_4b8939.pth',\n",
        "\n",
        "    # --- DATASET ---\n",
        "    'dataset_zip_path': '/content/drive/MyDrive/hvac_dataset_coco.zip',\n",
        "    'unzip_path': '/content/drive/MyDrive/hvac_dataset_coco',\n",
        "    'annotations_file_name': '_annotations.coco.json',\n",
        "\n",
        "    # --- OUTPUT & CHECKPOINTING ---\n",
        "    'output_dir': '/content/drive/MyDrive/sam_finetuning_results',\n",
        "    'best_model_save_path': '/content/drive/MyDrive/sam_finetuning_results/best_model_multiprompt_v1.pth',\n",
        "    'latest_checkpoint_save_path': '/content/drive/MyDrive/sam_finetuning_results/latest_checkpoint_multiprompt_v1.pth',\n",
        "    'resume_training': False, # SET TO TRUE TO RESUME FROM 'latest_checkpoint_save_path'\n",
        "\n",
        "    # --- PROMPT ENGINEERING STRATEGY ---\n",
        "    'prompt_strategy': 'multi_prompt',\n",
        "    'bbox_noise_factor': 0.1,\n",
        "\n",
        "    # --- MODEL & TRAINING HYPERPARAMETERS (T4 Optimized) ---\n",
        "    'model_type': 'vit_h',\n",
        "    'image_size': 1024,\n",
        "    'batch_size': 1,  # Critical for T4\n",
        "    'num_workers': 2, # Reduced for Colab RAM\n",
        "    'num_epochs': 1,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0,\n",
        "    'early_stopping_patience': 10,\n",
        "    'checkpoint_batch_interval': 300,\n",
        "    'min_mask_area': 100,\n",
        "    \n",
        "    # --- T4 SPECIFIC OPTIMIZATIONS ---\n",
        "    'mixed_precision': True,\n",
        "    'gradient_checkpointing': True,  # Implemented manually later\n",
        "    'use_custom_decoder': True,      # Enable custom HVAC decoder\n",
        "    'encoder_frozen': True,          # Freeze 99% of SAM parameters\n",
        "    'neck_unfrozen': True,           # Only train Conv2D neck + decoder\n",
        "    'clear_cache_every': 5,          # Clear CUDA cache every N batches\n",
        "    \n",
        "}\n",
        "\n",
        "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úì Configuration loaded. Using device: {CONFIG['device']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset if it hasn't been already\n",
        "if not os.path.exists(CONFIG['unzip_path']):\n",
        "    print(f\"üìÅ Unzipping dataset from {CONFIG['dataset_zip_path']}...\")\n",
        "    with zipfile.ZipFile(CONFIG['dataset_zip_path'], 'r') as zip_ref:\n",
        "        zip_ref.extractall(CONFIG['unzip_path'])\n",
        "    print(\"‚úÖ Unzipping complete.\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset already unzipped.\")"
      ],
      "metadata": {
        "id": "unzip_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase3_header"
      },
      "source": [
        "### Phase 3: Dataset Loading and DataLoader Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loading_cell"
      },
      "outputs": [],
      "source": [
        "def load_coco_split(dataset_root_path: str, split_name: str, annotations_file: str) -> Tuple[COCO, str]:\n",
        "    split_path = os.path.join(dataset_root_path, split_name)\n",
        "    annotations_path = os.path.join(split_path, annotations_file)\n",
        "    if not os.path.exists(annotations_path):\n",
        "        raise FileNotFoundError(f\"Annotations file not found for '{split_name}' at: {annotations_path}\")\n",
        "    print(f\"üîÑ Loading '{split_name}' annotations from: {annotations_path}\")\n",
        "    coco = COCO(annotations_path)\n",
        "    print(f\"üìä Found {len(coco.getImgIds())} images in '{split_name}'.\")\n",
        "    return coco, split_path\n",
        "\n",
        "def get_image_path(split_path: str, img_info: dict) -> str:\n",
        "    full_path = os.path.join(split_path, img_info['file_name'])\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Image file not found: {img_info['file_name']} in {split_path}\")\n",
        "    return full_path\n",
        "\n",
        "class HvacSamDataset(Dataset):\n",
        "    def __init__(self, coco: COCO, image_ids: List[int], split_path: str, is_training: bool = True):\n",
        "        self.coco = coco\n",
        "        self.image_ids = image_ids\n",
        "        self.split_path = split_path\n",
        "        self.is_training = is_training\n",
        "        self.resize_transform = ResizeLongestSide(CONFIG['image_size'])\n",
        "        self.pixel_mean = torch.tensor([123.675, 116.28, 103.53]).view(-1, 1, 1)\n",
        "        self.pixel_std = torch.tensor([58.395, 57.12, 57.375]).view(-1, 1, 1)\n",
        "        self.prompt_strategy = CONFIG.get('prompt_strategy', 'perfect_box')\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def _generate_prompt(self, mask: np.ndarray, bbox: List[float]):\n",
        "        # For validation/testing, always use the perfect box for consistent evaluation\n",
        "        if not self.is_training:\n",
        "            return {'box': np.array(bbox)}\n",
        "\n",
        "        # Determine the prompt type for this training item\n",
        "        if self.prompt_strategy == 'multi_prompt':\n",
        "            prompt_type = random.choice(['box', 'point', 'scribble'])\n",
        "        elif self.prompt_strategy == 'point':\n",
        "            prompt_type = 'point'\n",
        "        elif self.prompt_strategy == 'noisy_box':\n",
        "            prompt_type = 'box'\n",
        "        else: # 'perfect_box'\n",
        "            prompt_type = 'box'\n",
        "\n",
        "        if prompt_type == 'box':\n",
        "            noise_factor = CONFIG.get('bbox_noise_factor', 0) if self.prompt_strategy == 'noisy_box' or self.prompt_strategy == 'multi_prompt' else 0\n",
        "            x, y, w, h = bbox\n",
        "            x_noise = w * noise_factor * (random.random() - 0.5) * 2\n",
        "            y_noise = h * noise_factor * (random.random() - 0.5) * 2\n",
        "            w_noise = w * noise_factor * (random.random() - 0.5) * 2\n",
        "            h_noise = h * noise_factor * (random.random() - 0.5) * 2\n",
        "            box = np.array([x + x_noise, y + y_noise, w + w_noise, h + h_noise])\n",
        "            return {'box': box}\n",
        "\n",
        "        points = np.argwhere(mask)\n",
        "        if len(points) == 0: return None\n",
        "\n",
        "        if prompt_type == 'point':\n",
        "            point = points[random.randint(0, len(points) - 1)]\n",
        "            point_coords = np.array([[point[1], point[0]]]) # (x, y)\n",
        "            point_labels = np.array([1])\n",
        "            return {'point_coords': point_coords, 'point_labels': point_labels}\n",
        "\n",
        "        elif prompt_type == 'scribble':\n",
        "            num_points = min(5, len(points))\n",
        "            point_indices = np.random.choice(len(points), num_points, replace=False)\n",
        "            scribble_points = points[point_indices]\n",
        "            point_coords = scribble_points[:, ::-1] # (row, col) -> (x, y)\n",
        "            point_labels = np.ones(num_points)\n",
        "            return {'point_coords': point_coords, 'point_labels': point_labels}\n",
        "\n",
        "        return None\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_info = self.coco.loadImgs([img_id])[0]\n",
        "        image_path = get_image_path(self.split_path, img_info)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
        "        annotations = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        masks, prompts = [], []\n",
        "        for ann in annotations:\n",
        "            if 'segmentation' not in ann or ann.get('iscrowd', 0) == 1: continue\n",
        "            mask = self.coco.annToMask(ann)\n",
        "            if mask.sum() < CONFIG['min_mask_area']: continue\n",
        "\n",
        "            prompt = self._generate_prompt(mask, ann['bbox'])\n",
        "            if prompt:\n",
        "                masks.append(mask.astype(bool))\n",
        "                prompts.append(prompt)\n",
        "\n",
        "        original_size = image.shape[:2]\n",
        "        resized_image = self.resize_transform.apply_image(image)\n",
        "        input_image_torch = torch.as_tensor(resized_image, dtype=torch.float32).permute(2, 0, 1).contiguous()\n",
        "        input_image_torch = (input_image_torch - self.pixel_mean) / self.pixel_std\n",
        "        h, w = input_image_torch.shape[-2:]\n",
        "        padh, padw = CONFIG['image_size'] - h, CONFIG['image_size'] - w\n",
        "        input_image_padded = torch.nn.functional.pad(input_image_torch, (0, padw, 0, padh))\n",
        "\n",
        "        return {\n",
        "            'image': input_image_padded,\n",
        "            'masks': masks,\n",
        "            'prompts': prompts,\n",
        "            'original_size': original_size,\n",
        "            'input_size': (h, w)\n",
        "        }\n",
        "\n",
        "def custom_collate_fn(batch: List[Dict]) -> Dict[str, Any]:\n",
        "    return {\n",
        "        'image': torch.stack([item['image'] for item in batch]),\n",
        "        'masks': [item['masks'] for item in batch],\n",
        "        'prompts': [item['prompts'] for item in batch],\n",
        "        'original_size': [item['original_size'] for item in batch],\n",
        "        'input_size': [item['input_size'] for item in batch]\n",
        "    }\n",
        "\n",
        "# Load datasets\n",
        "train_coco, train_path = load_coco_split(CONFIG['unzip_path'], 'train', CONFIG['annotations_file_name'])\n",
        "val_coco, val_path = load_coco_split(CONFIG['unzip_path'], 'valid', CONFIG['annotations_file_name'])\n",
        "train_ids, val_ids = train_coco.getImgIds(), val_coco.getImgIds()\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = HvacSamDataset(train_coco, train_ids, train_path, is_training=True)\n",
        "val_dataset = HvacSamDataset(val_coco, val_ids, val_path, is_training=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
        "\n",
        "print(f\"\\n‚úÖ Training dataset initialized with {len(train_dataset)} samples.\")\n",
        "print(f\"‚úÖ Validation dataset initialized with {len(val_dataset)} samples.\")\n",
        "\n",
        "print(f\"\\nInspecting dataset directory: {CONFIG['unzip_path']}\")\n",
        "!ls -R {CONFIG['unzip_path']}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Define URL for the official ViT-H model\n",
        "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
        "CHECKPOINT_PATH = \"/content/sam_vit_h_4b8939.pth\"\n",
        "\n",
        "print(f\"‚¨áÔ∏è Downloading official SAM ViT-H weights to {CHECKPOINT_PATH}...\")\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    urllib.request.urlretrieve(CHECKPOINT_URL, CHECKPOINT_PATH)\n",
        "    print(\"‚úÖ Download complete.\")\n",
        "else:\n",
        "    print(\"‚úÖ File already exists.\")"
      ],
      "metadata": {
        "id": "du1oZDKE6GLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase4_header"
      },
      "source": [
        "### Phase 4: Model Preparation and Training Setup (T4 Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjTIJtLxP8ZG"
      },
      "outputs": [],
      "source": [
        "# Initialize base SAM model\n",
        "sam_model = sam_model_registry[CONFIG['model_type']]()\n",
        "sam_model.to(CONFIG['device'])\n",
        "\n",
        "# Load pre-trained weights\n",
        "print(f\"üîÑ Loading pre-trained model: {CONFIG['model_path']}\")\n",
        "sam_model.load_state_dict(torch.load(CONFIG['model_path'], map_location=CONFIG['device']))\n",
        "\n",
        "# --- T4 OPTIMIZATION: FREEZE & CUSTOM DECODER SETUP ---\n",
        "if CONFIG['use_custom_decoder']:\n",
        "    print(\"üîß Switching to custom HVAC-optimized decoder...\")\n",
        "    # Freeze the main ViT encoder\n",
        "    for param in sam_model.image_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Only keep the Conv2D neck unfrozen if specified\n",
        "    if CONFIG['neck_unfrozen']:\n",
        "        for name, param in sam_model.image_encoder.named_parameters():\n",
        "            if 'neck' in name:\n",
        "                param.requires_grad = True\n",
        "    \n",
        "    # Replace the mask decoder with the custom one\n",
        "    hvac_decoder = HvacOptimizedDecoder(sam_model.image_encoder)\n",
        "    hvac_decoder.to(CONFIG['device'])\n",
        "    model_to_train = hvac_decoder\n",
        "    \n",
        "    # Optimizer only for the new decoder and neck\n",
        "    trainable_params = [p for p in hvac_decoder.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(trainable_params, lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "    \n",
        "    print(f\"‚úÖ Custom decoder initialized. Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
        "else:\n",
        "    print(\"üîß Using standard SAM mask decoder...\")\n",
        "    # Standard fine-tuning setup\n",
        "    for name, param in sam_model.named_parameters():\n",
        "        if name.startswith(\"image_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "            param.requires_grad = False\n",
        "    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "    model_to_train = sam_model\n",
        "    trainable_params = sum(p.numel() for p in model_to_train.parameters() if p.requires_grad)\n",
        "    print(f\"‚úÖ Standard SAM configured for fine-tuning. Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "start_epoch = 0\n",
        "# --- NEW: RESUME TRAINING LOGIC ---\n",
        "if CONFIG['resume_training']:\n",
        "    print(f\"üîÑ Attempting to resume training from {CONFIG['latest_checkpoint_save_path']}\")\n",
        "    if os.path.exists(CONFIG['latest_checkpoint_save_path']):\n",
        "        checkpoint = torch.load(CONFIG['latest_checkpoint_save_path'], map_location=CONFIG['device'])\n",
        "        model_to_train.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"‚úÖ Resumed successfully. Starting from epoch {start_epoch}.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Resume checkpoint not found. Starting training from scratch with pre-trained SAM.\")\n",
        "        # Already loaded the base model above\n",
        "\n",
        "model_to_train.train()\n",
        "scaler = GradScaler() if CONFIG['mixed_precision'] and CONFIG['device'] == 'cuda' else None\n",
        "print(f\"‚úÖ Model and optimizer configured for T4 runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtPYpirbK3Wi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Setup scheduler and loss function\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "def combined_loss(pred_masks: torch.Tensor, true_masks: torch.Tensor) -> torch.Tensor:\n",
        "    bce_loss = nn.BCEWithLogitsLoss()(pred_masks, true_masks.float())\n",
        "    pred_flat = torch.sigmoid(pred_masks).reshape(-1)\n",
        "    true_flat = true_masks.reshape(-1)\n",
        "    intersection = (pred_flat * true_flat).sum()\n",
        "    dice_loss = 1 - (2. * intersection + 1e-8) / (pred_flat.sum() + true_flat.sum() + 1e-8)\n",
        "    return 0.8 * bce_loss + 0.2 * dice_loss\n",
        "\n",
        "print(\"‚úÖ Scheduler and loss function configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase5_header"
      },
      "source": [
        "### Phase 5: Memory-Efficient Training and Validation Loop (T4 Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRQ6yd_PM_B9"
      },
      "outputs": [],
      "source": [
        "### Phase 5: Memory-Efficient Training and Validation Loop (T4 Optimized)\n",
        "\n",
        "def process_single_annotation(model, image_embeddings, gt_mask, prompt, orig_size, input_size, device):\n",
        "    \"\"\"Process a single mask-prompt pair.\"\"\"\n",
        "    gt_mask_torch = torch.from_numpy(gt_mask).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    transform = ResizeLongestSide(CONFIG['image_size'])\n",
        "\n",
        "    box_torch, points_torch, points_label_torch = None, None, None\n",
        "    if 'box' in prompt:\n",
        "        box_torch = torch.as_tensor(transform.apply_boxes(prompt['box'].reshape(1, 4), orig_size), dtype=torch.float, device=device)\n",
        "    elif 'point_coords' in prompt:\n",
        "        points_torch = torch.as_tensor(transform.apply_coords(prompt['point_coords'], orig_size), dtype=torch.float, device=device).unsqueeze(0)\n",
        "        points_label_torch = torch.as_tensor(prompt['point_labels'], dtype=torch.float, device=device).unsqueeze(0)\n",
        "\n",
        "    if isinstance(model, HvacOptimizedDecoder):\n",
        "        # Custom decoder path\n",
        "        upscaled_masks = model(image_embeddings)\n",
        "    else:\n",
        "        # Standard SAM path\n",
        "        with torch.no_grad():\n",
        "            sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
        "                points=(points_torch, points_label_torch) if points_torch is not None else None,\n",
        "                boxes=box_torch,\n",
        "                masks=None\n",
        "            )\n",
        "\n",
        "        low_res_masks, iou_predictions = model.mask_decoder(\n",
        "            image_embeddings=image_embeddings,\n",
        "            image_pe=model.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embeddings,\n",
        "            dense_prompt_embeddings=dense_embeddings,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        "        upscaled_masks = model.postprocess_masks(low_res_masks, input_size, orig_size)\n",
        "\n",
        "    return combined_loss(upscaled_masks, gt_mask_torch)\n",
        "\n",
        "\n",
        "def run_epoch_optimized(model, dataloader, optimizer, is_training, device, epoch, scaler):\n",
        "    \"\"\"Memory-optimized training loop for T4 GPU\"\"\"\n",
        "    model.train(is_training)\n",
        "    epoch_losses, iou_scores = [], []\n",
        "\n",
        "    desc = \"Training\" if is_training else \"Validation\"\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=desc)):\n",
        "        try:\n",
        "            images = batch['image'].to(device, non_blocking=True)\n",
        "            all_gt_masks_list = batch['masks']\n",
        "            all_prompts_list = batch['prompts']\n",
        "\n",
        "            if is_training:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(is_training):\n",
        "                # Mixed precision training context\n",
        "                with autocast() if scaler else contextlib.nullcontext():\n",
        "                    if isinstance(model, HvacOptimizedDecoder):\n",
        "                        # Custom decoder: process the whole image at once\n",
                        "image_embeddings = images # The custom decoder incorporates the encoder\n",
                        "upscaled_masks = model(images)\n",
                        "\n",
                        "# Handle multiple masks in the image\n",
                        "total_loss = 0\n",
                        "num_annotations_in_image = 0\n",
                        "for j in range(len(all_gt_masks_list[0])):\n",
                        "    gt_mask_np = all_gt_masks_list[0][j]\n",
                        "    gt_mask_torch = torch.from_numpy(gt_mask_np).unsqueeze(0).unsqueeze(0).to(device)\n",
                        "    loss = combined_loss(upscaled_masks, gt_mask_torch)\n",
                        "    total_loss += loss\n",
                        "    num_annotations_in_image += 1\n",
                        "\n",
                        "if is_training:\n",
                        "    if scaler:\n",
                        "        scaler.scale(total_loss).backward()\n",
                        "        scaler.step(optimizer)\n",
                        "        scaler.update()\n",
                        "    else:\n",
                        "        total_loss.backward()\n",
                        "        optimizer.step()\n",
                        "\n",
                        "if num_annotations_in_image > 0:\n",
                        "    epoch_losses.append(total_loss.item() / num_annotations_in_image)\n",
                        "                    else:\n",
                        "                        # Standard SAM: process one annotation at a time\n",
                        "                        image_embeddings = model.image_encoder(images)\n",
                        "                        total_loss = 0\n",
                        "                        num_annotations_in_image = 0\n",
                        "\n",
                        "                        for j in range(len(all_gt_masks_list[0])):\n",
                        "                            gt_mask_np = all_gt_masks_list[0][j]\n",
                        "                            prompt = all_prompts_list[0][j]\n",
                        "                            num_annotations_in_image += 1\n",
                        "\n",
                        "                            loss = process_single_annotation(\n",
                        "                                model, image_embeddings, gt_mask_np, prompt,\n",
                        "                                batch['original_size'][0], batch['input_size'][0], device\n",
                        "                            )\n",
                        "\n                        total_loss += loss\n",
                        "\n",
                        "                        # --- GRADIENT ACCUMULATION STEP ---\n",
                        "                        if is_training and num_annotations_in_image > 0:\n",
                        "                            avg_loss = total_loss / num_annotations_in_image\n",
                        "                            if scaler:\n",
                        "                                scaler.scale(avg_loss).backward()\n",
                        "                                scaler.step(optimizer)\n",
                        "                                scaler.update()\n",
                        "                            else:\n",
                        "                                avg_loss.backward()\n",
                        "                                optimizer.step()\n",
                        "\n",
                        "                        if num_annotations_in_image > 0:\n",
                        "                            epoch_losses.append(total_loss.item() / num_annotations_in_image)\n",
                        "\n",
                        "# Memory cleanup\n",
                        "del images\n",
                        "if not isinstance(model, HvacOptimizedDecoder):\n",
                        "    del image_embeddings\n",
                        "torch.cuda.empty_cache()\n",
                        "gc.collect()\n",
                        "\n",
                        "# Checkpoint saving logic\n",
                        "if is_training and (batch_idx + 1) % CONFIG['checkpoint_batch_interval'] == 0:\n",
                        "    chk_path = CONFIG['latest_checkpoint_save_path']\n",
                        "    torch.save({\n",
                        "        'epoch': epoch,\n",
                        "        'model_state_dict': model.state_dict(),\n",
                        "        'optimizer_state_dict': optimizer.state_dict(),\n",
                        "        'scaler_state_dict': scaler.state_dict() if scaler else None\n",
                        "    }, chk_path)\n",
                        "    tqdm.write(f\"\\nüíæ Overwrote latest checkpoint (for crash recovery): {os.path.basename(chk_path)}\")\n",
                        "\n",
                        "        except RuntimeError as e:\n",
                        "            if \"out of memory\" in str(e):\n",
                        "                print(f\"‚ö†Ô∏è OOM at batch {batch_idx}, cleaning up...\")\n",
                        "                optimizer.zero_grad(set_to_none=True)\n",
                        "                torch.cuda.empty_cache()\n",
                        "                gc.collect()\n",
                        "                continue  # Skip this batch\n",
                        "            else:\n",
                        "                raise e\n",
                        "\n",
                        "    return {'loss': mean(epoch_losses) if epoch_losses else 0, 'iou': mean(iou_scores) if iou_scores else 0}\n",
                        "\n",
                        "\n",
                        "# --- Main Training Loop ---\n",
                        "best_val_iou = 0\n",
                        "patience_counter = 0\n",
                        "history = {'train_loss': [], 'val_loss': [], 'val_iou': []}\n",
                        "\n",
                        "print(f\"\\nüöÄ Starting training from epoch {start_epoch} for {CONFIG['num_epochs']} total epochs...\")\n",
                        "for epoch in range(start_epoch, CONFIG['num_epochs']):\n",
                        "    print(f\"\\n--- Epoch {epoch+1}/{CONFIG['num_epochs']} ---\")\n",
                        "\n",
                        "    train_metrics = run_epoch_optimized(model_to_train, train_loader, optimizer, is_training=True, device=CONFIG['device'], epoch=epoch, scaler=scaler)\n",
                        "    history['train_loss'].append(train_metrics['loss'])\n",
                        "\n",
                        "    val_metrics = run_epoch_optimized(model_to_train, val_loader, None, is_training=False, device=CONFIG['device'], epoch=epoch, scaler=None)\n",
                        "    history['val_loss'].append(val_metrics['loss'])\n",
                        "    history['val_iou'].append(val_metrics['iou'])\n",
                        "\n",
                        "    print(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
                        "    print(f\"Val Loss: {val_metrics['loss']:.4f} | Val IoU: {val_metrics['iou']:.4f}\")\n",
                        "\n",
                        "    scheduler.step(val_metrics['loss'])\n",
                        "\n",
                        "    if val_metrics['iou'] > best_val_iou:\n",
                        "        best_val_iou = val_metrics['iou']\n",
                        "        patience_counter = 0\n",
                        "        best_model_path = CONFIG['best_model_save_path']\n",
                        "        torch.save(model_to_train.state_dict(), best_model_path)\n",
                        "        print(f\"üèÜ New best model saved with IoU: {best_val_iou:.4f}\")\n",
                        "    else:\n",
                        "        patience_counter += 1\n",
                        "\n",
                        "    if patience_counter >= CONFIG['early_stopping_patience']:\n",
                        "        print(f\"üõë Early stopping triggered after {patience_counter} epochs with no improvement.\")\n",
                        "        break\n",
                        "\n",
                        "print(\"\\n‚úÖ Training completed!\")"
                      ]
                    },
                    {
                      "cell_type": "markdown",
                      "metadata": {
                        "id": "phase6_header"
                      },
                      "source": [
                        "### Phase 6: Results and Export"
                      ]
                    },
                    {
                      "cell_type": "code",
                      "execution_count": null,
                      "metadata": {
                        "id": "UKqIxUgAOTzp_1"
                      },
                      "outputs": [],
                      "source": [
                        "# Plot training metrics\n",
                        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                        "\n",
                        "ax1.plot(history['train_loss'], label='Training Loss', color='blue')\n",
                        "ax1.plot(history['val_loss'], label='Validation Loss', color='red')\n",
                        "ax1.set_title('Training and Validation Loss')\n",
                        "ax1.set_xlabel('Epoch')\n",
                        "ax1.set_ylabel('Loss')\n",
                        "ax1.legend()\n",
                        "ax1.grid(True)\n",
                        "\n",
                        "ax2.plot(history['val_iou'], label='Validation IoU', color='green')\n",
                        "ax2.set_title('Validation Metrics (IoU)')\n",
                        "ax2.set_xlabel('Epoch')\n",
                        "ax2.set_ylabel('Score')\n",
                        "ax2.legend()\n",
                        "ax2.grid(True)\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.savefig(os.path.join(CONFIG['output_dir'], 'training_metrics.png'))\n",
                        "plt.show()\n",
                        "\n",
                        "print(f\"\\nüìä FINAL METRICS:\")\n",
                        "if history['val_iou']:\n",
                        "    print(f\"Best Validation IoU: {max(history['val_iou']):.4f}\")\n",
                        "else:\n",
                        "    print(\"No validation metrics recorded.\")"
                      ]
                    },
                    {
                      "cell_type": "code",
                      "execution_count": null,
                      "metadata": {
                        "id": "UKqIxUgAOTzp_4"
                      },
                      "outputs": [],
                      "source": [
                        "# Save the final trained model (or the best one)\n",
                        "final_model_path = os.path.join(CONFIG['output_dir'], 'final_model.pth')\n",
                        "torch.save(model_to_train.state_dict(), final_model_path)\n",
                        "print(f\"‚úÖ Final model state saved to: {final_model_path}\")\n",
                        "print(f\"‚úÖ Best performing model (by IoU) saved to: {CONFIG['best_model_save_path']}\")"
                      ]
                    },
                    {
                      "cell_type": "code",
                      "source": [
                        "### Phase 7: Final, Unbiased Evaluation on the Test Set\n",
                        "\n",
                        "print(\"\\n--- Final Model Evaluation on Unseen Test Data ---\")\n",
                        "\n",
                        "# 1. Load the best performing model that was saved during training\n",
                        "best_model_path = CONFIG['best_model_save_path']\n",
                        "if os.path.exists(best_model_path):\n",
                        "    print(f\"üîÑ Loading best model from: {best_model_path}\")\n",
                        "    # We need to re-initialize the model structure before loading the state dict\n",
                        "    if CONFIG['use_custom_decoder']:\n",
                        "        eval_model = HvacOptimizedDecoder(sam_model_registry[CONFIG['model_type']]().image_encoder)\n",
                        "    else:\n",
                        "        eval_model = sam_model_registry[CONFIG['model_type']]()\n",
                        "        \n",
                        "    eval_model.load_state_dict(torch.load(best_model_path, map_location=CONFIG['device']))\n",
                        "    eval_model.to(CONFIG['device'])\n",
                        "    eval_model.eval()\n",
                        "else:\n",
                        "    print(\"‚ùå Best model file not found. Cannot perform final evaluation.\")\n",
                        "    eval_model = None\n",
                        "\n",
                        "# 2. Load the test dataset\n",
                        "try:\n",
                        "    test_coco, test_path = load_coco_split(CONFIG['unzip_path'], 'test', CONFIG['annotations_file_name'])\n",
                        "    test_ids = test_coco.getImgIds()\n",
                        "\n",
                        "    # Use is_training=False to ensure validation uses perfect boxes\n",
                        "    test_dataset = HvacSamDataset(test_coco, test_ids, test_path, is_training=False)\n",
                        "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
                        "                             num_workers=CONFIG['num_workers'], collate_fn=custom_collate_fn)\n",
                        "\n",
                        "    print(f\"\\n‚úÖ Test dataset loaded with {len(test_dataset)} samples.\")\n",
                        "\n",
                        "except FileNotFoundError as e:\n",
                        "    print(f\"\\n‚ö†Ô∏è Test split not found: {e}. Skipping final evaluation.\")\n",
                        "\n",
                        "# 3. Run a single evaluation pass on the test data\n",
                        "if 'test_loader' in locals() and eval_model is not None:\n",
                        "    print(\"\\nüöÄ Running final evaluation on the test set...\")\n",
                        "    final_test_metrics = run_epoch_optimized(eval_model, test_loader, optimizer=None, is_training=False, device=CONFIG['device'], epoch=0, scaler=None)\n",
                        "\n",
                        "    print(\"\\n\" + \"=\"*50)\n",
                        "    print(\"      üéâ FINAL UNBIASED PERFORMANCE METRICS üéâ\")\n",
                        "    print(\"=\"*50)\n",
                        "    print(f\"Final Test IoU:   {final_test_metrics['iou']:.4f}\")\n",
                        "    print(f\"Final Test Loss:  {final_test_metrics['loss']:.4f}\")\n",
                        "    print(\"=\"*50)\n",
                        "    print(\"\\nThis is the true expected performance of your model on new data.\")\n",
                        "elif eval_model is None:\n",
                        "    print(\"\\n‚ùå Skipping final evaluation due to missing model.\")"
                      ],
                      "metadata": {
                        "id": "anl9NMkohmRj"
                      },
                      "execution_count": null,
                      "outputs": []
                    }
                  ],
                  "metadata": {
                    "accelerator": "GPU",
                    "colab": {
                      "gpuType": "T4",
                      "provenance": []
                    },
                    "kernelspec": {
                      "display_name": "Python 3",
                      "name": "python3"
                    },
                    "language_info": {
                      "name": "python"
                    }
                  },
                  "nbformat": 4,
                  "nbformat_minor": 0
                }