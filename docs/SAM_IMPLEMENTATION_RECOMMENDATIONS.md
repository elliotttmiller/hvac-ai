# SAM Implementation Recommendations

**Date:** 2025-12-12  
**Based on:** [SAM_IMPLEMENTATION_AUDIT.md](./SAM_IMPLEMENTATION_AUDIT.md)  
**Priority:** CRITICAL â†’ LOW

This document provides actionable recommendations to achieve full compliance with the SAM_UPGRADE_IMPLEMENTATION.md specification.

## ðŸ”´ CRITICAL Priority

### 1. Implement Enhanced Classification System

**Current Status:** âŒ Placeholder implementation (10% compliant)

**Issue:** 
```python
def _classify_segment(self, mask: np.ndarray) -> str:
    """Functional placeholder for classification."""
    mask_sum = int(np.sum(mask))
    return HVAC_TAXONOMY[mask_sum % len(HVAC_TAXONOMY)]
```

This trivial logic essentially assigns random labels based on mask pixel count modulo.

**Required Implementation:**

#### Step 1: Extract Geometric Features
```python
def _extract_geometric_features(self, mask: np.ndarray) -> Dict[str, float]:
    """Extract geometric features from mask."""
    # Area and perimeter
    area = int(np.sum(mask))
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if not contours:
        return self._default_geometric_features()
    
    contour = max(contours, key=cv2.contourArea)
    perimeter = cv2.arcLength(contour, True)
    
    # Shape descriptors
    circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter > 0 else 0
    
    # Aspect ratio
    x, y, w, h = cv2.boundingRect(contour)
    aspect_ratio = float(w) / h if h > 0 else 1.0
    
    # Compactness
    hull = cv2.convexHull(contour)
    hull_area = cv2.contourArea(hull)
    solidity = float(area) / hull_area if hull_area > 0 else 0
    
    # Approximate polygon
    epsilon = 0.02 * perimeter
    approx = cv2.approxPolyDP(contour, epsilon, True)
    num_vertices = len(approx)
    
    return {
        'area': area,
        'perimeter': perimeter,
        'circularity': circularity,
        'aspect_ratio': aspect_ratio,
        'solidity': solidity,
        'num_vertices': num_vertices
    }
```

#### Step 2: Extract Visual Features
```python
def _extract_visual_features(self, image: np.ndarray, mask: np.ndarray) -> Dict[str, float]:
    """Extract visual features from masked region."""
    # Extract pixels in masked region
    masked_pixels = image[mask > 0]
    
    if len(masked_pixels) == 0:
        return self._default_visual_features()
    
    # Color intensity
    mean_intensity = np.mean(masked_pixels, axis=0)
    std_intensity = np.std(masked_pixels, axis=0)
    
    # Grayscale statistics
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    gray_masked = gray[mask > 0]
    
    return {
        'mean_r': float(mean_intensity[0]) if len(mean_intensity) > 0 else 0,
        'mean_g': float(mean_intensity[1]) if len(mean_intensity) > 1 else 0,
        'mean_b': float(mean_intensity[2]) if len(mean_intensity) > 2 else 0,
        'std_r': float(std_intensity[0]) if len(std_intensity) > 0 else 0,
        'std_g': float(std_intensity[1]) if len(std_intensity) > 1 else 0,
        'std_b': float(std_intensity[2]) if len(std_intensity) > 2 else 0,
        'mean_gray': float(np.mean(gray_masked)),
        'std_gray': float(np.std(gray_masked))
    }
```

#### Step 3: Component-Specific Classification Rules
```python
def _classify_by_geometric_rules(self, features: Dict[str, float]) -> List[Tuple[str, float]]:
    """Classify based on geometric heuristics."""
    scores = {}
    
    circularity = features['circularity']
    aspect_ratio = features['aspect_ratio']
    area = features['area']
    num_vertices = features['num_vertices']
    
    # Circular components (pumps, some valves)
    if circularity > 0.8:
        scores['Equipment-Pump-Centrifugal'] = 0.9
        scores['Valve-Ball'] = 0.85
        scores['Equipment-Motor'] = 0.8
    
    # Elongated components (pipes, actuators)
    if aspect_ratio > 3.0 or aspect_ratio < 0.33:
        scores['Pipe-Insulated'] = 0.8
        scores['Actuator-Pneumatic'] = 0.75
        scores['Fitting-Bend'] = 0.7
    
    # Square/rectangular (instruments, controllers)
    if 0.8 < aspect_ratio < 1.2 and circularity < 0.6:
        scores['Instrument-Pressure-Indicator'] = 0.85
        scores['Controller-PLC'] = 0.8
        scores['Controller-DCS'] = 0.75
    
    # Small components (fittings, accessories)
    if area < 500:
        scores['Fitting-Flange'] = 0.8
        scores['Accessory-Vent'] = 0.75
        scores['Fitting-Reducer'] = 0.7
    
    # Polygonal components (valves, dampers)
    if 4 <= num_vertices <= 8:
        scores['Valve-Gate'] = 0.8
        scores['Damper'] = 0.75
        scores['Valve-Butterfly'] = 0.7
    
    # Return top 3 predictions
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return sorted_scores[:3] if sorted_scores else [('Equipment-Generic', 0.5)]
```

#### Step 4: Weighted Combined Scoring
```python
def _classify_segment_enhanced(self, image: np.ndarray, mask: np.ndarray) -> Dict:
    """Enhanced multi-stage classification with confidence breakdown."""
    
    # Extract features
    geo_features = self._extract_geometric_features(mask)
    vis_features = self._extract_visual_features(image, mask)
    
    # Get geometric predictions
    geo_predictions = self._classify_by_geometric_rules(geo_features)
    
    # Get visual predictions (placeholder - can be enhanced with learned features)
    vis_predictions = self._classify_by_visual_rules(vis_features)
    
    # Combine scores with weights (60% geometric, 40% visual)
    combined_scores = {}
    
    for label, score in geo_predictions:
        combined_scores[label] = score * 0.6
    
    for label, score in vis_predictions:
        if label in combined_scores:
            combined_scores[label] += score * 0.4
        else:
            combined_scores[label] = score * 0.4
    
    # Sort and get top prediction
    sorted_scores = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
    
    if not sorted_scores:
        return {
            'label': 'Equipment-Generic',
            'confidence': 0.5,
            'confidence_breakdown': {'geometric': 0.5, 'visual': 0.5, 'combined': 0.5},
            'alternative_labels': []
        }
    
    top_label, top_score = sorted_scores[0]
    alternatives = sorted_scores[1:4]  # Top 3 alternatives
    
    # Get component scores for breakdown
    geo_score = next((s for l, s in geo_predictions if l == top_label), 0.5)
    vis_score = next((s for l, s in vis_predictions if l == top_label), 0.5)
    
    return {
        'label': top_label,
        'confidence': float(top_score),
        'confidence_breakdown': {
            'geometric': float(geo_score),
            'visual': float(vis_score),
            'combined': float(top_score)
        },
        'alternative_labels': alternatives
    }
```

#### Step 5: Update API Responses

**Update `segment()` method:**
```python
def segment(self, image: np.ndarray, prompt: Dict) -> List[Dict]:
    # ... existing code ...
    
    # Use enhanced classification
    classification = self._classify_segment_enhanced(image, binary_mask)
    
    return [{
        "label": classification['label'],
        "score": float(iou_score),
        "mask": rle_mask,
        "bbox": bbox,
        "mask_png": mask_png_b64,
        "confidence_breakdown": classification['confidence_breakdown'],
        "alternative_labels": classification['alternative_labels']
    }]
```

**Update `count()` method:**
```python
# In count() method, replace:
label = self._classify_segment(det['mask'])

# With:
classification = self._classify_segment_enhanced(image, det['mask'])
label = classification['label']
```

**Estimated Effort:** 2-3 days  
**Impact:** HIGH - Enables actual component recognition  
**Files to Modify:**
- `python-services/core/ai/sam_inference.py`
- Add tests in `python-services/tests/test_classification.py`

---

## ðŸŸ¡ HIGH Priority

### 2. Implement True LRU Cache with Metrics

**Current Status:** âš ï¸ Simple dictionary cache (70% compliant)

**Issue:**
```python
self.embedding_cache: Dict[str, torch.Tensor] = {}
self.MAX_CACHE_SIZE = CACHE_DEFAULT_SIZE  # 10
```

**Recommended Implementation:**

```python
from collections import OrderedDict
from typing import Optional
import time

class LRUCache:
    """Thread-safe LRU cache with performance metrics."""
    
    def __init__(self, max_size: int = 50):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
        self.evictions = 0
    
    def get(self, key: str) -> Optional[torch.Tensor]:
        """Get item from cache, moving it to end (most recently used)."""
        if key in self.cache:
            self.hits += 1
            self.cache.move_to_end(key)
            return self.cache[key]
        else:
            self.misses += 1
            return None
    
    def put(self, key: str, value: torch.Tensor) -> None:
        """Add item to cache, evicting oldest if at capacity."""
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            if len(self.cache) >= self.max_size:
                # Evict oldest (first item)
                self.cache.popitem(last=False)
                self.evictions += 1
        
        self.cache[key] = value
    
    def stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0
        
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'hits': self.hits,
            'misses': self.misses,
            'evictions': self.evictions,
            'hit_rate': hit_rate,
            'total_requests': total_requests
        }
    
    def clear(self) -> None:
        """Clear cache and reset metrics."""
        self.cache.clear()
        self.hits = 0
        self.misses = 0
        self.evictions = 0

# Usage in SAMInferenceEngine.__init__():
self.embedding_cache = LRUCache(max_size=50)

# Update _get_image_embedding():
def _get_image_embedding(self, image: np.ndarray) -> torch.Tensor:
    img_hash = self._compute_image_hash(image)
    
    cached = self.embedding_cache.get(img_hash)
    if cached is not None:
        logger.debug(f"Cache hit for image {img_hash[:8]}")
        return cached
    
    logger.debug(f"Cache miss for image {img_hash[:8]}")
    embedding = self._compute_embedding(image)
    self.embedding_cache.put(img_hash, embedding)
    
    return embedding
```

**Add cache stats to health endpoint:**
```python
@app.get("/health")
async def health_check():
    # ... existing code ...
    
    if sam_engine:
        health_status["cache_stats"] = sam_engine.embedding_cache.stats()
    
    return health_status
```

**Estimated Effort:** 1 day  
**Impact:** MEDIUM - Improves performance monitoring  
**Files to Modify:**
- `python-services/core/ai/sam_inference.py`
- `python-services/hvac_analysis_service.py`

---

## ðŸŸ¢ MEDIUM Priority

### 3. Standardize API Parameters

**Update `/api/v1/segment` endpoint to accept full prompt JSON:**

**Current:**
```python
@app.post("/api/v1/segment")
async def segment_component(image: UploadFile = File(...), coords: str = Form(...)):
```

**Recommended:**
```python
@app.post("/api/v1/segment")
async def segment_component(
    image: UploadFile = File(...),
    prompt: str = Form(None),  # JSON string: {"type": "point", "data": {...}}
    coords: str = Form(None),  # Backward compatibility
    return_top_k: int = Form(1),
    enable_refinement: bool = Form(True)
):
    """
    Interactive segmentation endpoint.
    
    Args:
        image: Image file
        prompt: JSON prompt (preferred): {"type": "point", "data": {"coords": [x, y], "label": 1}}
        coords: Simple "x,y" format (backward compatibility)
        return_top_k: Number of top predictions to return
        enable_refinement: Enable prompt refinement
    """
    
    # Parse prompt
    if prompt:
        prompt_obj = json.loads(prompt)
    elif coords:
        # Backward compatibility
        prompt_obj = {
            "type": "point",
            "data": {"coords": [float(c) for c in coords.split(',')], "label": 1}
        }
    else:
        raise HTTPException(status_code=400, detail="Either 'prompt' or 'coords' required")
    
    # Use return_top_k and enable_refinement in inference...
```

**Estimated Effort:** 0.5 days  
**Impact:** LOW - Better API flexibility  
**Files to Modify:**
- `python-services/hvac_analysis_service.py`
- `python-services/core/ai/sam_inference.py` (add support for top_k)

---

## ðŸ”µ LOW Priority

### 4. Implement Adaptive Grid Sizing

**Add to `/api/v1/count` endpoint:**

```python
def _get_adaptive_grid_size(self, height: int, width: int) -> int:
    """Calculate adaptive grid size based on image dimensions."""
    max_dim = max(height, width)
    
    if max_dim < 1000:
        return 24  # Small images: finer grid
    elif max_dim < 2000:
        return 32  # Medium images: standard grid
    else:
        return 48  # Large images: coarser grid

@app.post("/api/v1/count")
async def count_components(
    # ... existing parameters ...
    use_adaptive_grid: bool = Form(True)
):
    if use_adaptive_grid:
        h, w = image_np.shape[:2]
        grid_size = sam_engine._get_adaptive_grid_size(h, w)
        logger.info(f"Using adaptive grid size: {grid_size}px for {w}x{h} image")
```

**Estimated Effort:** 0.5 days  
**Impact:** LOW - Minor performance improvement  
**Files to Modify:**
- `python-services/core/ai/sam_inference.py`
- `python-services/hvac_analysis_service.py`

---

## ðŸ“‹ Summary of Recommendations

| Priority | Task | Effort | Impact | Compliance Gain |
|----------|------|--------|--------|----------------|
| ðŸ”´ CRITICAL | Enhanced Classification | 2-3 days | HIGH | +45% |
| ðŸŸ¡ HIGH | True LRU Cache | 1 day | MEDIUM | +10% |
| ðŸŸ¢ MEDIUM | API Standardization | 0.5 days | LOW | +3% |
| ðŸ”µ LOW | Adaptive Grid | 0.5 days | LOW | +2% |

**Total Estimated Effort:** 4.5-5 days  
**Expected Compliance After Completion:** **99.5%** (from current 84.5%)

---

## Testing Strategy

For each recommendation:

1. **Unit Tests**
   - Test feature extraction functions
   - Test classification logic
   - Test cache behavior
   - Test API parameter parsing

2. **Integration Tests**
   - Test full endpoint workflow
   - Test backward compatibility
   - Test error handling

3. **Performance Tests**
   - Measure cache hit rates
   - Measure classification accuracy
   - Measure inference time

4. **Regression Tests**
   - Ensure existing functionality unchanged
   - Verify RLE encoding still correct
   - Verify NMS still working

---

## Implementation Order

### Phase 1: Foundation (Days 1-2)
1. Implement LRU cache with metrics
2. Add cache stats to health endpoint
3. Add unit tests for cache

### Phase 2: Core Functionality (Days 3-4)
1. Implement geometric feature extraction
2. Implement visual feature extraction
3. Implement classification rules
4. Implement combined scoring
5. Add comprehensive classification tests

### Phase 3: API Updates (Day 5)
1. Update API endpoints with new parameters
2. Update response formats
3. Test backward compatibility
4. Update documentation

### Phase 4: Enhancements (Optional)
1. Add adaptive grid sizing
2. Add performance monitoring
3. Optimize for production

---

## Validation Checklist

After implementing recommendations:

- [ ] All tests in `test_sam_compliance.py` pass
- [ ] Classification returns real labels (not random)
- [ ] Cache reports hit/miss rates in `/health`
- [ ] API accepts both old and new parameter formats
- [ ] Response includes confidence breakdown
- [ ] Response includes alternative labels
- [ ] Adaptive grid sizing works for different image sizes
- [ ] Documentation updated
- [ ] Audit report updated with new compliance score

---

## Long-term Roadmap

### Machine Learning Enhancement (Future)
Replace rule-based classification with learned classifier:
- Train CNN on HVAC component dataset
- Extract deep visual features
- Fine-tune for 65-class taxonomy
- Integrate with SAM pipeline

### Performance Optimization (Future)
- Model quantization (INT8)
- Batch processing optimization
- Distributed inference for scale
- Edge deployment (ONNX)

### Feature Expansion (Future)
- Box prompt support
- Mask prompt support
- Multi-object tracking
- Component relationship detection

---

**Document Version:** 1.0  
**Last Updated:** 2025-12-12  
**Next Review:** After Phase 2 completion
