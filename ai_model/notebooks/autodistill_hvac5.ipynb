{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitive, Production-Grade Auto-Training Pipeline (Stable Grounded-SAM)\n",
    "\n",
    "This notebook implements the complete AutoDistill workflow using the stable and reliable **Grounded-SAM** base model. It is architected with a high-precision, **Iterative, Per-Class Detection** strategy to ensure the highest possible accuracy for the auto-labeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Environment (Robust & Stable Installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üö® Starting Stable AutoDistill Workflow Installation...\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(f\"Working Directory: {HOME}\")\n",
    "\n",
    "print(\"üì¶ Installing All Required Packages...\")\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q opencv-python-headless matplotlib numpy timm transformers huggingface-hub\n",
    "!pip install -q segment-anything\n",
    "# Install the STABLE versions of the autodistill ecosystem\n",
    "!pip install -q autodistill autodistill-grounded-sam autodistill-yolov8 roboflow supervision\n",
    "\n",
    "print(\"üéØ Downloading SAM Model Checkpoint...\")\n",
    "SAM_CHECKPOINT_PATH = \"sam_vit_h_4b8939.pth\"\n",
    "if not os.path.exists(SAM_CHECKPOINT_PATH):\n",
    "    !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nüéâ INSTALLATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# --- Main Paths ---\n",
    "BASE_DRIVE_PATH = \"/content/drive/MyDrive/\"\n",
    "TEMPLATE_FOLDER_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_templates/\")\n",
    "UNLABELED_IMAGES_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_example_images/\")\n",
    "DATASET_OUTPUT_PATH = os.path.join(HOME, \"hvac_autodistill_dataset/\")\n",
    "TRAINING_OUTPUT_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_yolov8_training/\")\n",
    "\n",
    "os.makedirs(DATASET_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(TRAINING_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# --- Auto-Labeling Parameters ---\n",
    "# Use high confidence with the per-class detection method\n",
    "BOX_THRESHOLD = 0.4\n",
    "TEXT_THRESHOLD = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Phase 1: High-Precision Auto-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill.detection import CaptionOntology\n",
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.core.dataset import DetectionDataset\n",
    "import os\n",
    "import glob\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Building HVAC Ontology from template library...\")\n",
    "\n",
    "all_template_files = glob.glob(os.path.join(TEMPLATE_FOLDER_PATH, '*.png')) + glob.glob(os.path.join(TEMPLATE_FOLDER_PATH, '*.PNG'))\n",
    "if not all_template_files: raise FileNotFoundError(f\"FATAL: No template files found in {TEMPLATE_FOLDER_PATH}.\")\n",
    "\n",
    "ontology_mapping = {}\n",
    "for f_path in all_template_files:\n",
    "    clean_name = os.path.splitext(os.path.basename(f_path))[0].replace('template_', '').replace('_', ' ')\n",
    "    ontology_mapping[clean_name] = clean_name\n",
    "\n",
    "ontology = CaptionOntology(ontology_mapping)\n",
    "print(f\"\\n‚úÖ Ontology Created with {len(ontology.classes())} classes.\")\n",
    "\n",
    "print(\"\\nInitializing Grounded-SAM 'Teacher' model...\")\n",
    "base_model = GroundedSAM(ontology=ontology, box_threshold=BOX_THRESHOLD, text_threshold=TEXT_THRESHOLD)\n",
    "\n",
    "print(f\"\\nStarting auto-labeling on images in: {UNLABELED_IMAGES_PATH}\")\n",
    "\n",
    "image_paths = glob.glob(os.path.join(UNLABELED_IMAGES_PATH, '*'))\n",
    "dataset = DetectionDataset(classes=ontology.classes(), base_dir=DATASET_OUTPUT_PATH)\n",
    "total_detections_by_class = Counter()\n",
    "images_processed = 0\n",
    "\n",
    "for img_path in image_paths:\n",
    "    print(f\"\\n--- Processing: {os.path.basename(img_path)} ---\")\n",
    "    try:\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None: continue\n",
    "        \n",
    "        # The key to precision: run predict() for EACH class\n",
    "        all_detections_for_image = []\n",
    "        for i, class_name in enumerate(ontology.classes()):\n",
    "            detections = base_model.predict(image, prompt=class_name)\n",
    "            detections.class_id = np.full(len(detections), i)\n",
    "            all_detections_for_image.append(detections)\n",
    "        \n",
    "        if all_detections_for_image:\n",
    "            final_detections = sv.Detections.merge(all_detections_for_image)\n",
    "            dataset.add_detection(image_path=img_path, detections=final_detections)\n",
    "            images_processed += 1\n",
    "            \n",
    "            class_counts = Counter([ontology.classes()[cid] for cid in final_detections.class_id])\n",
    "            total_detections_by_class.update(class_counts)\n",
    "            \n",
    "            avg_confidence = np.mean(final_detections.confidence) if len(final_detections.confidence) > 0 else 0\n",
    "            print(f\"  -> ‚úÖ SUCCESS: Found {len(final_detections)} symbols.\")\n",
    "            print(f\"     - Average Confidence: {avg_confidence:.2f}\")\n",
    "        else:\n",
    "            print(\"  -> INFO: No symbols detected in this image.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> ‚ùå ERROR: Failed to process image. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- PHASE 1 COMPLETE: Auto-Labeling Finished ---\")\n",
    "print(f\"Dataset saved at: {DATASET_OUTPUT_PATH}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Phase 2: Manual Approval Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "print(\"üîç Starting review of the auto-labeled dataset...\")\n",
    "\n",
    "IMAGES_DIR = os.path.join(DATASET_OUTPUT_PATH, \"train/images\")\n",
    "LABELS_DIR = os.path.join(DATASET_OUTPUT_PATH, \"train/labels\")\n",
    "DATA_YAML_PATH = os.path.join(DATASET_OUTPUT_PATH, \"data.yaml\")\n",
    "\n",
    "try:\n",
    "    review_dataset = sv.DetectionDataset.from_yolo(\n",
    "        images_directory_path=IMAGES_DIR,\n",
    "        annotations_directory_path=LABELS_DIR,\n",
    "        data_yaml_path=DATA_YAML_PATH\n",
    "    )\n",
    "    print(f\"  -> Successfully loaded dataset with {len(review_dataset)} images.\")\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"FATAL: Could not load the dataset. Error: {e}.\")\n",
    "\n",
    "SAMPLE_SIZE = 5\n",
    "if len(review_dataset) < SAMPLE_SIZE: SAMPLE_SIZE = len(review_dataset)\n",
    "\n",
    "if SAMPLE_SIZE > 0:\n",
    "    box_annotator = sv.BoxAnnotator(thickness=2)\n",
    "    label_annotator = sv.LabelAnnotator(text_thickness=1, text_scale=0.5, text_padding=3)\n",
    "    dataset_items = list(review_dataset)\n",
    "    sample_items = dataset_items[:SAMPLE_SIZE]\n",
    "    annotated_images, titles = [], []\n",
    "    \n",
    "    for image_path, detections in sample_items:\n",
    "        image = cv2.imread(image_path)\n",
    "        labels = [f\"{review_dataset.classes[class_id]}\" for class_id in detections.class_id]\n",
    "        annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "        annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "        annotated_images.append(annotated_image)\n",
    "        titles.append(os.path.basename(image_path))\n",
    "\n",
    "    print(f\"  -> Displaying a sample of {SAMPLE_SIZE} auto-labeled images for review...\")\n",
    "    sv.plot_images_grid(images=annotated_images, titles=titles, grid_size=(1, SAMPLE_SIZE), size=(20, 10))\n",
    "else:\n",
    "    print(\"  -> No images were labeled. Nothing to review.\")\n",
    "\n",
    "print(\"\\n--- Review Complete ---\")\n",
    "user_approval = input(\"\\nüõë Please review the auto-labeled images above. Do you approve this dataset for training? (yes/no): \")\n",
    "\n",
    "PROCEED_TO_TRAINING = (user_approval.lower() == 'yes')\n",
    "\n",
    "if PROCEED_TO_TRAINING:\n",
    "    print(\"\\n‚úÖ Dataset approved. Proceeding to Phase 3: Training.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Dataset rejected. Training will be skipped. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Phase 3: Train the YOLOv8 \"Student\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCEED_TO_TRAINING:\n",
    "    from autodistill_yolov8 import YOLOv8\n",
    "    import os\n",
    "    import torch\n",
    "    from ultralytics.nn.modules import C2f, Detect, Bottleneck, Conv, ConvTranspose, DFL\n",
    "    import locale\n",
    "\n",
    "    locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "    TRAIN_DATASET_PATH = os.path.join(DATASET_OUTPUT_PATH, \"data.yaml\")\n",
    "    EPOCHS = 50\n",
    "\n",
    "    print(\"\\nInitializing YOLOv8 'Student' model with security context...\")\n",
    "\n",
    "    SAFE_GLOBALS = [C2f, Detect, Bottleneck, Conv, ConvTranspose, DFL, torch.nn.ModuleList]\n",
    "\n",
    "    try:\n",
    "        with torch.serialization.safe_globals(SAFE_GLOBALS):\n",
    "            target_model = YOLOv8(\"yolov8n.pt\")\n",
    "        print(\"YOLOv8 model initialized successfully.\")\n",
    "\n",
    "        print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "        target_model.train(\n",
    "            data_path=TRAIN_DATASET_PATH, \n",
    "            epochs=EPOCHS,\n",
    "            project=TRAINING_OUTPUT_PATH\n",
    "        )\n",
    "        print(\"\\n--- PHASE 3 COMPLETE: YOLOv8 training finished. ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YOLOv8 Training Failed. Error: {e}\")\n",
    "        PROCEED_TO_TRAINING = False\n",
    "else:\n",
    "    print(\"Training skipped as per user input.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Phase 4: Inference with Your Custom-Trained Expert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCEED_TO_TRAINING:\n",
    "    from ultralytics import YOLO\n",
    "    import glob\n",
    "    import os\n",
    "    import cv2\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    run_folders = sorted(glob.glob(os.path.join(TRAINING_OUTPUT_PATH, 'train*')))\n",
    "    if not run_folders:\n",
    "        raise FileNotFoundError(f\"FATAL: No training runs found.\")\n",
    "    latest_run_folder = run_folders[-1]\n",
    "    TRAINED_MODEL_PATH = os.path.join(latest_run_folder, 'weights/best.pt')\n",
    "\n",
    "    print(f\"\\nLoading your custom-trained HVAC expert model from: {TRAINED_MODEL_PATH}\")\n",
    "    model = YOLO(TRAINED_MODEL_PATH)\n",
    "\n",
    "    image_paths = glob.glob(os.path.join(UNLABELED_IMAGES_PATH, '*'))\n",
    "    inference_image_path = image_paths[-1]\n",
    "    print(f\"Running inference on: {os.path.basename(inference_image_path)}\")\n",
    "\n",
    "    results = model(inference_image_path)\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    INFERENCE_OUTPUT_FOLDER = os.path.join(BASE_DRIVE_PATH, \"hvac_inference_results/\")\n",
    "    os.makedirs(INFERENCE_OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "    output_filename = f\"inference_result_{os.path.basename(inference_image_path)}.png\"\n",
    "    output_path = os.path.join(INFERENCE_OUTPUT_FOLDER, output_filename)\n",
    "    cv2.imwrite(output_path, annotated_frame)\n",
    "\n",
    "    print(f\"\\nInference complete. Found {len(results[0].boxes)} symbols.\")\n",
    "    print(f\"Saved inference result to: {output_path}\")\n",
    "    display(Image(filename=output_path, width=800))\n",
    "    print(\"\\n--- PIPELINE COMPLETE ---\")\n",
    "else:\n",
    "    print(\"Inference skipped as per user input.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}