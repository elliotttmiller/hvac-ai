{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fXLkcSp5Ljj"
      },
      "source": [
        "# üöÄ HVAC AI ‚Äî Production-Ready YOLO11 Inference Server\n",
        "**Optimized Turn-Key Backend/Inference Notebook**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Overview\n",
        "Production-ready YOLO11 inference deployment with:\n",
        "- ‚úÖ Comprehensive GPU & dependency validation\n",
        "- ‚úÖ Optimized configuration management\n",
        "- ‚úÖ Error handling & monitoring\n",
        "- ‚úÖ Testing & benchmarking\n",
        "- ‚úÖ Security best practices\n",
        "- ‚úÖ Turn-key deployment\n",
        "\n",
        "## üéØ Prerequisites\n",
        "1. **GPU Runtime**: T4 or better (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
        "2. **Trained Model**: YOLO11 `.pt` file in Google Drive\n",
        "3. **Ngrok Token**: Free token from [ngrok.com](https://ngrok.com/)\n",
        "4. **Test Image**: Sample HVAC blueprint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMM7FrIo5P26",
        "outputId": "8d46fdd4-113b-4fd8-db7b-e6ac983bae5b"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for model access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive mounted at: /content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RUwtJpK5Ljn",
        "outputId": "cefc9d35-2d5c-4117-a92f-389dff58a95d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîß Environment Setup & Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Clone repository\n",
        "print(\"\\nüì¶ Cloning repository...\")\n",
        "!git clone https://github.com/elliotttmiller/hvac-ai.git 2>/dev/null || echo \"Repository exists\"\n",
        "%cd hvac-ai\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\nüìö Installing dependencies (2-3 minutes)...\")\n",
        "!pip install -q ultralytics>=8.0.0 fastapi>=0.115.0 uvicorn[standard]>=0.34.0\n",
        "!pip install -q python-multipart>=0.0.9 pyngrok>=7.0.0 python-dotenv>=1.0.0\n",
        "!pip install -q Pillow>=10.0.0 numpy>=1.24.0 tqdm>=4.65.0\n",
        "\n",
        "# Validate environment\n",
        "print(\"\\nüîç System Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import torch\n",
        "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"   CUDA: {torch.version.cuda}\")\n",
        "    # Test GPU\n",
        "    test_tensor = torch.rand(1000, 1000).cuda()\n",
        "    _ = torch.matmul(test_tensor, test_tensor)\n",
        "    print(f\"   Test: ‚úÖ PASSED\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: No GPU! Set Runtime > GPU. Inference will be SLOW.\")\n",
        "\n",
        "print(\"\\n‚úÖ Environment Ready!\")\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax3wqr4_5Ljr",
        "outputId": "ad59f694-ae9d-43e3-f44f-0b498d276cef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚öôÔ∏è  Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- UPDATE THESE VALUES ---\n",
        "MODEL_PATH = \"/content/drive/Shareddrives/HVAC/yolo11m_run_v10/weights/best.pt\"\n",
        "NGROK_AUTHTOKEN = \"36hBoLt4A3L8yOYt96wKiCxxrwp_5wFbj1Frv6GoHARRQ6H6t\"  # Get from ngrok.com\n",
        "\n",
        "# Server settings\n",
        "PORT = 8000\n",
        "DEFAULT_CONF_THRESHOLD = 0.50\n",
        "DEFAULT_IOU_THRESHOLD = 0.45\n",
        "MAX_IMAGE_SIZE = 1024\n",
        "\n",
        "# Validation\n",
        "errors = []\n",
        "if not MODEL_PATH or not os.path.exists(MODEL_PATH):\n",
        "    errors.append(\"‚ùå MODEL_PATH invalid or not found\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model: {MODEL_PATH}\")\n",
        "    print(f\"   Size: {os.path.getsize(MODEL_PATH) / 1e6:.1f} MB\")\n",
        "\n",
        "if not NGROK_AUTHTOKEN or NGROK_AUTHTOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"‚ö†Ô∏è  Ngrok token not set (optional, for public URL)\")\n",
        "else:\n",
        "    print(f\"‚úÖ Ngrok: {'*' * 20}{NGROK_AUTHTOKEN[-8:]}\")\n",
        "\n",
        "print(f\"\\nüéØ Inference: conf={DEFAULT_CONF_THRESHOLD}, iou={DEFAULT_IOU_THRESHOLD}, size={MAX_IMAGE_SIZE}\")\n",
        "\n",
        "# Write .env\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"MODEL_PATH={MODEL_PATH}\\nNGROK_AUTHTOKEN={NGROK_AUTHTOKEN}\\nPORT={PORT}\\n\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\n‚ùå Errors:\", \"\\n\".join(errors))\n",
        "else:\n",
        "    print(\"\\n‚úÖ Configuration valid\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_gsoC_f5Ljs",
        "outputId": "1a995da9-0388-4e6b-c57e-0fc8af39e3e4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"ü§ñ Model Loading & Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüì• Loading model (10-30s)...\")\n",
        "start = time.time()\n",
        "model = YOLO(MODEL_PATH)\n",
        "print(f\"‚úÖ Loaded in {time.time() - start:.2f}s\")\n",
        "\n",
        "print(f\"\\nüìä Model Info:\")\n",
        "print(f\"   Device: {model.device}\")\n",
        "print(f\"   Classes: {len(model.names)}\")\n",
        "for idx, name in model.names.items():\n",
        "    print(f\"   [{idx}] {name}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')\n",
        "    print(f\"\\nüöÄ Model on GPU\")\n",
        "\n",
        "# Warm-up\n",
        "print(f\"\\nüî• Warm-up inference...\")\n",
        "dummy = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
        "start = time.time()\n",
        "_ = model.predict(dummy, verbose=False, conf=0.25)\n",
        "first_time = time.time() - start\n",
        "start = time.time()\n",
        "_ = model.predict(dummy, verbose=False, conf=0.25)\n",
        "second_time = time.time() - start\n",
        "\n",
        "print(f\"   First: {first_time*1000:.1f}ms\")\n",
        "print(f\"   Subsequent: {second_time*1000:.1f}ms (~{1.0/second_time:.0f} FPS)\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nüíæ GPU Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "\n",
        "print(\"\\n‚úÖ Model ready!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "eRMzH1w45Ljt",
        "outputId": "b4bf0106-321a-4e87-d881-3b36c8bc28fd"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np # Added import for numpy\n",
        "\n",
        "print(\"üß™ Test Inference\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüì§ Upload test image...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    img_path = list(uploaded.keys())[0]\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    print(f\"\\nüìä Image: {img.size[0]}x{img.size[1]}\")\n",
        "\n",
        "    print(f\"\\nüîÑ Running inference...\")\n",
        "    start = time.time()\n",
        "    results = model.predict(img_array, conf=DEFAULT_CONF_THRESHOLD,\n",
        "                           iou=DEFAULT_IOU_THRESHOLD, imgsz=MAX_IMAGE_SIZE, verbose=False)\n",
        "    inf_time = (time.time() - start) * 1000\n",
        "\n",
        "    result = results[0]\n",
        "    boxes = result.boxes\n",
        "\n",
        "    print(f\"\\n‚úÖ Complete: {inf_time:.1f}ms ({1000.0/inf_time:.1f} FPS)\")\n",
        "    if boxes is not None:\n",
        "        print(f\"   Detections: {len(boxes)}\")\n",
        "    else:\n",
        "        print(f\"   Detections: 0 (No objects detected)\")\n",
        "\n",
        "\n",
        "    if boxes is not None and len(boxes) > 0:\n",
        "        class_counts = {}\n",
        "        for box in boxes:\n",
        "            cls_id = int(box.cls[0])\n",
        "            name = model.names[cls_id]\n",
        "            class_counts[name] = class_counts.get(name, 0) + 1\n",
        "        print(f\"\\nüìä By Class:\")\n",
        "        for name, count in sorted(class_counts.items()):\n",
        "            print(f\"   {name}: {count}\")\n",
        "\n",
        "    # Visualize\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "    ax1.imshow(img_array)\n",
        "    ax1.set_title('Original')\n",
        "    ax1.axis('off')\n",
        "    ax2.imshow(result.plot())\n",
        "    # Ensure title is also conditional on boxes being present for count\n",
        "    if boxes is not None:\n",
        "        ax2.set_title(f'{len(boxes)} detections ({inf_time:.0f}ms)')\n",
        "    else:\n",
        "        ax2.set_title(f'0 detections ({inf_time:.0f}ms)')\n",
        "    ax2.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ùå No image uploaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5a_1AyI5Lju"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "print(\"üöÄ Deploying API Server\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Validate configuration\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"\\n‚ùå ERROR: MODEL_PATH not found. Check configuration.\")\n",
        "    raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
        "\n",
        "print(f\"‚úÖ Model found: {MODEL_PATH}\")\n",
        "\n",
        "# Setup ngrok tunnel\n",
        "if NGROK_AUTHTOKEN and NGROK_AUTHTOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"\\nüåê Setting up ngrok tunnel...\")\n",
        "    ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "    public_url = ngrok.connect(PORT)\n",
        "    print(f\"\\n‚úÖ API LIVE!\")\n",
        "    print(f\"   Public URL: {public_url.public_url}\")\n",
        "    print(f\"   API Docs: {public_url.public_url}/docs\")\n",
        "    print(f\"   Health: {public_url.public_url}/health\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No ngrok token - server will be local only\")\n",
        "    print(f\"   Local URL: http://localhost:{PORT}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üé¨ Starting server (Press STOP button to shutdown)...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Validate python-services directory exists\n",
        "if not os.path.exists('python-services'):\n",
        "    print(\"‚ùå ERROR: python-services directory not found\")\n",
        "    print(f\"   Current directory: {os.getcwd()}\")\n",
        "    print(\"   Please ensure you're in the hvac-ai repository root\")\n",
        "    raise FileNotFoundError(\"python-services directory not found\")\n",
        "\n",
        "%cd python-services\n",
        "# Use PORT variable via Python string formatting\n",
        "import subprocess\n",
        "subprocess.run([\"uvicorn\", \"hvac_analysis_service:app\", \"--host\", \"0.0.0.0\", \"--port\", str(PORT), \"--reload\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
