{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "# ðŸš€ HVAC AI - Remote Inference Server (v2 - Configured)\n",
        "\n",
        "This notebook acts as a GPU-powered backend server for the `hvac-ai` platform. It clones the repository, installs dependencies, and uses `ngrok` to create a public URL for your local frontend. This version is enhanced to use a `.env` file for secure and portable configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1_header"
      },
      "source": [
        "### Phase 1: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo_cell"
      },
      "outputs": [],
      "source": [
        "# 1. Clone the GitHub repository\n",
        "!git clone https://github.com/elliotttmiller/hvac-ai.git\n",
        "\n",
        "# 2. Change the current working directory to the cloned repo\n",
        "%cd hvac-ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps_cell"
      },
      "outputs": [],
      "source": [
        "# 3. Install all required Python packages\n",
        "\n",
        "# Web server, tunneling, and configuration dependencies\n",
        "!pip install flask flask-cors pyngrok python-dotenv --quiet\n",
        "\n",
        "# Machine learning and SAM dependencies\n",
        "!pip install torch torchvision --quiet\n",
        "!pip install opencv-python pycocotools matplotlib onnxruntime onnx --quiet\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git --quiet\n",
        "\n",
        "print(\"âœ… All dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2_header"
      },
      "source": [
        "### Phase 2: Create and Configure `.env` File\n",
        "\n",
        "This is the most important step for configuring your server securely.\n",
        "\n",
        "1.  **Get your `ngrok` authtoken** from your dashboard: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "2.  **Run the cell below**, pasting your `ngrok` token and the correct Google Drive path to your fine-tuned model (`.pth` file) where indicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_env_cell"
      },
      "outputs": [],
      "source": [
        "%%writefile .env\n",
        "# .env file for HVAC AI Inference Server\n",
        "\n",
        "# 1. Paste your secret token from the ngrok dashboard\n",
        "NGROK_AUTHTOKEN=\"YOUR_NGROK_AUTHTOKEN_HERE\"\n",
        "\n",
        "# 2. Update with the full path to your fine-tuned model in Google Drive\n",
        "MODEL_PATH=\"/content/drive/MyDrive/sam_finetuning_results/best_model_expert_v1.pth\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase3_header"
      },
      "source": [
        "### Phase 3: Authenticate `ngrok`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngrok_auth_cell"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load the environment variables from the .env file we just created\n",
        "load_dotenv()\n",
        "\n",
        "# Get the token\n",
        "ngrok_token = os.getenv(\"NGROK_AUTHTOKEN\")\n",
        "\n",
        "if ngrok_token and \"YOUR_NGROK_AUTHTOKEN_HERE\" not in ngrok_token:\n",
        "    print(\"NGROK_AUTHTOKEN loaded successfully. Authenticating...\")\n",
        "    !ngrok authtoken {ngrok_token}\n",
        "else:\n",
        "    print(\"âŒ ERROR: NGROK_AUTHTOKEN not found or not updated in the .env file.\")\n",
        "    print(\"ðŸ‘‰ Please edit the cell in Phase 2 with your real token and run it again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase4_header"
      },
      "source": [
        "### Phase 4: Prepare and Launch the Inference Server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "action_required_markdown"
      },
      "source": [
        "#### âš ï¸ **Action Required: Update the Backend Script**\n",
        "\n",
        "The final step is to ensure your `backend/app.py` script is configured to load variables from the `.env` file.\n",
        "\n",
        "1. In the file browser on the left, navigate to `hvac-ai` -> `backend`.\n",
        "2. Double-click on `app.py` to open it.\n",
        "3. **Replace the entire content** of `app.py` with the provided professional, `.env`-aware code from our discussion. This code should import `dotenv` and use `os.getenv(\"MODEL_PATH\")`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_server_cell"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# --- 1. Mount Google Drive to access the model file ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ… Drive mounted.\")\n",
        "\n",
        "# --- 2. Set up the ngrok tunnel ---\n",
        "PORT = 5000\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(PORT).public_url\n",
        "print(\"-\"*50)\n",
        "print(f\"âœ… Your inference server is live at: {public_url}\")\n",
        "print(\"ðŸ‘‰ Use this URL as the API endpoint in your local frontend application.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# --- 3. Run the Flask Application ---\n",
        "# The script will now read its configuration from the .env file.\n",
        "# This cell will run continuously and display live logs from your server.\n",
        "print(\"\\nðŸš€ Starting the backend server... (Logs will appear below)\")\n",
        "!python backend/app.py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}