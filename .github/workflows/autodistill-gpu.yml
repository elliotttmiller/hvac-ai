# .github/workflows/autodistill-gpu.yml
name: AutoDistill GPU Pipeline Execution

on:
  workflow_dispatch:  # Allow manual triggering
  push:
    branches: [main, dev]
    paths:
      - 'notebooks/**'
      - 'python-services/**'
      - '.github/workflows/autodistill-gpu.yml'
  pull_request:
    branches: [main]
    paths:
      - 'notebooks/**'
      - 'python-services/**'

jobs:
  run-autodistill-pipeline:
    runs-on: ubuntu-latest
    hardware:
      gpu: T4  # NVIDIA Tesla T4 GPU - good balance of cost/performance
      # Alternative options: A10G, L4, V100, A100 (more expensive but faster)
    permissions:
      contents: read
      actions: read
    env:
      PYTHON_VERSION: "3.10"
      CUDA_VERSION: "12.1"
      NVIDIA_DRIVER_VERSION: "535"
      NOTEBOOK_NAME: "autodistill_pipeline.ipynb"  # Update this to your actual notebook name
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            curl \
            libgl1 \
            libsm6 \
            libxrender1 \
            libxext6 \
            libgl1-mesa-glx \
            libglib2.0-0 \
            ffmpeg \
            libgl1-mesa-glx \
            libgomp1 \
            git \
            wget \
            unzip
            
          # Install NVIDIA driver dependencies
          sudo apt-get install -y --no-install-recommends \
            nvidia-driver-${{ env.NVIDIA_DRIVER_VERSION }} \
            nvidia-cuda-toolkit

      - name: Verify GPU availability
        run: |
          echo "=== GPU INFORMATION ==="
          nvidia-smi
          echo ""
          echo "=== CUDA VERSION ==="
          nvcc --version || echo "nvcc not found in PATH"
          echo ""
          echo "=== DRIVER VERSION ==="
          cat /proc/driver/nvidia/version || echo "NVIDIA driver version not available"
          echo ""
          echo "=== GPU MEMORY ==="
          nvidia-smi --query-gpu=memory.total --format=csv
          echo ""
          echo "=== GPU COMPUTE CAPABILITY ==="
          nvidia-smi --query-gpu=compute_cap --format=csv

      - name: Install Python dependencies
        run: |
          cd python-services
          
          # Create virtual environment
          python -m venv .venv
          source .venv/bin/activate
          
          # Upgrade pip and setuptools
          pip install --upgrade pip setuptools wheel
          
          # Install NVIDIA index for optimized packages
          pip install nvidia-pyindex
          
          # Install CUDA runtime libraries
          pip install nvidia-cuda-runtime-cu12
          
          # Install GPU-enabled PyTorch (CUDA 12.1)
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
          
          # Install AutoDistill ecosystem with GPU support
          pip install autodistill autodistill-grounded-sam autodistill-yolov8 supervision
          
          # Install supporting GPU-accelerated libraries
          pip install opencv-python-headless transformers sentence-transformers ultralytics numpy pandas matplotlib jupyter nbconvert jupyter-client jupyter-core
          
          # Install project-specific requirements if they exist
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: Download model weights
        run: |
          cd python-services
          source .venv/bin/activate
          
          # Create weights directory
          mkdir -p weights
          
          echo "=== DOWNLOADING MODEL WEIGHTS ==="
          
          # Download SAM weights
          if [ ! -f weights/sam_vit_h_4b8939.pth ]; then
            echo "Downloading SAM weights (large model)..."
            wget -O weights/sam_vit_h_4b8939.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
            echo "SAM weights downloaded successfully"
          else
            echo "SAM weights already exist"
          fi
          
          # Download GroundingDINO weights
          if [ ! -f weights/groundingdino_swint_ogc.pth ]; then
            echo "Downloading GroundingDINO weights..."
            wget -O weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
            echo "GroundingDINO weights downloaded successfully"
          else
            echo "GroundingDINO weights already exist"
          fi
          
          # Create directory for notebook execution
          mkdir -p ../notebooks/executed
          
          echo "Model weights setup completed"

      - name: Prepare notebook for execution
        run: |
          cd notebooks
          
          # Create execution directory if it doesn't exist
          mkdir -p executed
          
          # Copy notebook to execution directory
          if [ -f ${{ env.NOTEBOOK_NAME }} ]; then
            cp ${{ env.NOTEBOOK_NAME }} executed/
            echo "âœ… Notebook copied to execution directory"
          else
            echo "âŒ Notebook ${{ env.NOTEBOOK_NAME }} not found in notebooks directory"
            echo "Available notebooks in notebooks directory:"
            ls -la
            exit 1
          fi
          
          # Create a wrapper script to ensure proper GPU detection
          cat > executed/gpu_validation.py << 'EOF'
import torch
import subprocess
import sys
import os

def validate_gpu_environment():
    """Comprehensive GPU environment validation for AutoDistill pipeline"""
    print("=== ðŸ”¥ GPU ENVIRONMENT VALIDATION FOR EXECUTION ===")
    
    # Check CUDA availability
    cuda_available = torch.cuda.is_available()
    print(f"âœ… CUDA Available: {cuda_available}")
    
    if not cuda_available:
        print("âŒ FATAL ERROR: CUDA not available. AutoDistill requires GPU acceleration.")
        print("   Troubleshooting steps:")
        print("   1. Verify NVIDIA drivers are installed")
        print("   2. Check CUDA toolkit is properly configured")
        print("   3. Run 'nvidia-smi' to verify GPU detection")
        sys.exit(1)
    
    # Get CUDA version from PyTorch
    cuda_version = torch.version.cuda
    print(f"âœ… PyTorch CUDA Version: {cuda_version}")
    
    # Get driver version
    try:
        driver_output = subprocess.check_output(['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'], stderr=subprocess.DEVNULL)
        driver_version = driver_output.decode().strip()
        print(f"âœ… NVIDIA Driver Version: {driver_version}")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âš ï¸ WARNING: Could not verify NVIDIA driver version")
    
    # Check available GPUs and memory
    gpu_count = torch.cuda.device_count()
    print(f"âœ… Available GPUs: {gpu_count}")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        total_memory = torch.cuda.get_device_properties(i).total_memory / 1e9  # Convert to GB
        print(f"   GPU {i}: {gpu_name} - {total_memory:.1f}GB VRAM")
    
    # Test CUDA operations
    try:
        print("   Testing CUDA tensor operations...")
        x = torch.rand(1000, 1000, device='cuda')
        y = torch.rand(1000, 1000, device='cuda')
        z = torch.matmul(x, y)
        print("âœ… CUDA tensor operations successful")
    except Exception as e:
        print(f"âŒ CUDA operation failed: {str(e)}")
        sys.exit(1)
    
    # Set CUDA device explicitly
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… Using device: {device}")
    
    return device

# Execute validation
device = validate_gpu_environment()
EOF

          echo "âœ… GPU validation script created"

      - name: Execute AutoDistill pipeline notebook
        run: |
          cd notebooks/executed
          
          # Activate virtual environment
          source ../../python-services/.venv/bin/activate
          
          echo "=== ðŸš€ EXECUTING AUTODISTILL PIPELINE NOTEBOOK ==="
          echo "Notebook: ${{ env.NOTEBOOK_NAME }}"
          echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)"
          
          # First validate GPU environment
          python gpu_validation.py
          
          # Execute notebook with GPU support
          jupyter nbconvert \
            --to notebook \
            --execute ${{ env.NOTEBOOK_NAME }} \
            --output executed_${{ env.NOTEBOOK_NAME }} \
            --ExecutePreprocessor.timeout=7200 \  # 2-hour timeout for large notebooks
            --ExecutePreprocessor.kernel_name=python3 \
            --log-level=DEBUG
          
          echo "=== âœ… NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY ==="
          
          # Display execution summary
          echo "=== EXECUTION SUMMARY ==="
          echo "Status: SUCCESS"
          echo "Output notebook: executed_${{ env.NOTEBOOK_NAME }}"
          echo "GPU utilization during execution:"
          nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv

      - name: Upload executed notebook as artifact
        uses: actions/upload-artifact@v4
        with:
          name: executed-notebook-${{ github.sha }}
          path: notebooks/executed/executed_${{ env.NOTEBOOK_NAME }}
          retention-days: 7

      - name: Upload pipeline results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-results-${{ github.sha }}
          path: |
            notebooks/executed/results/
            notebooks/executed/output/
            notebooks/executed/*.json
            notebooks/executed/*.csv
            notebooks/executed/*.png
            notebooks/executed/*.jpg
            notebooks/executed/*.pt
            notebooks/executed/*.pth
          retention-days: 7

      - name: Post-execution cleanup
        if: always()
        run: |
          echo "=== ðŸ§¹ POST-EXECUTION CLEANUP ==="
          
          # Clear GPU memory
          echo "Clearing GPU memory cache..."
          python -c "import torch; torch.cuda.empty_cache()"
          
          # Reset GPU (if needed)
          echo "Resetting GPU state..."
          nvidia-smi --gpu-reset || echo "GPU reset not available"
          
          # Display final GPU state
          echo "=== FINAL GPU STATE ==="
          nvidia-smi

      - name: Generate execution report
        if: always()
        run: |
          cat > execution_report.md << EOF
# AutoDistill Pipeline Execution Report
**Workflow:** ${{ github.workflow }}
**Run ID:** ${{ github.run_id }}
**Status:** ${{ job.status }}
**Timestamp:** $(date '+%Y-%m-%d %H:%M:%S UTC')

## GPU Configuration
- **GPU Type:** NVIDIA Tesla T4
- **CUDA Version:** ${{ env.CUDA_VERSION }}
- **Driver Version:** ${{ env.NVIDIA_DRIVER_VERSION }}

## Execution Details
- **Notebook:** ${{ env.NOTEBOOK_NAME }}
- **Repository:** ${{ github.repository }}
- **Branch:** ${{ github.ref_name }}

## Results
- **Executed Notebook:** [Download](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)
- **Pipeline Results:** [Download](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)

## Next Steps
1. Review the executed notebook for any errors or warnings
2. Analyze the pipeline results in the artifacts
3. If execution failed, check the logs for GPU memory issues or CUDA errors

---
*This report was automatically generated by GitHub Actions workflow*
EOF
          
          echo "âœ… Execution report generated"

      - name: Upload execution report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: execution-report-${{ github.sha }}
          path: execution_report.md
          retention-days: 30

  notify-completion:
    needs: run-autodistill-pipeline
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Send completion notification
        run: |
          STATUS="${{ needs.run-autodistill-pipeline.result }}"
          
          if [ "$STATUS" = "success" ]; then
            echo "ðŸŽ‰ AutoDistill pipeline executed successfully!"
            echo "âœ… All artifacts have been uploaded and are available for download."
          else
            echo "ðŸš¨ AutoDistill pipeline execution failed!"
            echo "âŒ Check the workflow logs for detailed error information."
            echo "Common issues:"
            echo "  - GPU memory exhaustion (OSError: CUDA out of memory)"
            echo "  - CUDA driver compatibility issues"
            echo "  - Missing model weights or dependencies"
            echo "  - Notebook execution timeout (2 hours maximum)"
          fi
          
          echo ""
          echo "ðŸ“Š GPU Runner Cost Information:"
          echo "T4 GPU: ~$0.00096 per second (~$3.46/hour)"
          echo "A10G GPU: ~$0.00192 per second (~$6.91/hour)"
          echo "ðŸ’¡ Tip: Use smaller GPUs for development, larger GPUs for production runs"