{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ HVAC AI ‚Äî Production-Ready YOLOv11-OBB Pipeline\n",
        "**End-to-End Oriented Bounding Box (OBB) Analysis & Inference Server**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Overview\n",
        "This notebook deploys a high-performance API for HVAC blueprint analysis.\n",
        "\n",
        "### üåü Configuration\n",
        "- **Endpoint**: `/api/v1/analyze/stream` (POST)\n",
        "- **Format**: YOLOv11-OBB (Rotated Bounding Boxes)\n",
        "- **Optimization**: FP16 (Half-Precision) & CUDA\n",
        "- **Security**: CORS enabled for Frontend access\n",
        "\n",
        "## üéØ Instructions\n",
        "1. Set Runtime to **GPU**.\n",
        "2. Run all cells from top to bottom.\n",
        "3. Copy the **Ngrok Public URL** into your Frontend `.env` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "print(\"üîß Environment Setup\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Install Dependencies\n",
        "print(\"\\nüì¶ Installing libraries...\")\n",
        "!pip install -q ultralytics>=8.3.0 fastapi>=0.115.0 uvicorn[standard]>=0.34.0\n",
        "!pip install -q python-multipart pyngrok>=7.0.0 python-dotenv opencv-python-headless\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "print(f\"\\n‚úÖ Python: {sys.version.split()[0]}\")\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Running on CPU (Slow)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "print(\"‚öôÔ∏è  Pipeline Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- USER CONFIGURATION START ---\n",
        "# Path to your YOLOv11-OBB model\n",
        "MODEL_PATH = \"/content/drive/Shareddrives/HVAC/DECEMBER 24 OUTPUT WEIGHTS {dataset2}/hvac_obb_l_20251224_214011/weights/best.pt\"\n",
        "\n",
        "# Inference Settings\n",
        "CONF_THRESHOLD = 0.50      # Confidence threshold\n",
        "IOU_THRESHOLD = 0.45       # NMS IoU threshold\n",
        "IMG_SIZE = 1024            # Inference image size\n",
        "HALF_PRECISION = torch.cuda.is_available() # Use FP16 if GPU is available\n",
        "\n",
        "# Server Settings\n",
        "PORT = 8000\n",
        "NGROK_AUTHTOKEN = \"36hBoLt4A3L8yOYt96wKiCxxrwp_5wFbj1Frv6GoHARRQ6H6t\" # <--- PASTE TOKEN HERE\n",
        "# --- USER CONFIGURATION END ---\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚ùå ERROR: Model not found at {MODEL_PATH}\")\n",
        "    print(\"   Please update MODEL_PATH to point to your .pt file.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model Path: {MODEL_PATH}\")\n",
        "    print(f\"‚úÖ Config: Conf={CONF_THRESHOLD}, IoU={IOU_THRESHOLD}, FP16={HALF_PRECISION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen_server"
      },
      "outputs": [],
      "source": [
        "print(\"üìù Generating Production Server Code (app.py)...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Updates:\n",
        "# 1. Added GET handler for /api/v1/analyze/stream to fix 405 errors.\n",
        "# 2. CORS enabled for all origins.\n",
        "# 3. Double-brace escaping for Python dictionaries in f-string.\n",
        "\n",
        "server_code = f\"\"\"\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_PATH = r'{MODEL_PATH}'\n",
        "CONF_THRES = {CONF_THRESHOLD}\n",
        "IOU_THRES = {IOU_THRESHOLD}\n",
        "IMG_SIZE = {IMG_SIZE}\n",
        "HALF = {HALF_PRECISION}\n",
        "\n",
        "# --- LOGGING ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(\"HVAC-Service\")\n",
        "\n",
        "app = FastAPI(title=\"HVAC YOLOv11-OBB Inference API\")\n",
        "\n",
        "# --- CORS POLICY ---\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "model = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def load_model():\n",
        "    global model\n",
        "    logger.info(f\"Loading model from {{MODEL_PATH}}...\")\n",
        "    try:\n",
        "        model = YOLO(MODEL_PATH)\n",
        "        if torch.cuda.is_available():\n",
        "            model.to('cuda')\n",
        "            logger.info(\"Model loaded on GPU\")\n",
        "        else:\n",
        "            logger.info(\"Model loaded on CPU\")\n",
        "\n",
        "        # Warmup\n",
        "        model.predict(np.zeros((640,640,3), dtype=np.uint8), verbose=False, half=HALF)\n",
        "        logger.info(\"Model Warmup Complete\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model: {{e}}\")\n",
        "        raise RuntimeError(\"Model loading failed\")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {{\"message\": \"HVAC Inference Server Online\", \"docs\": \"/docs\"}}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    if model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Model initializing\")\n",
        "    return {{\"status\": \"healthy\", \"device\": str(model.device)}}\n",
        "\n",
        "# --- MAIN INFERENCE ENDPOINT ---\n",
        "\n",
        "# 1. Handle GET requests (Browser/Health checks)\n",
        "@app.get(\"/api/v1/analyze/stream\")\n",
        "def analyze_help():\n",
        "    return JSONResponse(\n",
        "        status_code=200,\n",
        "        content={{\"message\": \"Endpoint ready. Send a POST request with an image file to perform inference.\"}}\n",
        "    )\n",
        "\n",
        "# 2. Handle POST requests (Actual Inference)\n",
        "@app.post(\"/api/v1/analyze/stream\")\n",
        "async def analyze_image(file: UploadFile = File(...)):\n",
        "    if not model:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
        "\n",
        "    try:\n",
        "        contents = await file.read()\n",
        "        nparr = np.frombuffer(contents, np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if img is None:\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid image format\")\n",
        "\n",
        "        results = model.predict(\n",
        "            img,\n",
        "            conf=CONF_THRES,\n",
        "            iou=IOU_THRES,\n",
        "            imgsz=IMG_SIZE,\n",
        "            half=HALF\n",
        "        )\n",
        "\n",
        "        result = results[0]\n",
        "        detections = []\n",
        "\n",
        "        # Handle OBB vs Standard\n",
        "        if hasattr(result, 'obb') and result.obb is not None:\n",
        "            for box in result.obb:\n",
        "                # xywhr: x_center, y_center, width, height, rotation\n",
        "                r_box = box.xywhr[0].cpu().numpy().tolist()\n",
        "                cls_id = int(box.cls[0].item())\n",
        "                conf = float(box.conf[0].item())\n",
        "\n",
        "                detections.append({{\n",
        "                    \"class\": result.names[cls_id],\n",
        "                    \"confidence\": conf,\n",
        "                    \"type\": \"OBB\",\n",
        "                    \"bbox\": {{\n",
        "                        \"x_center\": r_box[0],\n",
        "                        \"y_center\": r_box[1],\n",
        "                        \"width\": r_box[2],\n",
        "                        \"height\": r_box[3],\n",
        "                        \"rotation\": r_box[4]\n",
        "                    }}\n",
        "                }})\n",
        "        else:\n",
        "            # Fallback for standard rect models\n",
        "            for box in result.boxes:\n",
        "                xyxy = box.xyxy[0].cpu().numpy().tolist()\n",
        "                cls_id = int(box.cls[0].item())\n",
        "                conf = float(box.conf[0].item())\n",
        "                detections.append({{\n",
        "                    \"class\": result.names[cls_id],\n",
        "                    \"confidence\": conf,\n",
        "                    \"type\": \"RECT\",\n",
        "                    \"bbox\": xyxy\n",
        "                }})\n",
        "\n",
        "        return JSONResponse(content={{\n",
        "            \"count\": len(detections),\n",
        "            \"detections\": detections\n",
        "        }})\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Inference error: {{e}}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"‚úÖ Generated app.py successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_server"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import sys\n",
        "from pyngrok import ngrok\n",
        "\n",
        "print(\"üöÄ Launching & Validating Server\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Cleanup\n",
        "ngrok.kill()\n",
        "\n",
        "# 2. Start Uvicorn\n",
        "print(\"‚è≥ Starting Uvicorn process...\")\n",
        "process = subprocess.Popen(\n",
        "    [sys.executable, \"-m\", \"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", str(PORT)],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    encoding='utf-8',\n",
        "    bufsize=1\n",
        ")\n",
        "\n",
        "# 3. Health Check\n",
        "print(\"üè• Checking server health (timeout: 60s)...\\n\")\n",
        "server_ready = False\n",
        "health_url = f\"http://localhost:{PORT}/health\"\n",
        "\n",
        "start_time = time.time()\n",
        "while time.time() - start_time < 60:\n",
        "    try:\n",
        "        response = requests.get(health_url, timeout=1)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"\\n‚úÖ Server is HEALTHY!\")\n",
        "            print(f\"   Status: {data['status']}\")\n",
        "            print(f\"   Device: {data['device']}\")\n",
        "            server_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(2)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è Unexpected error: {e}\")\n",
        "\n",
        "if not server_ready:\n",
        "    print(\"\\n‚ùå Server failed to start.\")\n",
        "    print(process.stdout.read())\n",
        "    process.terminate()\n",
        "    raise RuntimeError(\"Server startup failed\")\n",
        "\n",
        "# 4. Ngrok Tunnel\n",
        "print(\"\\nüåê Initializing Public Tunnel...\")\n",
        "if NGROK_AUTHTOKEN and NGROK_AUTHTOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    try:\n",
        "        ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "        tunnel = ngrok.connect(PORT)\n",
        "        public_url = tunnel.public_url\n",
        "        print(f\"\\nüéâ API IS LIVE at: {public_url}\")\n",
        "        print(f\"   üìÑ Docs: {public_url}/docs\")\n",
        "        print(f\"   üîó Endpoint: {public_url}/api/v1/analyze/stream\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Ngrok Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No Ngrok token. Server local only.\")\n",
        "\n",
        "print(\"\\nüìú Streaming Logs (Press STOP to exit)...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 5. Log Loop\n",
        "try:\n",
        "    while True:\n",
        "        line = process.stdout.readline()\n",
        "        if line:\n",
        "            print(line.strip())\n",
        "        if process.poll() is not None:\n",
        "            print(\"‚ùå Server process terminated.\")\n",
        "            break\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Server stopped by user.\")\n",
        "    process.terminate()\n",
        "    ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_client"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Run this cell to verify the API works using a dummy image\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "print(\"üß™ Running Self-Test on /api/v1/analyze/stream\")\n",
        "\n",
        "# Create dummy image\n",
        "img = Image.fromarray(np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8))\n",
        "buf = io.BytesIO()\n",
        "img.save(buf, format='JPEG')\n",
        "buf.seek(0)\n",
        "\n",
        "try:\n",
        "    # Send POST request locally\n",
        "    response = requests.post(\n",
        "        f\"http://localhost:{PORT}/api/v1/analyze/stream\",\n",
        "        files={\"file\": (\"test.jpg\", buf, \"image/jpeg\")}\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úÖ Success! API Response:\")\n",
        "        print(response.json())\n",
        "    else:\n",
        "        print(f\"‚ùå Failed: {response.status_code}\")\n",
        "        print(response.text)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}