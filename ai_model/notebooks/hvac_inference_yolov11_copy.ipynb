{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_md"
      },
      "source": [
        "# üöÄ HVAC AI ‚Äî Production-Ready YOLOv11-OBB Inference Pipeline\n",
        "**End-to-End Oriented Bounding Box (OBB) Analysis for HVAC Blueprints**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Overview\n",
        "This notebook provides a complete, production-grade pipeline for analyzing HVAC diagrams using **YOLOv11-OBB**. Unlike standard object detection, OBB handles rotated components (VAVs, diffusers, thermostats) precisely, ensuring tight bounding box alignment.\n",
        "\n",
        "### üåü Key Enhancements\n",
        "- **True OBB Support**: Handles `xyxyxyxy` polygon coordinates for rotated detection.\n",
        "- **FP16 Optimization**: Automatic half-precision inference for 2x speed on T4/A100 GPUs.\n",
        "- **Self-Contained Deployment**: Generates a robust FastAPI server script dynamically.\n",
        "- **Blueprint Visualization**: Professional-grade plotting with synchronized legends.\n",
        "\n",
        "## üéØ Prerequisites\n",
        "1. **GPU Runtime**: T4 or better (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
        "2. **Model**: A trained YOLOv11-OBB model (`.pt` file).\n",
        "3. **Ngrok Token**: For exposing the API publicly (Optional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for model access\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Drive mounted at: /content/drive/MyDrive\")\n",
        "else:\n",
        "    print(\"‚úÖ Drive already mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_env"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üîß Environment Setup & Optimization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Install optimized dependencies\n",
        "print(\"\\nüì¶ Installing libraries...\")\n",
        "!pip install -q ultralytics>=8.3.0 fastapi>=0.115.0 uvicorn[standard]>=0.34.0\n",
        "!pip install -q python-multipart pyngrok>=7.0.0 python-dotenv opencv-python-headless\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# System Validation\n",
        "print(f\"\\nüêç Python: {sys.version.split()[0]}\")\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n",
        "print(f\"üëÅÔ∏è OpenCV: {cv2.__version__}\")\n",
        "\n",
        "DEVICE = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"\\n‚úÖ GPU Detected: {gpu_name} ({total_mem:.2f} GB)\")\n",
        "\n",
        "    # Enable cuDNN benchmark for optimized performance on fixed size inputs\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(\"   üöÄ cuDNN Benchmark Enabled\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: No GPU detected. Inference will be slow.\")\n",
        "\n",
        "print(\"\\n‚úÖ Environment Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_cell"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"‚öôÔ∏è  Pipeline Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- USER CONFIGURATION START ---\n",
        "# Path to your YOLOv11-OBB model\n",
        "MODEL_PATH = \"/content/drive/Shareddrives/HVAC/DECEMBER 24 OUTPUT WEIGHTS {dataset2}/hvac_obb_l_20251224_214011/weights/best.pt\"\n",
        "\n",
        "# Inference Settings\n",
        "CONF_THRESHOLD = 0.50      # Confidence threshold (0.0 - 1.0)\n",
        "IOU_THRESHOLD = 0.45       # NMS IoU threshold\n",
        "IMG_SIZE = 1024            # Inference image size (pixels)\n",
        "HALF_PRECISION = True      # Use FP16 if GPU is available (Faster)\n",
        "\n",
        "# Server Settings\n",
        "PORT = 8000\n",
        "NGROK_AUTHTOKEN = \"36hBoLt4A3L8yOYt96wKiCxxrwp_5wFbj1Frv6GoHARRQ6H6t\" # Optional: Add token from ngrok.com\n",
        "# --- USER CONFIGURATION END ---\n",
        "\n",
        "# Validation logic\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚ùå ERROR: Model not found at {MODEL_PATH}\")\n",
        "    print(\"   Please update MODEL_PATH to point to your .pt file.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model Path: {MODEL_PATH}\")\n",
        "\n",
        "print(f\"üéØ Configuration: Conf={CONF_THRESHOLD}, IoU={IOU_THRESHOLD}, Size={IMG_SIZE}, FP16={HALF_PRECISION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"üß† Model Loading & Initialization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    start_t = time.time()\n",
        "    model = YOLO(MODEL_PATH)\n",
        "\n",
        "    # Check if model supports OBB\n",
        "    task = model.task\n",
        "    print(f\"   Detected Task: {task.upper()}\")\n",
        "    if task != 'obb':\n",
        "        print(\"‚ö†Ô∏è  WARNING: This does not appear to be an OBB model. Results may vary.\")\n",
        "\n",
        "    # Move to GPU\n",
        "    model.to(DEVICE)\n",
        "    print(f\"‚úÖ Model loaded on {DEVICE} in {time.time()-start_t:.2f}s\")\n",
        "\n",
        "    # Print Classes\n",
        "    print(f\"\\nüìö Classes ({len(model.names)}):\")\n",
        "    for i, name in model.names.items():\n",
        "        if i < 5: print(f\"   - {name}\")\n",
        "    if len(model.names) > 5: print(\"   ... (and others)\")\n",
        "\n",
        "    # Warmup\n",
        "    print(\"\\nüî• Warming up GPU...\")\n",
        "    dummy_input = np.zeros((640, 640, 3), dtype=np.uint8)\n",
        "    model.predict(dummy_input, verbose=False, half=HALF_PRECISION)\n",
        "    print(\"‚úÖ Warmup complete\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Fatal Error loading model: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_vis"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "print(\"üñºÔ∏è  Interactive Inference & OBB Visualization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def run_inference_pipeline():\n",
        "    # 1. Upload Image\n",
        "    print(\"\\nüì§ Please upload an HVAC diagram image...\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded: return\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "\n",
        "    # 2. Preprocessing\n",
        "    original_pil = Image.open(filename).convert('RGB')\n",
        "    img_np = np.array(original_pil)\n",
        "\n",
        "    # 3. Inference (Optimized)\n",
        "    print(f\"\\nüîÑ Analyzing {filename}...\")\n",
        "    start_inf = time.time()\n",
        "    results = model.predict(\n",
        "        img_np,\n",
        "        conf=CONF_THRESHOLD,\n",
        "        iou=IOU_THRESHOLD,\n",
        "        imgsz=IMG_SIZE,\n",
        "        half=HALF_PRECISION,\n",
        "        verbose=False\n",
        "    )\n",
        "    end_inf = time.time()\n",
        "    result = results[0]\n",
        "\n",
        "    # 4. Process OBB Results\n",
        "    # Create a copy for drawing\n",
        "    draw_img = img_np.copy()\n",
        "\n",
        "    # Generate distinct colors for classes\n",
        "    np.random.seed(42)\n",
        "    color_map = {id: tuple(np.random.randint(0, 255, 3).tolist()) for id in result.names}\n",
        "\n",
        "    detections_found = 0\n",
        "    class_counts = {}\n",
        "\n",
        "    # Check for OBB data first, fallback to Boxes\n",
        "    if hasattr(result, 'obb') and result.obb is not None:\n",
        "        boxes_data = result.obb\n",
        "        is_obb = True\n",
        "    else:\n",
        "        boxes_data = result.boxes\n",
        "        is_obb = False\n",
        "        print(\"‚ö†Ô∏è  No OBB data found, falling back to standard boxes.\")\n",
        "\n",
        "    if boxes_data is not None:\n",
        "        for box in boxes_data:\n",
        "            cls_id = int(box.cls[0].item())\n",
        "            cls_name = result.names[cls_id]\n",
        "            conf = box.conf[0].item()\n",
        "            color = color_map[cls_id]\n",
        "\n",
        "            # Update stats\n",
        "            detections_found += 1\n",
        "            class_counts[cls_name] = class_counts.get(cls_name, 0) + 1\n",
        "\n",
        "            # Draw Logic\n",
        "            if is_obb:\n",
        "                # OBB returns xyxyxyxy (4 points)\n",
        "                # tensor shape: [4, 2]\n",
        "                pts = box.xyxyxyxy[0].cpu().numpy().astype(np.int32)\n",
        "                pts = pts.reshape((-1, 1, 2))\n",
        "                cv2.polylines(draw_img, [pts], isClosed=True, color=color, thickness=3)\n",
        "            else:\n",
        "                # Standard Box\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                cv2.rectangle(draw_img, (x1, y1), (x2, y2), color, 3)\n",
        "\n",
        "    # 5. Visualization with Matplotlib\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    gs = fig.add_gridspec(1, 2, width_ratios=[3, 1])\n",
        "    ax_img = fig.add_subplot(gs[0])\n",
        "    ax_legend = fig.add_subplot(gs[1])\n",
        "\n",
        "    # Image Plot\n",
        "    ax_img.imshow(draw_img)\n",
        "    ax_img.axis('off')\n",
        "    ax_img.set_title(f\"Inference Result ({detections_found} detections) | {(end_inf-start_inf)*1000:.1f}ms\", fontsize=14)\n",
        "\n",
        "    # Legend Plot\n",
        "    legend_elements = []\n",
        "    sorted_counts = dict(sorted(class_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    for name, count in sorted_counts.items():\n",
        "        # Find ID for name\n",
        "        cid = [k for k, v in result.names.items() if v == name][0]\n",
        "        c_norm = [c/255 for c in color_map[cid]] # Normalize for matplotlib\n",
        "        legend_elements.append(Patch(facecolor=c_norm, edgecolor='black', label=f\"{name}: {count}\"))\n",
        "\n",
        "    ax_legend.axis('off')\n",
        "    if legend_elements:\n",
        "        ax_legend.legend(handles=legend_elements, loc='center', title=\"Component Counts\", fontsize=12, title_fontsize=14)\n",
        "    else:\n",
        "        ax_legend.text(0.5, 0.5, \"No Detections\", ha='center', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('obb_result.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"‚úÖ Results saved to obb_result.png\")\n",
        "\n",
        "# Run the pipeline\n",
        "run_inference_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_server"
      },
      "outputs": [],
      "source": [
        "print(\"üìù Generating Production Server Code...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# We write the server code to a file so it runs independently\n",
        "# Note: In the f-string below, we use {{ }} for actual Python dictionaries\n",
        "# and { } for the variables we want to inject from the notebook.\n",
        "\n",
        "server_code = f\"\"\"\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import logging\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_PATH = r'{MODEL_PATH}'\n",
        "CONF_THRES = {CONF_THRESHOLD}\n",
        "IOU_THRES = {IOU_THRESHOLD}\n",
        "IMG_SIZE = {IMG_SIZE}\n",
        "HALF = {HALF_PRECISION}\n",
        "\n",
        "# --- LOGGING ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"HVAC-Service\")\n",
        "\n",
        "app = FastAPI(title=\"HVAC YOLOv11-OBB Inference API\")\n",
        "\n",
        "# --- GLOBAL MODEL LOADER ---\n",
        "model = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def load_model():\n",
        "    global model\n",
        "    logger.info(f\"Loading model from {{MODEL_PATH}}...\")\n",
        "    try:\n",
        "        model = YOLO(MODEL_PATH)\n",
        "        if torch.cuda.is_available():\n",
        "            model.to('cuda')\n",
        "            logger.info(\"Model loaded on GPU\")\n",
        "        else:\n",
        "            logger.info(\"Model loaded on CPU\")\n",
        "\n",
        "        # Warmup\n",
        "        model.predict(np.zeros((640,640,3), dtype=np.uint8), verbose=False, half=HALF)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model: {{e}}\")\n",
        "        raise RuntimeError(\"Model loading failed\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {{\"status\": \"healthy\", \"device\": str(model.device) if model else \"unknown\"}}\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze_image(file: UploadFile = File(...)):\n",
        "    if not model:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
        "\n",
        "    try:\n",
        "        # Read Image\n",
        "        contents = await file.read()\n",
        "        nparr = np.frombuffer(contents, np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if img is None:\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid image format\")\n",
        "\n",
        "        # Inference\n",
        "        results = model.predict(\n",
        "            img,\n",
        "            conf=CONF_THRES,\n",
        "            iou=IOU_THRES,\n",
        "            imgsz=IMG_SIZE,\n",
        "            half=HALF\n",
        "        )\n",
        "\n",
        "        result = results[0]\n",
        "        detections = []\n",
        "\n",
        "        # Handle OBB vs Standard\n",
        "        if hasattr(result, 'obb') and result.obb is not None:\n",
        "            # OBB Logic\n",
        "            for box in result.obb:\n",
        "                # xywhr: x_center, y_center, width, height, rotation\n",
        "                r_box = box.xywhr[0].cpu().numpy().tolist()\n",
        "                cls_id = int(box.cls[0].item())\n",
        "                conf = float(box.conf[0].item())\n",
        "\n",
        "                detections.append({{\n",
        "                    \"class\": result.names[cls_id],\n",
        "                    \"confidence\": conf,\n",
        "                    \"type\": \"OBB\",\n",
        "                    \"bbox\": {{\n",
        "                        \"x_center\": r_box[0],\n",
        "                        \"y_center\": r_box[1],\n",
        "                        \"width\": r_box[2],\n",
        "                        \"height\": r_box[3],\n",
        "                        \"rotation\": r_box[4]\n",
        "                    }}\n",
        "                }})\n",
        "        else:\n",
        "            # Standard Box Logic fallback\n",
        "            for box in result.boxes:\n",
        "                xyxy = box.xyxy[0].cpu().numpy().tolist()\n",
        "                cls_id = int(box.cls[0].item())\n",
        "                conf = float(box.conf[0].item())\n",
        "                detections.append({{\n",
        "                    \"class\": result.names[cls_id],\n",
        "                    \"confidence\": conf,\n",
        "                    \"type\": \"RECT\",\n",
        "                    \"bbox\": xyxy\n",
        "                }})\n",
        "\n",
        "        return JSONResponse(content={{\n",
        "            \"count\": len(detections),\n",
        "            \"detections\": detections\n",
        "        }})\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Inference error: {{e}}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"‚úÖ Generated app.py successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_server"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "print(\"üöÄ Launching Inference Server\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Setup Ngrok (Optional)\n",
        "if NGROK_AUTHTOKEN and NGROK_AUTHTOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    try:\n",
        "        ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "        public_url = ngrok.connect(PORT).public_url\n",
        "        print(f\"\\nüåê Public API URL: {public_url}\")\n",
        "        print(f\"   Docs: {public_url}/docs\")\n",
        "        print(f\"   Test Endpoint: {public_url}/analyze (POST image)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Ngrok Error: {e}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No Ngrok token provided. Server running locally only.\")\n",
        "    print(f\"   Local URL: http://localhost:{PORT}\")\n",
        "\n",
        "# 2. Run Uvicorn in Background\n",
        "def run_uvicorn():\n",
        "    subprocess.run([\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", str(PORT)])\n",
        "\n",
        "print(\"\\n‚è≥ Starting server... (Check the 'Stop' button to kill)\")\n",
        "thread = threading.Thread(target=run_uvicorn)\n",
        "thread.start()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}