{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fXLkcSp5Ljj"
      },
      "source": [
        "# üöÄ HVAC AI ‚Äî Production-Ready YOLO11 Inference Server\n",
        "**Optimized Turn-Key Backend/Inference Notebook**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Overview\n",
        "Production-ready YOLO11 inference deployment with:\n",
        "- ‚úÖ Comprehensive GPU & dependency validation\n",
        "- ‚úÖ Optimized configuration management\n",
        "- ‚úÖ Error handling & monitoring\n",
        "- ‚úÖ Testing & benchmarking\n",
        "- ‚úÖ Security best practices\n",
        "- ‚úÖ Turn-key deployment\n",
        "\n",
        "## üéØ Prerequisites\n",
        "1. **GPU Runtime**: T4 or better (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
        "2. **Trained Model**: YOLO11 `.pt` file in Google Drive\n",
        "3. **Ngrok Token**: Free token from [ngrok.com](https://ngrok.com/)\n",
        "4. **Test Image**: Sample HVAC blueprint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMM7FrIo5P26"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for model access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive mounted at: /content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RUwtJpK5Ljn"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîß Environment Setup & Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Clone repository\n",
        "print(\"\\nüì¶ Cloning repository...\")\n",
        "!git clone https://github.com/elliotttmiller/hvac-ai.git 2>/dev/null || echo \"Repository exists\"\n",
        "%cd hvac-ai\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\nüìö Installing dependencies (2-3 minutes)...\")\n",
        "!pip install -q ultralytics>=8.0.0 fastapi>=0.115.0 uvicorn[standard]>=0.34.0\n",
        "!pip install -q python-multipart>=0.0.9 pyngrok>=7.0.0 python-dotenv>=1.0.0\n",
        "!pip install -q Pillow>=10.0.0 numpy>=1.24.0 tqdm>=4.65.0\n",
        "\n",
        "# Validate environment\n",
        "print(\"\\nüîç System Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import torch\n",
        "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"   CUDA: {torch.version.cuda}\")\n",
        "    # Test GPU\n",
        "    test_tensor = torch.rand(1000, 1000).cuda()\n",
        "    _ = torch.matmul(test_tensor, test_tensor)\n",
        "    print(f\"   Test: ‚úÖ PASSED\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: No GPU! Set Runtime > GPU. Inference will be SLOW.\")\n",
        "\n",
        "print(\"\\n‚úÖ Environment Ready!\")\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax3wqr4_5Ljr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚öôÔ∏è  Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- UPDATE THESE VALUES ---\n",
        "MODEL_PATH = \"/content/drive/Shareddrives/HVAC/DECEMBER 24 OUTPUT WEIGHTS {dataset2}/hvac_obb_l_20251224_214011/weights/best.pt\"\n",
        "NGROK_AUTHTOKEN = \"36hBoLt4A3L8yOYt96wKiCxxrwp_5wFbj1Frv6GoHARRQ6H6t\"  # Get from ngrok.com\n",
        "\n",
        "# Server settings\n",
        "PORT = 8000\n",
        "DEFAULT_CONF_THRESHOLD = 0.75  # Increased from 0.50 for higher precision\n",
        "DEFAULT_IOU_THRESHOLD = 0.65   # Increased from 0.45 for better box filtering\n",
        "MAX_IMAGE_SIZE = 1024\n",
        "\n",
        "# Validation\n",
        "errors = []\n",
        "if not MODEL_PATH or not os.path.exists(MODEL_PATH):\n",
        "    errors.append(\"‚ùå MODEL_PATH invalid or not found\")\n",
        "else:\n",
        "    print(f\"‚úÖ Model: {MODEL_PATH}\")\n",
        "    print(f\"   Size: {os.path.getsize(MODEL_PATH) / 1e6:.1f} MB\")\n",
        "\n",
        "if not NGROK_AUTHTOKEN or NGROK_AUTHTOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"‚ö†Ô∏è  Ngrok token not set (optional, for public URL)\")\n",
        "else:\n",
        "    print(f\"‚úÖ Ngrok: {'*' * 20}{NGROK_AUTHTOKEN[-8:]}\")\n",
        "\n",
        "print(f\"\\nüéØ Inference: conf={DEFAULT_CONF_THRESHOLD}, iou={DEFAULT_IOU_THRESHOLD}, size={MAX_IMAGE_SIZE}\")\n",
        "\n",
        "# Write .env\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"MODEL_PATH={MODEL_PATH}\\nNGROK_AUTHTOKEN={NGROK_AUTHTOKEN}\\nPORT={PORT}\\n\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\n‚ùå Errors:\", \"\\n\".join(errors))\n",
        "else:\n",
        "    print(\"\\n‚úÖ Configuration valid\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_gsoC_f5Ljs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"ü§ñ Model Loading & Validation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüì• Loading model (10-30s)...\")\n",
        "start = time.time()\n",
        "model = YOLO(MODEL_PATH)\n",
        "print(f\"‚úÖ Loaded in {time.time() - start:.2f}s\")\n",
        "\n",
        "print(f\"\\nüìä Model Info:\")\n",
        "print(f\"   Device: {model.device}\")\n",
        "print(f\"   Classes: {len(model.names)}\")\n",
        "for idx, name in model.names.items():\n",
        "    print(f\"   [{idx}] {name}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')\n",
        "    print(f\"\\nüöÄ Model on GPU\")\n",
        "\n",
        "# Warm-up\n",
        "print(f\"\\nüî• Warm-up inference...\")\n",
        "dummy = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
        "start = time.time()\n",
        "_ = model.predict(dummy, verbose=False, conf=0.25)\n",
        "first_time = time.time() - start\n",
        "start = time.time()\n",
        "_ = model.predict(dummy, verbose=False, conf=0.25)\n",
        "second_time = time.time() - start\n",
        "\n",
        "print(f\"   First: {first_time*1000:.1f}ms\")\n",
        "print(f\"   Subsequent: {second_time*1000:.1f}ms (~{1.0/second_time:.0f} FPS)\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nüíæ GPU Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "\n",
        "print(\"\\n‚úÖ Model ready!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRMzH1w45Ljt"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "print(\"üöÄ HVAC AI ‚Äî Fully Synchronized Inference Pipeline\")\n",
        "print(\"=\"*75)\n",
        "print(\"‚úÖ Features: Guaranteed sync | No text labels | Professional legend | OBB compatible\")\n",
        "print(\"=\"*75)\n",
        "\n",
        "# =====================================================================\n",
        "# CONFIGURATION\n",
        "# =====================================================================\n",
        "print(\"\\nüîß Configuration & Setup\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load environment variables\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    print(\"‚úÖ Environment variables loaded\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è python-dotenv not installed, using defaults\")\n",
        "\n",
        "# Configuration with fallbacks\n",
        "MODEL_PATH = os.getenv('MODEL_PATH', \"/content/drive/Shareddrives/HVAC/yolo11m_run_v10/weights/best.pt\")\n",
        "DEFAULT_CONF_THRESHOLD = float(os.getenv('DEFAULT_CONF_THRESHOLD', '0.35'))\n",
        "DEFAULT_IOU_THRESHOLD = float(os.getenv('DEFAULT_IOU_THRESHOLD', '0.45'))\n",
        "MAX_IMAGE_SIZE = int(os.getenv('MAX_IMAGE_SIZE', '1024'))\n",
        "\n",
        "print(f\"üìã Configuration:\")\n",
        "print(f\"   Model Path: {MODEL_PATH}\")\n",
        "print(f\"   Confidence Threshold: {DEFAULT_CONF_THRESHOLD:.2f}\")\n",
        "print(f\"   IOU Threshold: {DEFAULT_IOU_THRESHOLD:.2f}\")\n",
        "print(f\"   Max Image Size: {MAX_IMAGE_SIZE}\")\n",
        "\n",
        "# Validate model path\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"\\n‚ùå ERROR: Model not found at {MODEL_PATH}\")\n",
        "    # Try to mount drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"\\nüîÑ Attempting to mount Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        if os.path.exists(MODEL_PATH):\n",
        "            print(\"‚úÖ Drive mounted successfully\")\n",
        "        else:\n",
        "            print(f\"‚ùå Model still not found at {MODEL_PATH}\")\n",
        "            raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to mount drive: {str(e)}\")\n",
        "        raise\n",
        "else:\n",
        "    print(f\"‚úÖ Model found: {os.path.getsize(MODEL_PATH) / 1e6:.1f} MB\")\n",
        "\n",
        "# =====================================================================\n",
        "# MODEL LOADING\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üß† Model Loading & Validation\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    print(\"üì• Loading model...\")\n",
        "    model = YOLO(MODEL_PATH)\n",
        "    print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "    # Verify model type\n",
        "    is_obb = hasattr(model, 'predict_obb') or 'obb' in MODEL_PATH.lower()\n",
        "    print(f\"üîç Model Type: {'OBB' if is_obb else 'Standard'}\")\n",
        "\n",
        "    # Print class information\n",
        "    print(\"\\nüìã Model Class Information:\")\n",
        "    print(f\"   Total classes: {len(model.names)}\")\n",
        "    for idx, name in model.names.items():\n",
        "        print(f\"   [{idx}] {name}\")\n",
        "\n",
        "    # GPU setup\n",
        "    if torch.cuda.is_available():\n",
        "        model.to('cuda')\n",
        "        print(\"üöÄ Model moved to GPU\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è CPU Only Mode - Inference will be slower\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# =====================================================================\n",
        "# IMAGE UPLOAD & PROCESSING\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üñºÔ∏è Image Upload & Processing\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nüì§ Upload test image...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"‚ùå No image uploaded. Please upload an image to continue.\")\n",
        "    raise ValueError(\"No image uploaded\")\n",
        "\n",
        "img_path = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Image uploaded: {img_path}\")\n",
        "\n",
        "try:\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    print(f\"\\nüìä Image Analysis:\")\n",
        "    print(f\"   Dimensions: {img.size[0]}x{img.size[1]} pixels\")\n",
        "    print(f\"   Mode: {img.mode}\")\n",
        "\n",
        "    # Check if image needs resizing\n",
        "    max_dim = max(img.size)\n",
        "    if max_dim > MAX_IMAGE_SIZE:\n",
        "        scale_factor = MAX_IMAGE_SIZE / max_dim\n",
        "        new_size = (int(img.size[0] * scale_factor), int(img.size[1] * scale_factor))\n",
        "        print(f\"   üîç Resizing from {img.size} to {new_size}\")\n",
        "        img = img.resize(new_size, Image.LANCZOS)\n",
        "        img_array = np.array(img)\n",
        "\n",
        "    # Basic image quality check\n",
        "    mean_pixel = np.mean(img_array)\n",
        "    std_pixel = np.std(img_array)\n",
        "    print(f\"\\nüîç Image Quality Check:\")\n",
        "    print(f\"   Brightness: {mean_pixel:.1f}\")\n",
        "    print(f\"   Contrast: {std_pixel:.1f}\")\n",
        "\n",
        "    if mean_pixel < 50 or mean_pixel > 200:\n",
        "        print(\"   ‚ö†Ô∏è Unusual brightness - may affect detection\")\n",
        "    if std_pixel < 30:\n",
        "        print(\"   ‚ö†Ô∏è Low contrast - may affect detection\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error processing image: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# =====================================================================\n",
        "# INFERENCE & SYNCHRONIZED DETECTION COUNTING\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîç Inference & Synchronized Detection Counting\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    print(f\"\\nüîÑ Running inference with conf={DEFAULT_CONF_THRESHOLD}...\")\n",
        "    start = time.time()\n",
        "\n",
        "    # Run inference\n",
        "    results = model.predict(img_array,\n",
        "                           conf=DEFAULT_CONF_THRESHOLD,\n",
        "                           iou=DEFAULT_IOU_THRESHOLD,\n",
        "                           imgsz=MAX_IMAGE_SIZE,\n",
        "                           verbose=False)\n",
        "\n",
        "    inf_time = (time.time() - start) * 1000\n",
        "    result = results[0]\n",
        "\n",
        "    # CRITICAL: Synchronized detection counting\n",
        "    # Check for both standard and OBB results\n",
        "    if hasattr(result, 'boxes') and result.boxes is not None and len(result.boxes) > 0:\n",
        "        boxes = result.boxes\n",
        "        detection_count = len(boxes)\n",
        "        print(f\"‚úÖ Detections found: {detection_count}\")\n",
        "    elif hasattr(result, 'obb') and result.obb is not None and len(result.obb) > 0:\n",
        "        boxes = result.obb\n",
        "        detection_count = len(boxes)\n",
        "        print(f\"‚úÖ Detections found: {detection_count} (OBB model)\")\n",
        "    else:\n",
        "        boxes = None\n",
        "        detection_count = 0\n",
        "        print(\"‚ö†Ô∏è No detections found\")\n",
        "\n",
        "    print(f\"‚úÖ Complete: {inf_time:.1f}ms ({1000.0/inf_time:.1f} FPS)\")\n",
        "\n",
        "    # Class breakdown\n",
        "    class_counts = {}\n",
        "    if detection_count > 0:\n",
        "        for box in boxes:\n",
        "            cls_id = int(box.cls[0])\n",
        "            name = model.names[cls_id]\n",
        "            class_counts[name] = class_counts.get(name, 0) + 1\n",
        "\n",
        "        print(f\"\\nüìä Detection Breakdown by Class:\")\n",
        "        for name, count in sorted(class_counts.items()):\n",
        "            print(f\"   {name}: {count}\")\n",
        "    else:\n",
        "        print(\"\\nüìä No components detected\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during inference: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# =====================================================================\n",
        "# VISUALIZATION & LEGEND (FULLY SYNCHRONIZED)\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üé® Visualization & Legend (Fully Synchronized)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    # Create visualization with no text labels\n",
        "    plot_img = img_array.copy()\n",
        "\n",
        "    # Draw bounding boxes with colors but no text\n",
        "    if detection_count > 0:\n",
        "        # Custom high-contrast color palette\n",
        "        colors = [\n",
        "            (255, 0, 0),       # Red\n",
        "            (0, 255, 0),       # Green\n",
        "            (0, 0, 255),       # Blue\n",
        "            (255, 255, 0),     # Yellow\n",
        "            (255, 0, 255),     # Magenta\n",
        "            (0, 255, 255),     # Cyan\n",
        "            (128, 0, 0),       # Maroon\n",
        "            (0, 128, 0),       # Dark Green\n",
        "            (0, 0, 128),       # Navy\n",
        "            (128, 128, 0),     # Olive\n",
        "            (128, 0, 128),     # Purple\n",
        "            (0, 128, 128),     # Teal\n",
        "        ]\n",
        "\n",
        "        # Draw all boxes\n",
        "        for i, box in enumerate(boxes):\n",
        "            # Get coordinates\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "\n",
        "            # Get class ID and color\n",
        "            cls_id = int(box.cls[0])\n",
        "            color = colors[cls_id % len(colors)]\n",
        "\n",
        "            # Draw rectangle\n",
        "            cv2.rectangle(plot_img, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "    # Create figure with legend\n",
        "    fig = plt.figure(figsize=(24, 10))\n",
        "    gs = fig.add_gridspec(1, 3, width_ratios=[1, 1.2, 0.4])\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[0])\n",
        "    ax2 = fig.add_subplot(gs[1])\n",
        "    ax3 = fig.add_subplot(gs[2])\n",
        "\n",
        "    # Original image\n",
        "    ax1.imshow(img_array)\n",
        "    ax1.set_title('Original Image', fontsize=14)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Result with bounding boxes (NO TEXT)\n",
        "    ax2.imshow(plot_img)\n",
        "    ax2.set_title(f'Detections: {detection_count} ({inf_time:.0f}ms)', fontsize=14)\n",
        "    ax2.axis('off')\n",
        "\n",
        "    # Legend panel\n",
        "    ax3.axis('off')\n",
        "    legend_elements = []\n",
        "\n",
        "    # Add legend entries\n",
        "    if detection_count > 0:\n",
        "        for cls_id, cls_name in model.names.items():\n",
        "            if cls_id < len(colors):\n",
        "                color = colors[cls_id]\n",
        "                normalized_color = (color[0]/255, color[1]/255, color[2]/255)\n",
        "\n",
        "                # Count detections\n",
        "                count = class_counts.get(cls_name, 0)\n",
        "\n",
        "                if count > 0:  # Only show classes that were detected\n",
        "                    legend_elements.append(Patch(\n",
        "                        facecolor=normalized_color,\n",
        "                        edgecolor='black',\n",
        "                        label=f\"{cls_name} ({count})\"\n",
        "                    ))\n",
        "\n",
        "    # Create legend\n",
        "    if legend_elements:\n",
        "        ax3.legend(\n",
        "            handles=legend_elements,\n",
        "            loc='center',\n",
        "            fontsize=10,\n",
        "            frameon=True,\n",
        "            framealpha=0.95,\n",
        "            title=\"HVAC Components\",\n",
        "            title_fontsize=12,\n",
        "            facecolor='white'\n",
        "        )\n",
        "        ax3.set_title(\"Detection Legend\", fontsize=14, pad=20)\n",
        "    else:\n",
        "        ax3.text(0.5, 0.5, \"No components detected\",\n",
        "                 ha='center', va='center', fontsize=14)\n",
        "        ax3.text(0.5, 0.4, \"Check model compatibility\",\n",
        "                 ha='center', va='center', fontsize=12)\n",
        "        ax3.set_title(\"No Detections\", fontsize=14, pad=20)\n",
        "\n",
        "    plt.tight_layout(pad=3.0)\n",
        "    plt.savefig('/content/synchronized_inference_result.png', bbox_inches='tight', dpi=150)\n",
        "    print(\"‚úÖ Synchronized visualization saved to /content/synchronized_inference_result.png\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during visualization: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# =====================================================================\n",
        "# FINAL SUMMARY\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Final Summary\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üéØ Total Detections: {detection_count}\")\n",
        "print(f\"‚ö° Inference Time: {inf_time:.1f}ms\")\n",
        "print(f\"üìä Class Distribution: {len(class_counts)} classes detected\")\n",
        "print(f\"üíæ Results saved to: /content/synchronized_inference_result.png\")\n",
        "\n",
        "if detection_count == 0:\n",
        "    print(\"\\nüö® CRITICAL: No detections found\")\n",
        "    print(\"   Possible causes:\")\n",
        "    print(\"   1. Model incompatible with diagram style\")\n",
        "    print(\"   2. Confidence threshold too high\")\n",
        "    print(\"   3. Image quality issues\")\n",
        "    print(\"   4. Model not trained on this component type\")\n",
        "else:\n",
        "    print(\"\\nüéâ SUCCESS: Detections properly synchronized with visualization\")\n",
        "    print(\"   The legend now accurately reflects the detected components\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üí° Pro Tips for Production:\")\n",
        "print(\"   ‚Ä¢ Always count detections immediately after inference\")\n",
        "print(\"   ‚Ä¢ Use the same result object for visualization and counting\")\n",
        "print(\"   ‚Ä¢ Verify model compatibility with your diagram style\")\n",
        "print(\"   ‚Ä¢ Save this pipeline as a reusable function\")\n",
        "print(\"=\"*75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5a_1AyI5Lju"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "print(\"üöÄ Deploying API Server\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Validate configuration\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"\\n‚ùå ERROR: MODEL_PATH not found. Check configuration.\")\n",
        "    raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
        "\n",
        "print(f\"‚úÖ Model found: {MODEL_PATH}\")\n",
        "\n",
        "# Setup ngrok tunnel\n",
        "if NGROK_AUTHTOKEN and NGROK_AUTHTOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"\\nüåê Setting up ngrok tunnel...\")\n",
        "    ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "    public_url = ngrok.connect(PORT)\n",
        "    print(f\"\\n‚úÖ API LIVE!\")\n",
        "    print(f\"   Public URL: {public_url.public_url}\")\n",
        "    print(f\"   API Docs: {public_url.public_url}/docs\")\n",
        "    print(f\"   Health: {public_url.public_url}/health\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No ngrok token - server will be local only\")\n",
        "    print(f\"   Local URL: http://localhost:{PORT}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üé¨ Starting server (Press STOP button to shutdown)...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Validate python-services directory exists\n",
        "if not os.path.exists('python-services'):\n",
        "    print(\"‚ùå ERROR: python-services directory not found\")\n",
        "    print(f\"   Current directory: {os.getcwd()}\")\n",
        "    print(\"   Please ensure you're in the hvac-ai repository root\")\n",
        "    raise FileNotFoundError(\"python-services directory not found\")\n",
        "\n",
        "%cd python-services\n",
        "# Use PORT variable via Python string formatting\n",
        "import subprocess\n",
        "subprocess.run([\"uvicorn\", \"hvac_analysis_service:app\", \"--host\", \"0.0.0.0\", \"--port\", str(PORT), \"--reload\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}