{
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "# üöÄ HVAC AI ‚Äî Inference Server (Final, Secure Version)\n",
        "\n",
        "This notebook provides a robust, one-click solution for deploying the HVAC AI backend on a Google Colab GPU. It clones the repository, installs dependencies, and correctly launches the FastAPI server with a public `ngrok` URL.\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the cells in order.\n",
        "2. When you reach **Cell 4**, edit it to add your **NEW** `NGROK_AUTHTOKEN` and verify the `MODEL_PATH`.\n",
        "3. Run the remaining cells to launch the server."
      ],
      "id": "intro_markdown"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1_header"
      },
      "source": [
        "### Phase 1: Environment Setup"
      ],
      "id": "phase1_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo_cell"
      },
      "outputs": [],
      "source": [
        "# Cell 1 ‚Äî Clone repo and change to project root\n",
        "!git clone https://github.com/elliotttmiller/hvac-ai.git || true\n",
        "%cd hvac-ai\n",
        "print('‚úÖ Repo ready at /content/hvac-ai')"
      ],
      "id": "clone_repo_cell"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps_cell"
      },
      "outputs": [],
      "source": [
        "# Cell 2 ‚Äî Install dependencies\n",
        "!pip install -q fastapi uvicorn python-multipart flask flask-cors pyngrok python-dotenv nest-asyncio\n",
        "!pip install -q torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118 || true\n",
        "!pip install -q opencv-python pycocotools matplotlib onnxruntime onnx || true\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git || true\n",
        "print('‚úÖ Dependencies installation attempted.')"
      ],
      "id": "install_deps_cell"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive_cell"
      },
      "outputs": [],
      "source": [
        "# Cell 3 ‚Äî Mount Google Drive\n",
        "from google.colab import drive\n",
        "print('Mounting Google Drive to access the fine-tuned model...')\n",
        "drive.mount('/content/drive')\n",
        "print('‚úÖ Drive mounted.')"
      ],
      "id": "mount_drive_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2_header"
      },
      "source": [
        "### Phase 2: Create and Configure `.env` File\n",
        "\n",
        "This is the **only configuration step** you need to perform."
      ],
      "id": "phase2_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_env_cell"
      },
      "outputs": [],
      "source": [
        "%%writefile .env\n",
        "# ‚ö†Ô∏è ACTION REQUIRED: Edit these values before running the next cell\n",
        "\n",
        "# 1. Paste your NEW, secure ngrok token here\n",
        "NGROK_AUTHTOKEN=\"36hBoLt4A3L8yOYt96wKiCxxrwp_5wFbj1Frv6GoHARRQ6H6t\"\n",
        "\n",
        "# 2. Verify this path points to your fine-tuned model on Google Drive\n",
        "MODEL_PATH=\"/content/drive/MyDrive/sam_finetuning_results/latest_checkpoint_multiprompt_v1.pth\"\n",
        "\n",
        "# 3. The port for the server.\n",
        "PORT=8000\n",
        "print('‚úÖ .env file created. Please edit the placeholder values before proceeding.')"
      ],
      "id": "create_env_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase3_header"
      },
      "source": [
        "### Phase 3: Authenticate `ngrok`"
      ],
      "id": "phase3_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngrok_auth_cell"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "ngrok_token = os.getenv('NGROK_AUTHTOKEN')\n",
        "if not ngrok_token or 'REPLACE_WITH' in ngrok_token:\n",
        " raise SystemExit('‚ùå CRITICAL: NGROK_AUTHTOKEN is missing or is still a placeholder. Please edit the .env cell above and rerun it.')\n",
        "print('Authenticating ngrok...')\n",
        "!ngrok authtoken {ngrok_token}\n",
        "print('‚úÖ ngrok authtoken set successfully.')"
      ],
      "id": "ngrok_auth_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase4_header"
      },
      "source": [
        "### Phase 4: Launch the Inference Server"
      ],
      "id": "phase4_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_server_cell"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "PORT = int(os.getenv('PORT', '8000'))\n",
        "\n",
        "# Ensure any stale tunnels are closed for a clean start\n",
        "ngrok.kill()\n",
        "\n",
        "# Start the ngrok tunnel\n",
        "public_url = ngrok.connect(PORT).public_url\n",
        "print('-' * 60)\n",
        "print(f'‚úÖ Your inference server is live at: {public_url}')\n",
        "print(f'üëâ Use this URL for the NEXT_PUBLIC_AI_SERVICE_URL in your frontend .env.local file.')\n",
        "print('-' * 60)\n",
        "\n",
        "print('\\nüöÄ Starting FastAPI backend... This cell will run continuously and show live server logs.')\n",
        "\n",
        "# Change directory into the services folder so the module import is valid\n",
        "%cd python-services\n",
        "\n",
        "# Run uvicorn using the correct module:app syntax. The --reload flag is removed for stability.\n",
        "!uvicorn hvac_analysis_service:app --host 0.0.0.0 --port {PORT}"
      ],
      "id": "launch_server_cell"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  }
}