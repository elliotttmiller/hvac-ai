{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "50c2c412",
      "metadata": {},
      "source": [
        "# üü£ HVAC-Specific SAM Fine-Tuning Pipeline (DGX Master - v4.0)\n",
        "## ‚ö° Full Multi-GPU Architecture with HVAC-Specialized Logic\n",
        "\n",
        "**Pipeline Overview:**\n",
        "1.  **Environment:** Installs headless libraries to prevent DGX display driver crashes.\n",
        "2.  **Data Prep:** Checks, downloads, and intelligently extracts your HVAC dataset structure.\n",
        "3.  **Script Generation:** Writes the **complete** training logic (Dataset, Adaptive Prompting, AMP, DDP) to `train_dgx.py`.\n",
        "4.  **Execution:** Launches the job across all available GPUs using `torchrun`.\n",
        "5.  **Visualization:** Loads the results and plots metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "356fe5a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Environment & Dependency Check\n",
        "import sys, subprocess, os\n",
        "\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-cache-dir\", package])\n",
        "\n",
        "print(\"‚öôÔ∏è Checking DGX Environment...\")\n",
        "try:\n",
        "    import segment_anything\n",
        "    import cv2\n",
        "    import pycocotools\n",
        "except ImportError:\n",
        "    print(\"   Installing missing libraries...\")\n",
        "    install(\"git+https://github.com/facebookresearch/segment-anything.git\")\n",
        "    install(\"opencv-python-headless\") # Critical for server environments\n",
        "    install(\"pycocotools\")\n",
        "    install(\"matplotlib\")\n",
        "    install(\"onnxruntime\")\n",
        "    install(\"onnx\")\n",
        "\n",
        "print(\"‚úÖ Environment Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8125d88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Configuration, Asset Management & Data Extraction\n",
        "import os, json, torch, zipfile, urllib.request, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. PATH CONFIGURATION (DGX Standard) ---\n",
        "WORKSPACE_ROOT = Path(os.getcwd()) \n",
        "DATA_DIR = WORKSPACE_ROOT / 'data'\n",
        "OUTPUT_DIR = WORKSPACE_ROOT / 'results'\n",
        "\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- 2. HYPERPARAMETERS (Complete) ---\n",
        "CONFIG = {\n",
        "    # Architecture\n",
        "    'model_type': 'vit_h',\n",
        "    'image_size': 1024,\n",
        "    \n",
        "    # DGX Scaling\n",
        "    'batch_size': 4,          # Per-GPU batch size. (e.g., 4 GPUs = Effective Batch 16)\n",
        "    'num_workers': 8,         # High I/O for engineering drawings\n",
        "    'num_epochs': 50,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    'use_amp': True,          # Automatic Mixed Precision (A100 Tensor Core Optimization)\n",
        "    \n",
        "    # HVAC Specialization (Adaptive Prompting)\n",
        "    'prompt_strategy': 'multi_prompt', # Uses Box, Point, and Scribble\n",
        "    'bbox_noise_max': 0.1,    # High noise for large objects (Ducts)\n",
        "    'bbox_noise_min': 0.02,   # Low noise for small objects (Valves)\n",
        "    'min_mask_area': 25,      # Sensitivity threshold for small instruments\n",
        "    \n",
        "    # Checkpointing\n",
        "    'resume_training': False,\n",
        "    'checkpoint_interval': 300, # Batches between checkpoints\n",
        "    \n",
        "    # Asset Paths\n",
        "    'model_url': \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
        "    'model_path': str(DATA_DIR / 'sam_vit_h_4b8939.pth'),\n",
        "    'dataset_zip_path': str(DATA_DIR / 'hvac_dataset_coco.zip'),\n",
        "    'unzip_path': str(DATA_DIR / 'hvac_dataset_coco'),\n",
        "    'annotations_file_name': '_annotations.coco.json',\n",
        "    'best_model_save_path': str(OUTPUT_DIR / 'best_model_dgx.pth'),\n",
        "    'latest_checkpoint_save_path': str(OUTPUT_DIR / 'latest_checkpoint_dgx.pth'),\n",
        "    'metrics_save_path': str(OUTPUT_DIR / 'training_log.json')\n",
        "}\n",
        "\n",
        "# Save config for the script to load later\n",
        "CONFIG_PATH = WORKSPACE_ROOT / 'config.json'\n",
        "with open(CONFIG_PATH, 'w') as f:\n",
        "    json.dump(CONFIG, f, indent=4)\n",
        "\n",
        "# --- 3. ASSET FETCHING ---\n",
        "if not os.path.exists(CONFIG['model_path']):\n",
        "    print(f\"‚¨áÔ∏è Downloading Base SAM Weights...\")\n",
        "    urllib.request.urlretrieve(CONFIG['model_url'], CONFIG['model_path'])\n",
        "else:\n",
        "    print(\"‚úÖ Base SAM Weights Present.\")\n",
        "\n",
        "# --- 4. SMART DATASET EXTRACTION ---\n",
        "# Ensures hvac_dataset_coco/train/_annotations.coco.json structure exists\n",
        "if not os.path.exists(CONFIG['unzip_path']):\n",
        "    if os.path.exists(CONFIG['dataset_zip_path']):\n",
        "        print(f\"üìÇ Unzipping HVAC Dataset...\")\n",
        "        with zipfile.ZipFile(CONFIG['dataset_zip_path'], 'r') as zf:\n",
        "            # Detect if root folder exists in zip\n",
        "            first_file = zf.namelist()[0]\n",
        "            if first_file.startswith('hvac_dataset_coco/'):\n",
        "                zf.extractall(DATA_DIR)\n",
        "            else:\n",
        "                zf.extractall(CONFIG['unzip_path'])\n",
        "        print(f\"‚úÖ Dataset Extracted.\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è WARNING: Dataset Zip not found at {CONFIG['dataset_zip_path']}. Please upload it.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset directory ready at: {CONFIG['unzip_path']}\")\n",
        "\n",
        "print(f\"\\nüöÄ Ready to launch on {torch.cuda.device_count()} GPUs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f31873b",
      "metadata": {},
      "source": [
        "### Phase 3: The Complete Training Script (`train_dgx.py`)\n",
        "This cell generates the actual python script. It contains **all** the logic from the original notebook, including the Dataset class, the specialized prompt generation, the combined loss function, and the training loop, wrapped in DDP (Distributed Data Parallel) code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87869595",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train_dgx.py\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from statistics import mean\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools import mask as mask_utils\n",
        "\n",
        "# Distributed Training Imports\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "# SAM Imports\n",
        "from segment_anything import sam_model_registry\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "# --- DDP INFRASTRUCTURE ---\n",
        "def setup_ddp():\n",
        "    dist.init_process_group(backend=\"nccl\")\n",
        "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "    torch.cuda.set_device(local_rank)\n",
        "    return local_rank\n",
        "\n",
        "def cleanup_ddp():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "def is_main_process():\n",
        "    return dist.get_rank() == 0\n",
        "\n",
        "# --- HVAC SPECIALIZED DATASET CLASS ---\n",
        "class HvacSamDataset(Dataset):\n",
        "    def __init__(self, coco: COCO, image_ids: List[int], split_path: str, config: dict, is_training: bool = True):\n",
        "        self.coco = coco\n",
        "        self.image_ids = image_ids\n",
        "        self.split_path = split_path\n",
        "        self.is_training = is_training\n",
        "        self.config = config\n",
        "        self.resize_transform = ResizeLongestSide(config['image_size'])\n",
        "        self.pixel_mean = torch.tensor([123.675, 116.28, 103.53]).view(-1, 1, 1)\n",
        "        self.pixel_std = torch.tensor([58.395, 57.12, 57.375]).view(-1, 1, 1)\n",
        "        self.prompt_strategy = config.get('prompt_strategy', 'multi_prompt')\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def _generate_prompt(self, mask: np.ndarray, bbox: List[float], img_area: float):\n",
        "        # Validation always uses perfect box for metrics\n",
        "        if not self.is_training:\n",
        "            return {'box': np.array(bbox)}\n",
        "\n",
        "        # Determine prompt type\n",
        "        if self.prompt_strategy == 'multi_prompt':\n",
        "            prompt_type = random.choice(['box', 'point', 'scribble'])\n",
        "        elif self.prompt_strategy == 'point':\n",
        "            prompt_type = 'point'\n",
        "        else:\n",
        "            prompt_type = 'box'\n",
        "\n",
        "        # --- HVAC ADAPTIVE LOGIC ---\n",
        "        if prompt_type == 'box':\n",
        "            x, y, w, h = bbox\n",
        "            box_area = w * h\n",
        "            rel_size = box_area / (img_area + 1e-6)\n",
        "            \n",
        "            # Adaptive Noise: Small objects (Valves) get less noise to prevent box drift off-target.\n",
        "            # Large objects (Ducts) get more noise to force model generalization.\n",
        "            if rel_size < 0.05: \n",
        "                noise_factor = self.config.get('bbox_noise_min', 0.02)\n",
        "            else:\n",
        "                noise_factor = self.config.get('bbox_noise_max', 0.1)\n",
        "                \n",
        "            x_noise = w * noise_factor * (random.random() - 0.5) * 2\n",
        "            y_noise = h * noise_factor * (random.random() - 0.5) * 2\n",
        "            w_noise = w * noise_factor * (random.random() - 0.5) * 2\n",
        "            h_noise = h * noise_factor * (random.random() - 0.5) * 2\n",
        "            \n",
        "            box = np.array([x + x_noise, y + y_noise, w + w_noise, h + h_noise])\n",
        "            return {'box': box}\n",
        "\n",
        "        points = np.argwhere(mask)\n",
        "        if len(points) == 0: return None\n",
        "\n",
        "        if prompt_type == 'point':\n",
        "            point = points[random.randint(0, len(points) - 1)]\n",
        "            point_coords = np.array([[point[1], point[0]]]) # (x, y)\n",
        "            point_labels = np.array([1])\n",
        "            return {'point_coords': point_coords, 'point_labels': point_labels}\n",
        "\n",
        "        elif prompt_type == 'scribble':\n",
        "            num_points = min(5, len(points))\n",
        "            point_indices = np.random.choice(len(points), num_points, replace=False)\n",
        "            scribble_points = points[point_indices]\n",
        "            point_coords = scribble_points[:, ::-1]\n",
        "            point_labels = np.ones(num_points)\n",
        "            return {'point_coords': point_coords, 'point_labels': point_labels}\n",
        "\n",
        "        return None\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_info = self.coco.loadImgs([img_id])[0]\n",
        "        \n",
        "        # Handle nested directory structures\n",
        "        image_path = os.path.join(self.split_path, img_info['file_name'])\n",
        "        if not os.path.exists(image_path):\n",
        "             image_path = os.path.join(os.path.dirname(self.split_path), img_info['file_name'])\n",
        "             if not os.path.exists(image_path):\n",
        "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "            \n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        orig_h, orig_w = image.shape[:2]\n",
        "        img_area = orig_h * orig_w\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
        "        annotations = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        masks, prompts = [], []\n",
        "        for ann in annotations:\n",
        "            if 'segmentation' not in ann or ann.get('iscrowd', 0) == 1: continue\n",
        "            mask = self.coco.annToMask(ann)\n",
        "            # HVAC Filter: Skip tiny noise artifacts, keep small valves\n",
        "            if mask.sum() < self.config['min_mask_area']: continue\n",
        "\n",
        "            prompt = self._generate_prompt(mask, ann['bbox'], img_area)\n",
        "            if prompt:\n",
        "                masks.append(mask.astype(bool))\n",
        "                prompts.append(prompt)\n",
        "\n",
        "        original_size = image.shape[:2]\n",
        "        resized_image = self.resize_transform.apply_image(image)\n",
        "        input_image_torch = torch.as_tensor(resized_image, dtype=torch.float32).permute(2, 0, 1).contiguous()\n",
        "        input_image_torch = (input_image_torch - self.pixel_mean) / self.pixel_std\n",
        "        h, w = input_image_torch.shape[-2:]\n",
        "        padh, padw = self.config['image_size'] - h, self.config['image_size'] - w\n",
        "        input_image_padded = torch.nn.functional.pad(input_image_torch, (0, padw, 0, padh))\n",
        "\n",
        "        return {\n",
        "            'image': input_image_padded,\n",
        "            'masks': masks,\n",
        "            'prompts': prompts,\n",
        "            'original_size': original_size,\n",
        "            'input_size': (h, w)\n",
        "        }\n",
        "\n",
        "def custom_collate_fn(batch: List[Dict]) -> Dict[str, Any]:\n",
        "    return {\n",
        "        'image': torch.stack([item['image'] for item in batch]),\n",
        "        'masks': [item['masks'] for item in batch],\n",
        "        'prompts': [item['prompts'] for item in batch],\n",
        "        'original_size': [item['original_size'] for item in batch],\n",
        "        'input_size': [item['input_size'] for item in batch]\n",
        "    }\n",
        "\n",
        "# --- OPTIMIZED LOSS FUNCTION ---\n",
        "def combined_loss(pred_masks: torch.Tensor, true_masks: torch.Tensor) -> torch.Tensor:\n",
        "    # 80% BCE (Pixel accuracy) + 20% Dice (Shape overlap) - Optimal for Engineering Drawings\n",
        "    bce_loss = nn.BCEWithLogitsLoss()(pred_masks, true_masks.float())\n",
        "    pred_flat = torch.sigmoid(pred_masks).reshape(-1)\n",
        "    true_flat = true_masks.reshape(-1)\n",
        "    intersection = (pred_flat * true_flat).sum()\n",
        "    dice_loss = 1 - (2. * intersection + 1e-8) / (pred_flat.sum() + true_flat.sum() + 1e-8)\n",
        "    return 0.8 * bce_loss + 0.2 * dice_loss\n",
        "\n",
        "# --- CORE TRAINING LOOP ---\n",
        "def run_epoch(model, dataloader, optimizer, is_training, device, config, scaler):\n",
        "    model.train(is_training)\n",
        "    epoch_losses, epoch_ious = [], []\n",
        "    \n",
        "    if is_main_process():\n",
        "        print(f\"{'Training' if is_training else 'Validating'}...\")\n",
        "    \n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        images = batch['image'].to(device, non_blocking=True)\n",
        "        all_gt_masks = batch['masks']\n",
        "        all_prompts = batch['prompts']\n",
        "\n",
        "        if is_training:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(is_training):\n",
        "            # AMP Context: Runs Image Encoder in FP16 for speed\n",
        "            with autocast(enabled=config['use_amp']):\n",
        "                image_embeddings = model.module.image_encoder(images)\n",
        "\n",
        "            batch_loss = 0\n",
        "            num_anns = 0\n",
        "\n",
        "            # Loop through batch images\n",
        "            for i in range(len(all_gt_masks)):\n",
        "                gt_masks = all_gt_masks[i]\n",
        "                prompts = all_prompts[i]\n",
        "                if len(gt_masks) == 0: continue\n",
        "\n",
        "                curr_embedding = image_embeddings[i].unsqueeze(0)\n",
        "\n",
        "                # Loop through HVAC components in current image\n",
        "                for j in range(len(gt_masks)):\n",
        "                    gt_mask_np = gt_masks[j]\n",
        "                    prompt = prompts[j]\n",
        "                    num_anns += 1\n",
        "\n",
        "                    gt_mask_torch = torch.from_numpy(gt_mask_np).unsqueeze(0).unsqueeze(0).to(device)\n",
        "                    transform = ResizeLongestSide(config['image_size'])\n",
        "                    box_torch, points_torch, labels_torch = None, None, None\n",
        "\n",
        "                    if 'box' in prompt:\n",
        "                        box_torch = torch.as_tensor(transform.apply_boxes(prompt['box'].reshape(1, 4), batch['original_size'][i]), dtype=torch.float, device=device)\n",
        "                    elif 'point_coords' in prompt:\n",
        "                        points_torch = torch.as_tensor(transform.apply_coords(prompt['point_coords'], batch['original_size'][i]), dtype=torch.float, device=device).unsqueeze(0)\n",
        "                        labels_torch = torch.as_tensor(prompt['point_labels'], dtype=torch.float, device=device).unsqueeze(0)\n",
        "\n",
        "                    # Decoder Forward Pass (Mixed Precision)\n",
        "                    with autocast(enabled=config['use_amp']):\n",
        "                        with torch.no_grad():\n",
        "                            sparse, dense = model.module.prompt_encoder(\n",
        "                                points=(points_torch, labels_torch) if points_torch is not None else None,\n",
        "                                boxes=box_torch,\n",
        "                                masks=None\n",
        "                            )\n",
        "                        \n",
        "                        low_res_masks, _ = model.module.mask_decoder(\n",
        "                            image_embeddings=curr_embedding,\n",
        "                            image_pe=model.module.prompt_encoder.get_dense_pe(),\n",
        "                            sparse_prompt_embeddings=sparse,\n",
        "                            dense_prompt_embeddings=dense,\n",
        "                            multimask_output=False\n",
        "                        )\n",
        "\n",
        "                        upscaled_masks = model.module.postprocess_masks(low_res_masks, batch['input_size'][i], batch['original_size'][i])\n",
        "                        loss = combined_loss(upscaled_masks, gt_mask_torch)\n",
        "                        batch_loss += loss\n",
        "\n",
        "                    if not is_training:\n",
        "                        pred = (torch.sigmoid(upscaled_masks) > 0.5).squeeze().cpu().numpy().astype(np.uint8)\n",
        "                        iou = mask_utils.iou([mask_utils.encode(np.asfortranarray(pred))], [mask_utils.encode(np.asfortranarray(gt_mask_np.astype(np.uint8)))], [0])[0][0]\n",
        "                        epoch_ious.append(iou)\n",
        "\n",
        "            if num_anns > 0:\n",
        "                norm_loss = batch_loss / num_anns\n",
        "                if is_training:\n",
        "                    # AMP Scaled Backward Pass\n",
        "                    scaler.scale(norm_loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "                epoch_losses.append(norm_loss.item())\n",
        "\n",
        "    # Sync Metrics across GPUs\n",
        "    mean_loss = torch.tensor(mean(epoch_losses) if epoch_losses else 0.0).to(device)\n",
        "    mean_iou = torch.tensor(mean(epoch_ious) if epoch_ious else 0.0).to(device)\n",
        "    dist.all_reduce(mean_loss, op=dist.ReduceOp.SUM)\n",
        "    dist.all_reduce(mean_iou, op=dist.ReduceOp.SUM)\n",
        "    world_size = dist.get_world_size()\n",
        "    return mean_loss.item() / world_size, mean_iou.item() / world_size\n",
        "\n",
        "# --- MAIN ORCHESTRATOR ---\n",
        "def main(config):\n",
        "    local_rank = setup_ddp()\n",
        "    device = torch.device(\"cuda\", local_rank)\n",
        "\n",
        "    # Load Datasets\n",
        "    train_dir = os.path.join(config['unzip_path'], 'train')\n",
        "    valid_dir = os.path.join(config['unzip_path'], 'valid')\n",
        "    \n",
        "    train_coco = COCO(os.path.join(train_dir, config['annotations_file_name']))\n",
        "    val_coco = COCO(os.path.join(valid_dir, config['annotations_file_name']))\n",
        "    \n",
        "    train_ds = HvacSamDataset(train_coco, train_coco.getImgIds(), train_dir, config, is_training=True)\n",
        "    val_ds = HvacSamDataset(val_coco, val_coco.getImgIds(), valid_dir, config, is_training=False)\n",
        "\n",
        "    train_sampler = DistributedSampler(train_ds)\n",
        "    val_sampler = DistributedSampler(val_ds, shuffle=False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], sampler=train_sampler, \n",
        "                              num_workers=config['num_workers'], collate_fn=custom_collate_fn, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=config['batch_size'], sampler=val_sampler, \n",
        "                            num_workers=config['num_workers'], collate_fn=custom_collate_fn, pin_memory=True)\n",
        "\n",
        "    # Model Setup\n",
        "    sam_model = sam_model_registry[config['model_type']]()\n",
        "    \n",
        "    # Load Weights\n",
        "    if config['resume_training'] and os.path.exists(config['latest_checkpoint_save_path']):\n",
        "        checkpoint = torch.load(config['latest_checkpoint_save_path'], map_location='cpu')\n",
        "        sam_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        if is_main_process(): print(f\"üîÑ Resuming from epoch {start_epoch}\")\n",
        "    else:\n",
        "        sam_model.load_state_dict(torch.load(config['model_path'], map_location='cpu'))\n",
        "        start_epoch = 0\n",
        "        if is_main_process(): print(\"üîÑ Loaded base SAM weights.\")\n",
        "\n",
        "    sam_model.to(device)\n",
        "    \n",
        "    # Freeze Encoders\n",
        "    for name, param in sam_model.named_parameters():\n",
        "        if name.startswith(\"image_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "            param.requires_grad = False\n",
        "\n",
        "    model = DDP(sam_model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "    scaler = GradScaler(enabled=config['use_amp'])\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "    best_iou = 0.0\n",
        "    \n",
        "    metrics_history = {'train_loss': [], 'val_loss': [], 'val_iou': []}\n",
        "\n",
        "    # Epoch Loop\n",
        "    for epoch in range(start_epoch, config['num_epochs']):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        train_loss, _ = run_epoch(model, train_loader, optimizer, True, device, config, scaler)\n",
        "        val_loss, val_iou = run_epoch(model, val_loader, None, False, device, config, scaler)\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if is_main_process():\n",
        "            print(f\"Epoch {epoch+1}/{config['num_epochs']} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val IoU: {val_iou:.4f}\")\n",
        "            \n",
        "            metrics_history['train_loss'].append(train_loss)\n",
        "            metrics_history['val_loss'].append(val_loss)\n",
        "            metrics_history['val_iou'].append(val_iou)\n",
        "            \n",
        "            # Save Metrics\n",
        "            with open(config['metrics_save_path'], 'w') as f:\n",
        "                json.dump(metrics_history, f)\n",
        "\n",
        "            # Save Checkpoint\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.module.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, config['latest_checkpoint_save_path'])\n",
        "\n",
        "            # Save Best Model\n",
        "            if val_iou > best_iou:\n",
        "                best_iou = val_iou\n",
        "                torch.save(model.module.state_dict(), config['best_model_save_path'])\n",
        "                print(f\"üèÜ New Best Model Saved (IoU: {best_iou:.4f})\")\n",
        "\n",
        "    cleanup_ddp()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "    with open(args.config, 'r') as f:\n",
        "        main(json.load(f))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08ae3127",
      "metadata": {},
      "source": [
        "### Phase 4: Launch Multi-GPU Training\n",
        "This uses `torchrun` to spawn one process per GPU on the node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8a7ad2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Execute Distributed Training\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"\\nüöÄ Launching training on {num_gpus} GPUs...\")\n",
        "\n",
        "!torchrun --nproc_per_node={num_gpus} train_dgx.py --config config.json\n",
        "\n",
        "print(\"\\n‚úÖ Training pipeline finished. Check 'results/' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84203282",
      "metadata": {},
      "source": [
        "### Phase 5: Visualization & Final Reporting\n",
        "Since training ran in a subprocess, we load the saved JSON metrics to visualize performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f84f5919",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Visualize Results\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "metrics_path = os.path.join(os.getcwd(), 'results', 'training_log.json')\n",
        "\n",
        "if os.path.exists(metrics_path):\n",
        "    with open(metrics_path, 'r') as f:\n",
        "        history = json.load(f)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Loss Plot\n",
        "    ax1.plot(history['train_loss'], label='Train Loss', color='blue')\n",
        "    ax1.plot(history['val_loss'], label='Val Loss', color='red')\n",
        "    ax1.set_title('Training vs Validation Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # IoU Plot\n",
        "    ax2.plot(history['val_iou'], label='Validation IoU', color='green')\n",
        "    ax2.set_title('Mean IoU Performance')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('IoU Score')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.show()\n",
        "    print(f\"üèÜ Peak Validation IoU: {max(history['val_iou']):.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Metrics file not found. Did training complete successfully?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
