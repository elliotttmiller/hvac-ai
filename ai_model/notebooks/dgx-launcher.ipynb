# train_dgx.py - DDP Optimized Training Script

import os, json, random, argparse, time, shutil
import numpy as np, cv2, torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.cuda.amp import autocast, GradScaler
from pathlib import Path
from typing import Dict, List, Tuple
from tqdm import tqdm
from statistics import mean
from pycocotools.coco import COCO
from pycocotools import mask as mask_utils
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from segment_anything import sam_model_registry
from segment_anything.utils.transforms import ResizeLongestSide

def setup_ddp():
    dist.init_process_group("nccl")
    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))

def cleanup_ddp():
    dist.destroy_process_group()

def is_main_process():
    return dist.get_rank() == 0

# (The HvacSamDataset and custom_collate_fn are pasted here, unchanged)
# ...

def combined_loss(pred_masks, true_masks):
    bce_loss = nn.BCEWithLogitsLoss()(pred_masks, true_masks.float())
    pred_flat = torch.sigmoid(pred_masks).reshape(-1)
    true_flat = true_masks.reshape(-1)
    intersection = (pred_flat * true_flat).sum()
    dice_loss = 1 - (2. * intersection + 1e-8) / (pred_flat.sum() + true_flat.sum() + 1e-8)
    return 0.8 * bce_loss + 0.2 * dice_loss

def main(config):
    local_rank = int(os.environ["LOCAL_RANK"])
    setup_ddp()

    # ... (Data loading logic using COCO) ...
    
    train_sampler = DistributedSampler(train_dataset, shuffle=True)
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], sampler=train_sampler, num_workers=config['num_workers'], collate_fn=..., pin_memory=True, persistent_workers=True)
    
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size']*2, ...) if is_main_process() else None

    model = sam_model_registry[config['model_type']]().to(local_rank)
    # ... (Resume/start fresh logic) ...
    model = DDP(model, device_ids=[local_rank])

    optimizer = optim.Adam(model.module.mask_decoder.parameters(), lr=config['learning_rate'], ...)
    scaler = GradScaler(enabled=config['use_amp'])
    
    if is_main_process(): print(f"Starting training for {config['num_epochs']} epochs...")

    for epoch in range(start_epoch, config['num_epochs']):
        train_loader.sampler.set_epoch(epoch)
        model.train()
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} Train", disable=(not is_main_process()))
        for i, batch in enumerate(pbar):
            with autocast(enabled=config['use_amp']):
                # ... (Your forward pass logic) ...
                loss = ... / config['accumulation_steps']
            scaler.scale(loss).backward()

            if (i + 1) % config['accumulation_steps'] == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
        
        if is_main_process():
            # ... (Validation loop) ...
            # ... (Checkpointing) ...
            print(f"Epoch {epoch+1} complete. Val IoU: {...}")

    cleanup_ddp()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, required=True)
    args = parser.parse_args()
    with open(args.config, 'r') as f: config = json.load(f)
    main(config)