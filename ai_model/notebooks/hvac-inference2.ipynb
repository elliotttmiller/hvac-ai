{
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "# üöÄ HVAC AI ‚Äî Inference Server (Final, v2)\n",
        "\n",
        "This notebook provides a robust, one-click solution for deploying the HVAC AI backend on a Google Colab GPU. It clones the repository, installs dependencies, and correctly launches the FastAPI server with a public `ngrok` URL.\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the cells in order.\n",
        "2. When you reach **Cell 4**, edit it to add your `NGROK_AUTHTOKEN` and verify the `MODEL_PATH`.\n",
        "3. Run the remaining cells to launch the server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1_header"
      },
      "source": [
        "### Phase 1: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo_cell"
      },
      "outputs": [],
      "source": [
        "# Cell 1 ‚Äî Clone repo and change to project root\n",
        "!git clone https://github.com/elliotttmiller/hvac-ai.git || true\n",
        "%cd hvac-ai\n",
        "print('‚úÖ Repo ready at /content/hvac-ai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps_cell"
      },
      "outputs": [],
      "source": [
        "# Cell 2 ‚Äî Install dependencies\n",
        "!pip install -q fastapi uvicorn python-multipart flask flask-cors pyngrok python-dotenv nest-asyncio\n",
        "!pip install -q torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118 || true\n",
        "!pip install -q opencv-python pycocotools matplotlib onnxruntime onnx || true\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git || true\n",
        "print('‚úÖ Dependencies installation attempted.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive_cell"
      },
      "outputs": [],
      "source": [
        "# Cell 3 ‚Äî Mount Google Drive\n",
        "from google.colab import drive\n",
        "print('Mounting Google Drive to access the fine-tuned model...')\n",
        "drive.mount('/content/drive')\n",
        "print('‚úÖ Drive mounted.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_env_cell"
      },
      "outputs": [],
      "source": [
        "%%writefile .env\n",
        "# ‚ö†Ô∏è ACTION REQUIRED: Edit these values before running the launch cell\n",
        "\n",
        "# 1. Get your token from https://dashboard.ngrok.com/get-started/your-authtoken and paste it here\n",
        "NGROK_AUTHTOKEN=\"REPLACE_WITH_YOUR_NGROK_AUTHTOKEN\"\n",
        "\n",
        "# 2. Verify this path points to your fine-tuned model on Google Drive\n",
        "MODEL_PATH=\"/content/drive/MyDrive/sam_finetuning_results/latest_checkpoint_multiprompt_v1.pth\"\n",
        "\n",
        "# 3. The port for the server. This must match the port in the launch cell.\n",
        "PORT=8000\n",
        "print('‚úÖ .env file created. Please edit the placeholder values before proceeding.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngrok_auth_cell"
      },
      "outputs": [],
      "source": [
        "# Cell 5 ‚Äî Authenticate ngrok (reads from .env)\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "ngrok_token = os.getenv('NGROK_AUTHTOKEN')\n",
        "if not ngrok_token or 'REPLACE_WITH' in ngrok_token:\n",
        " raise SystemExit('‚ùå CRITICAL: NGROK_AUTHTOKEN is missing or is still a placeholder. Please edit the .env cell above and rerun it.')\n",
        "print('Authenticating ngrok...')\n",
        "!ngrok authtoken {ngrok_token}\n",
        "print('‚úÖ ngrok authtoken set successfully.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_server_cell"
      },
      "outputs": [],
      "source": [
        "# Cell 6 ‚Äî Launch the Server\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "PORT = int(os.getenv('PORT', '8000'))\n",
        "\n",
        "# Ensure any stale tunnels are closed for a clean start\n",
        "ngrok.kill()\n",
        "\n",
        "# Start the ngrok tunnel\n",
        "public_url = ngrok.connect(PORT).public_url\n",
        "print('-' * 60)\n",
        "print(f'‚úÖ Your inference server is live at: {public_url}')\n",
        "print(f'üëâ Use this URL for the NEXT_PUBLIC_AI_SERVICE_URL in your frontend .env.local file.')\n",
        "print('-' * 60)\n",
        "\n",
        "print('\\nüöÄ Starting FastAPI backend... This cell will run continuously and show live server logs.')\n",
        "\n",
        "# Change directory into the services folder so the module import is valid\n",
        "%cd python-services\n",
        "\n",
        "# Run uvicorn using the correct module:app syntax. This blocks the cell.\n",
        "!uvicorn hvac_analysis_service:app --host 0.0.0.0 --port {PORT} --reload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting_markdown"
      },
      "source": [
        "### Notes & Troubleshooting\n",
        "- **`FileNotFoundError` on startup?** The most likely cause is an incorrect `MODEL_PATH` in your `.env` file. Double-check the path in your Google Drive.\n",
        "- **`ERR_NGROK_8012` (Connection Refused)?** This notebook is designed to prevent this by using the same `PORT` variable for both `ngrok` and `uvicorn`. If it still occurs, it means the `uvicorn` server crashed. Check the cell output for Python errors.\n",
        "- **Frontend Connection:** After the server starts and you see the public URL, update your frontend's `.env.local` file, then **restart your Next.js dev server** (`npm run dev`) for the change to take effect."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  }
}