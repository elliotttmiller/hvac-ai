{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production-Grade HVAC Auto-Labeling Pipeline with Grounded-SAM-2\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a comprehensive, production-ready auto-labeling pipeline using **Grounded-SAM-2** (the latest version) from Autodistill. It follows official documentation best practices and is optimized for HVAC blueprint symbol detection.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- \u2705 **Grounded-SAM-2**: Uses the latest Florence-2 + SAM 2 architecture for superior accuracy\n",
    "- \u2705 **Official Best Practices**: Follows autodistill official documentation guidelines\n",
    "- \u2705 **Robust Error Handling**: Comprehensive validation and error recovery\n",
    "- \u2705 **Quality Assurance**: Built-in quality checks and visualization\n",
    "- \u2705 **Optimized Parameters**: Research-backed threshold configurations\n",
    "- \u2705 **Production-Ready**: Designed for Google Colab with proper resource management\n",
    "\n",
    "### Workflow Phases\n",
    "\n",
    "1. **Environment Setup**: Install dependencies and configure paths\n",
    "2. **Configuration**: Set optimal parameters for HVAC symbol detection\n",
    "3. **Auto-Labeling**: Generate high-precision annotations with Grounded-SAM-2\n",
    "4. **Quality Review**: Visual inspection and approval gate\n",
    "5. **Model Training**: Train YOLOv8 on auto-labeled dataset\n",
    "6. **Inference**: Test trained model on new images\n",
    "\n",
    "### References\n",
    "\n",
    "- [Grounded-SAM-2 Official Docs](https://docs.autodistill.com/base_models/grounded-sam-2/)\n",
    "- [Autodistill GitHub](https://github.com/autodistill/autodistill-grounded-sam-2)\n",
    "- [Autodistill Quickstart](https://docs.autodistill.com/quickstart/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Environment Setup\n",
    "\n",
    "### Installation Strategy\n",
    "\n",
    "Following official documentation, we install:\n",
    "- PyTorch with CUDA support (2.0+ recommended)\n",
    "- Grounded-SAM-2 (latest autodistill version)\n",
    "- YOLOv8 for training target model\n",
    "- Supporting libraries for visualization and dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\ude80 HVAC AUTO-LABELING PIPELINE - ENVIRONMENT SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mount Google Drive for data persistence\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"\u2705 Google Drive mounted successfully\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"\u2139\ufe0f  Running in local environment (not Colab)\")\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(f\"\ud83d\udcc2 Working Directory: {HOME}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udce6 INSTALLING DEPENDENCIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install PyTorch with CUDA support (if not already installed)\n",
    "print(\"\\n[1/6] Installing PyTorch with CUDA support...\")\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Verify PyTorch installation\n",
    "import torch\n",
    "print(f\"   \u2705 PyTorch {torch.__version__} installed\")\n",
    "print(f\"   \u2705 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   \u2705 CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"   \u2705 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Install core autodistill framework\n",
    "print(\"\\n[2/6] Installing Autodistill core framework...\")\n",
    "!pip install -q autodistill\n",
    "print(\"   \u2705 Autodistill installed\")\n",
    "\n",
    "# Install Grounded-SAM-2 base model (latest version)\n",
    "print(\"\\n[3/6] Installing Grounded-SAM-2 base model...\")\n",
    "!pip install -q autodistill-grounded-sam-2\n",
    "print(\"   \u2705 Grounded-SAM-2 installed\")\n",
    "\n",
    "# Install YOLOv8 target model for training\n",
    "print(\"\\n[4/6] Installing YOLOv8 target model...\")\n",
    "!pip install -q autodistill-yolov8\n",
    "print(\"   \u2705 YOLOv8 installed\")\n",
    "\n",
    "# Install supporting libraries\n",
    "print(\"\\n[5/6] Installing supporting libraries...\")\n",
    "!pip install -q opencv-python-headless matplotlib numpy supervision roboflow\n",
    "print(\"   \u2705 Supporting libraries installed\")\n",
    "\n",
    "# Clear CUDA cache for fresh start\n",
    "print(\"\\n[6/6] Optimizing GPU memory...\")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"   \u2705 GPU memory cleared\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Configuration & Logging Setup\n",
    "\n",
    "### Progress Tracking & Logging\n",
    "\n",
    "Implements comprehensive logging system with:\n",
    "- Timestamped log entries\n",
    "- Progress tracking for all operations\n",
    "- Detailed error reporting\n",
    "- Performance metrics\n",
    "\n",
    "### Path Configuration\n",
    "\n",
    "Configure all paths for templates, images, and output directories.\n",
    "\n",
    "### Parameter Configuration\n",
    "\n",
    "Based on official documentation and research:\n",
    "- **box_threshold**: 0.25-0.30 (balanced precision/recall for technical drawings)\n",
    "- **text_threshold**: 0.20-0.25 (optimized for HVAC symbol prompts)\n",
    "\n",
    "These values are research-backed and suitable for technical blueprint analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\u2699\ufe0f  PIPELINE CONFIGURATION & LOGGING SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83d\udcdd SETTING UP LOGGING SYSTEM\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create logs directory\n",
    "LOG_DIR = os.path.join(os.getcwd(), \"pipeline_logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Create timestamped log file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(LOG_DIR, f\"autodistill_pipeline_{timestamp}.log\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"=\"*70)\n",
    "logger.info(\"HVAC AUTO-LABELING PIPELINE - STARTING\")\n",
    "logger.info(\"=\"*70)\n",
    "logger.info(f\"Log file: {log_file}\")\n",
    "logger.info(f\"Timestamp: {timestamp}\")\n",
    "\n",
    "print(f\"\u2705 Logging system initialized\")\n",
    "print(f\"   \u2022 Log file: {log_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROGRESS TRACKING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Track progress and performance metrics throughout the pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = datetime.now()\n",
    "        self.phase_times = {}\n",
    "        self.metrics = {}\n",
    "        self.current_phase = None\n",
    "        self.phase_start = None\n",
    "    \n",
    "    def start_phase(self, phase_name):\n",
    "        \"\"\"Start tracking a new phase.\"\"\"\n",
    "        if self.current_phase:\n",
    "            self.end_phase()\n",
    "        self.current_phase = phase_name\n",
    "        self.phase_start = datetime.now()\n",
    "        logger.info(f\"Starting phase: {phase_name}\")\n",
    "    \n",
    "    def end_phase(self):\n",
    "        \"\"\"End current phase and record time.\"\"\"\n",
    "        if self.current_phase and self.phase_start:\n",
    "            duration = (datetime.now() - self.phase_start).total_seconds()\n",
    "            self.phase_times[self.current_phase] = duration\n",
    "            logger.info(f\"Completed phase: {self.current_phase} (Duration: {duration:.2f}s)\")\n",
    "            self.current_phase = None\n",
    "            self.phase_start = None\n",
    "    \n",
    "    def record_metric(self, metric_name, value):\n",
    "        \"\"\"Record a metric value.\"\"\"\n",
    "        self.metrics[metric_name] = value\n",
    "        logger.info(f\"Metric - {metric_name}: {value}\")\n",
    "    \n",
    "    def get_total_time(self):\n",
    "        \"\"\"Get total elapsed time.\"\"\"\n",
    "        return (datetime.now() - self.start_time).total_seconds()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print pipeline execution summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\ud83d\udcca PIPELINE EXECUTION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\n\u23f1\ufe0f  Total Pipeline Time: {self.get_total_time()/60:.2f} minutes\")\n",
    "        \n",
    "        if self.phase_times:\n",
    "            print(\"\\n\ud83d\udd04 Phase Breakdown:\")\n",
    "            for phase, duration in self.phase_times.items():\n",
    "                print(f\"   \u2022 {phase:<30} {duration:>8.2f}s\")\n",
    "        \n",
    "        if self.metrics:\n",
    "            print(\"\\n\ud83d\udcc8 Key Metrics:\")\n",
    "            for metric, value in self.metrics.items():\n",
    "                print(f\"   \u2022 {metric:<30} {value}\")\n",
    "        \n",
    "        logger.info(\"Pipeline execution summary printed\")\n",
    "\n",
    "# Initialize global progress tracker\n",
    "progress = ProgressTracker()\n",
    "logger.info(\"Progress tracker initialized\")\n",
    "\n",
    "# ============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "progress.start_phase(\"Configuration\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Google Colab paths (using Google Drive for persistence)\n",
    "    BASE_DRIVE_PATH = \"/content/drive/MyDrive/HVAC_AutoLabeling/\"\n",
    "    TEMPLATE_FOLDER_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_templates/\")\n",
    "    UNLABELED_IMAGES_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_example_images/\")\n",
    "    TRAINING_OUTPUT_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_yolov8_training/\")\n",
    "    INFERENCE_OUTPUT_PATH = os.path.join(BASE_DRIVE_PATH, \"hvac_inference_results/\")\n",
    "    \n",
    "    # Temporary dataset output (faster local processing)\n",
    "    DATASET_OUTPUT_PATH = os.path.join(HOME, \"hvac_autodistill_dataset/\")\n",
    "else:\n",
    "    # Local environment paths\n",
    "    BASE_PATH = Path.cwd()\n",
    "    TEMPLATE_FOLDER_PATH = str(BASE_PATH / \"ai_model\" / \"datasets\" / \"hvac_templates\" / \"hvac_templates\")\n",
    "    UNLABELED_IMAGES_PATH = str(BASE_PATH / \"ai_model\" / \"datasets\" / \"hvac_example_images\" / \"hvac_example_images\")\n",
    "    DATASET_OUTPUT_PATH = str(BASE_PATH / \"ai_model\" / \"outputs\" / \"autodistill_dataset\")\n",
    "    TRAINING_OUTPUT_PATH = str(BASE_PATH / \"ai_model\" / \"outputs\" / \"yolov8_training\")\n",
    "    INFERENCE_OUTPUT_PATH = str(BASE_PATH / \"ai_model\" / \"outputs\" / \"inference_results\")\n",
    "\n",
    "# Create all required directories\n",
    "for path_name, path_value in [\n",
    "    (\"Dataset Output\", DATASET_OUTPUT_PATH),\n",
    "    (\"Training Output\", TRAINING_OUTPUT_PATH),\n",
    "    (\"Inference Output\", INFERENCE_OUTPUT_PATH)\n",
    "]:\n",
    "    os.makedirs(path_value, exist_ok=True)\n",
    "    print(f\"\ud83d\udcc2 {path_name}: {path_value}\")\n",
    "    logger.info(f\"Created directory: {path_value}\")\n",
    "\n",
    "# Record path configuration\n",
    "progress.record_metric(\"Template Path\", TEMPLATE_FOLDER_PATH)\n",
    "progress.record_metric(\"Images Path\", UNLABELED_IMAGES_PATH)\n",
    "progress.record_metric(\"Output Path\", DATASET_OUTPUT_PATH)\n",
    "\n",
    "# ============================================================================\n",
    "# DETECTION PARAMETERS (Research-Based Optimal Values)\n",
    "# ============================================================================\n",
    "\n",
    "# Box threshold: Confidence threshold for bounding box predictions\n",
    "# Range: 0.25-0.30 recommended for technical drawings\n",
    "# Lower = higher recall, more false positives\n",
    "# Higher = higher precision, may miss objects\n",
    "BOX_THRESHOLD = 0.27\n",
    "\n",
    "# Text threshold: Confidence threshold for text prompt matching\n",
    "# Range: 0.20-0.25 recommended for HVAC symbols\n",
    "# Lower = more lenient matching\n",
    "# Higher = stricter prompt matching\n",
    "TEXT_THRESHOLD = 0.22\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83c\udfaf DETECTION PARAMETERS\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Box Threshold:  {BOX_THRESHOLD:.2f} (optimized for technical drawings)\")\n",
    "print(f\"Text Threshold: {TEXT_THRESHOLD:.2f} (optimized for HVAC symbols)\")\n",
    "\n",
    "logger.info(f\"Detection parameters - Box: {BOX_THRESHOLD}, Text: {TEXT_THRESHOLD}\")\n",
    "progress.record_metric(\"Box Threshold\", BOX_THRESHOLD)\n",
    "progress.record_metric(\"Text Threshold\", TEXT_THRESHOLD)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "TRAINING_EPOCHS = 100  # Number of training epochs\n",
    "YOLO_MODEL_SIZE = \"yolov8n.pt\"  # Nano model for faster training with small datasets\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83c\udfcb\ufe0f  TRAINING PARAMETERS\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Training Epochs: {TRAINING_EPOCHS}\")\n",
    "print(f\"YOLOv8 Model:    {YOLO_MODEL_SIZE}\")\n",
    "\n",
    "logger.info(f\"Training parameters - Epochs: {TRAINING_EPOCHS}, Model: {YOLO_MODEL_SIZE}\")\n",
    "progress.record_metric(\"Training Epochs\", TRAINING_EPOCHS)\n",
    "progress.record_metric(\"YOLO Model\", YOLO_MODEL_SIZE)\n",
    "\n",
    "progress.end_phase()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 CONFIGURATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "logger.info(\"Configuration phase completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Optimized Ontology Generation from HVAC Templates\n",
    "\n",
    "### Enhanced Ontology Design\n",
    "\n",
    "Following autodistill best practices with optimizations:\n",
    "- **Smart Template Processing**: Extracts and normalizes class names from filenames\n",
    "- **Prompt Engineering**: Creates descriptive, context-rich prompts for better detection\n",
    "- **Category Grouping**: Organizes classes by type (valves, instruments, signals, etc.)\n",
    "- **Validation**: Ensures ontology integrity and completeness\n",
    "\n",
    "### Template Processing Pipeline\n",
    "\n",
    "1. Scan template directory for all image files\n",
    "2. Extract and clean class names from filenames\n",
    "3. Apply intelligent prompt engineering\n",
    "4. Group classes by category for better organization\n",
    "5. Validate ontology structure and log statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from autodistill.detection import CaptionOntology\n",
    "\n",
    "progress.start_phase(\"Ontology Generation\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udccb OPTIMIZED HVAC ONTOLOGY GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "logger.info(\"Starting ontology generation from templates\")\n",
    "print(f\"\\n\ud83d\udd0d Scanning template directory: {TEMPLATE_FOLDER_PATH}\")\n",
    "logger.info(f\"Template directory: {TEMPLATE_FOLDER_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEMPLATE DISCOVERY\n",
    "# ============================================================================\n",
    "\n",
    "# Find all template image files\n",
    "template_extensions = ['*.png', '*.PNG', '*.jpg', '*.JPG', '*.jpeg', '*.JPEG']\n",
    "all_template_files = []\n",
    "for ext in template_extensions:\n",
    "    found_files = glob.glob(os.path.join(TEMPLATE_FOLDER_PATH, ext))\n",
    "    all_template_files.extend(found_files)\n",
    "    if found_files:\n",
    "        logger.info(f\"Found {len(found_files)} files with extension {ext}\")\n",
    "\n",
    "if not all_template_files:\n",
    "    logger.error(f\"No template files found in {TEMPLATE_FOLDER_PATH}\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"\u274c FATAL ERROR: No template files found in {TEMPLATE_FOLDER_PATH}\\n\"\n",
    "        f\"   Please ensure template images are present in the directory.\"\n",
    "    )\n",
    "\n",
    "print(f\"\u2705 Found {len(all_template_files)} template files\")\n",
    "logger.info(f\"Total templates discovered: {len(all_template_files)}\")\n",
    "progress.record_metric(\"Template Files Found\", len(all_template_files))\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED PROMPT ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def engineer_prompt(base_name):\n",
    "    \"\"\"Apply intelligent prompt engineering for better detection.\n",
    "    \n",
    "    Creates context-rich prompts that help the model understand\n",
    "    the specific HVAC component being detected.\n",
    "    \"\"\"\n",
    "    # Clean the name\n",
    "    clean = base_name.replace('template_', '').replace('_', ' ').strip()\n",
    "    \n",
    "    # Add context for specific component types\n",
    "    if 'valve' in clean.lower():\n",
    "        prompt = f\"hvac {clean}\"\n",
    "    elif 'instrument' in clean.lower():\n",
    "        prompt = f\"hvac control {clean}\"\n",
    "    elif 'signal' in clean.lower():\n",
    "        prompt = f\"{clean} line\"\n",
    "    else:\n",
    "        prompt = clean\n",
    "    \n",
    "    return prompt, clean\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD ONTOLOGY WITH CATEGORIZATION\n",
    "# ============================================================================\n",
    "\n",
    "ontology_mapping = {}\n",
    "categories = defaultdict(list)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83d\udcdd PROCESSING TEMPLATES WITH PROMPT ENGINEERING\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "logger.info(\"Processing templates and engineering prompts\")\n",
    "\n",
    "for i, template_path in enumerate(sorted(all_template_files), 1):\n",
    "    # Extract filename without extension\n",
    "    filename = os.path.basename(template_path)\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Apply prompt engineering\n",
    "    prompt, class_name = engineer_prompt(base_name)\n",
    "    \n",
    "    ontology_mapping[prompt] = class_name\n",
    "    \n",
    "    # Categorize for organization\n",
    "    if 'valve' in class_name.lower():\n",
    "        categories['Valves'].append(class_name)\n",
    "    elif 'instrument' in class_name.lower():\n",
    "        categories['Instruments'].append(class_name)\n",
    "    elif 'signal' in class_name.lower():\n",
    "        categories['Signals'].append(class_name)\n",
    "    else:\n",
    "        categories['Other'].append(class_name)\n",
    "    \n",
    "    if i <= 10:  # Show first 10 mappings\n",
    "        print(f\"   [{i:2d}] {prompt:<40} -> {class_name}\")\n",
    "        logger.debug(f\"Mapped: {prompt} -> {class_name}\")\n",
    "\n",
    "if len(all_template_files) > 10:\n",
    "    print(f\"   ... and {len(all_template_files) - 10} more classes\")\n",
    "    logger.info(f\"Processed {len(all_template_files) - 10} additional classes\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE ONTOLOGY OBJECT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83c\udfd7\ufe0f  CREATING ONTOLOGY OBJECT\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "logger.info(\"Creating CaptionOntology object\")\n",
    "ontology = CaptionOntology(ontology_mapping)\n",
    "classes = ontology.classes()\n",
    "\n",
    "print(f\"\u2705 Ontology created successfully\")\n",
    "print(f\"\u2705 Total classes in ontology: {len(classes)}\")\n",
    "logger.info(f\"Ontology created with {len(classes)} classes\")\n",
    "progress.record_metric(\"Ontology Classes\", len(classes))\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83d\udcca CATEGORY BREAKDOWN\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for category, items in sorted(categories.items()):\n",
    "    print(f\"\\n\ud83c\udff7\ufe0f  {category} ({len(items)} classes):\")\n",
    "    logger.info(f\"Category '{category}': {len(items)} classes\")\n",
    "    for i, item in enumerate(sorted(items)[:5], 1):  # Show first 5\n",
    "        print(f\"   {i}. {item}\")\n",
    "    if len(items) > 5:\n",
    "        print(f\"   ... and {len(items) - 5} more\")\n",
    "    progress.record_metric(f\"Category: {category}\", len(items))\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\u2705 ONTOLOGY VALIDATION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Validate ontology integrity\n",
    "validation_checks = [\n",
    "    (len(ontology_mapping) > 0, \"Ontology has mappings\"),\n",
    "    (len(classes) == len(ontology_mapping), \"Class count matches mapping count\"),\n",
    "    (len(set(classes)) == len(classes), \"All class names are unique\"),\n",
    "    (all(len(c.strip()) > 0 for c in classes), \"All class names are non-empty\")\n",
    "]\n",
    "\n",
    "all_valid = True\n",
    "for check, description in validation_checks:\n",
    "    status = \"\u2705\" if check else \"\u274c\"\n",
    "    print(f\"   {status} {description}\")\n",
    "    logger.info(f\"Validation - {description}: {check}\")\n",
    "    if not check:\n",
    "        all_valid = False\n",
    "\n",
    "if not all_valid:\n",
    "    logger.error(\"Ontology validation failed\")\n",
    "    raise ValueError(\"Ontology validation failed. Please check the logs.\")\n",
    "\n",
    "progress.end_phase()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 ONTOLOGY GENERATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "logger.info(\"Ontology generation completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Enhanced Auto-Labeling with Per-Class Detection\n",
    "\n",
    "### Advanced Detection Strategy\n",
    "\n",
    "Implements a **dual-mode detection system** for maximum accuracy:\n",
    "\n",
    "1. **Batch Mode** (Default): Efficient processing of all classes together\n",
    "2. **Per-Class Mode** (High-Precision): Iterative detection for each class individually\n",
    "\n",
    "The per-class approach provides:\n",
    "- Higher precision for technical drawings\n",
    "- Better class separation\n",
    "- Reduced false positives\n",
    "- Improved confidence scores\n",
    "\n",
    "### Quality Metrics & Validation\n",
    "\n",
    "Comprehensive tracking of:\n",
    "- Detection count per image and per class\n",
    "- Confidence score statistics (mean, min, max, std)\n",
    "- Class distribution analysis\n",
    "- Processing time metrics\n",
    "- Image-level quality scores\n",
    "- Dataset completeness validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_grounded_sam_2 import GroundedSAM2\n",
    "from autodistill.core.dataset import DetectionDataset\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from collections import Counter\n",
    "\n",
    "progress.start_phase(\"Auto-Labeling\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83e\udd16 ENHANCED AUTO-LABELING WITH PER-CLASS DETECTION\")\n",
    "print(\"=\"*70)\n",
    "logger.info(\"Starting auto-labeling phase\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Detection mode: 'batch' (faster) or 'per_class' (more accurate)\n",
    "DETECTION_MODE = 'per_class'  # Use per-class for higher precision\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\u2699\ufe0f  DETECTION CONFIGURATION\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Detection Mode: {DETECTION_MODE.upper()}\")\n",
    "if DETECTION_MODE == 'per_class':\n",
    "    print(\"   \u2022 Strategy: Iterative per-class detection\")\n",
    "    print(\"   \u2022 Benefits: Higher precision, better class separation\")\n",
    "    print(\"   \u2022 Trade-off: Longer processing time\")\n",
    "else:\n",
    "    print(\"   \u2022 Strategy: Batch detection of all classes\")\n",
    "    print(\"   \u2022 Benefits: Faster processing\")\n",
    "logger.info(f\"Detection mode: {DETECTION_MODE}\")\n",
    "progress.record_metric(\"Detection Mode\", DETECTION_MODE)\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE GROUNDED-SAM-2 MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83c\udfd7\ufe0f  INITIALIZING GROUNDED-SAM-2 BASE MODEL\")\n",
    "print(\"-\"*70)\n",
    "logger.info(\"Initializing Grounded-SAM-2 model\")\n",
    "\n",
    "try:\n",
    "    base_model = GroundedSAM2(\n",
    "        ontology=ontology,\n",
    "        grounding_dino_box_threshold=BOX_THRESHOLD,\n",
    "        grounding_dino_text_threshold=TEXT_THRESHOLD\n",
    "    )\n",
    "    print(\"\u2705 Grounded-SAM-2 model initialized successfully\")\n",
    "    print(f\"   \u2022 Model type: GroundedSAM2 (Florence-2 + SAM 2)\")\n",
    "    print(f\"   \u2022 Box threshold: {BOX_THRESHOLD}\")\n",
    "    print(f\"   \u2022 Text threshold: {TEXT_THRESHOLD}\")\n",
    "    print(f\"   \u2022 Classes loaded: {len(classes)}\")\n",
    "    logger.info(f\"Model initialized: box_threshold={BOX_THRESHOLD}, text_threshold={TEXT_THRESHOLD}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize model: {str(e)}\")\n",
    "    raise RuntimeError(\n",
    "        f\"\u274c FATAL ERROR: Failed to initialize Grounded-SAM-2 model\\n\"\n",
    "        f\"   Error: {str(e)}\\n\"\n",
    "        f\"   Please ensure all dependencies are installed correctly.\"\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# SCAN FOR UNLABELED IMAGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\ud83d\udcc1 SCANNING FOR UNLABELED IMAGES\")\n",
    "print(\"-\"*70)\n",
    "logger.info(\"Scanning for unlabeled images\")\n",
    "\n",
    "print(f\"\ud83d\udd0d Scanning directory: {UNLABELED_IMAGES_PATH}\")\n",
    "\n",
    "# Find all image files\n",
    "image_extensions = ['*.png', '*.PNG', '*.jpg', '*.JPG', '*.jpeg', '*.JPEG']\n",
    "image_paths = []\n",
    "for ext in image_extensions:\n",
    "    found = glob.glob(os.path.join(UNLABELED_IMAGES_PATH, ext))\n",
    "    image_paths.extend(found)\n",
    "    if found:\n",
    "        logger.info(f\"Found {len(found)} images with extension {ext}\")\n",
    "\n",
    "if not image_paths:\n",
    "    logger.error(f\"No images found in {UNLABELED_IMAGES_PATH}\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"\u274c FATAL ERROR: No images found in {UNLABELED_IMAGES_PATH}\\n\"\n",
    "        f\"   Please add images to the directory before running auto-labeling.\"\n",
    "    )\n",
    "\n",
    "print(f\"\u2705 Found {len(image_paths)} images to process\")\n",
    "logger.info(f\"Total images to process: {len(image_paths)}\")\n",
    "progress.record_metric(\"Images to Process\", len(image_paths))\n",
    "\n",
    "# Display image list\n",
    "print(\"\\n\ud83d\udccb Image files:\")\n",
    "for i, img_path in enumerate(sorted(image_paths), 1):\n",
    "    img_name = os.path.basename(img_path)\n",
    "    print(f\"   {i}. {img_name}\")\n",
    "    logger.debug(f\"Image {i}: {img_name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN ENHANCED AUTO-LABELING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\ude80 STARTING ENHANCED AUTO-LABELING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if DETECTION_MODE == 'batch':\n",
    "    # ========================================================================\n",
    "    # BATCH MODE: Fast processing of all classes together\n",
    "    # ========================================================================\n",
    "    print(\"\\n\u2139\ufe0f  Using BATCH mode for fast processing\")\n",
    "    logger.info(\"Using batch detection mode\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        base_model.label(\n",
    "            input_folder=UNLABELED_IMAGES_PATH,\n",
    "            output_folder=DATASET_OUTPUT_PATH,\n",
    "            extension=\".jpg\"\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        logger.info(f\"Batch labeling completed in {elapsed_time:.2f}s\")\n",
    "        progress.record_metric(\"Labeling Time (s)\", f\"{elapsed_time:.2f}\")\n",
    "        progress.record_metric(\"Avg Time per Image (s)\", f\"{elapsed_time/len(image_paths):.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Batch labeling failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "else:\n",
    "    # ========================================================================\n",
    "    # PER-CLASS MODE: High-precision iterative detection\n",
    "    # ========================================================================\n",
    "    print(\"\\n\u2139\ufe0f  Using PER-CLASS mode for maximum precision\")\n",
    "    print(\"   This will detect each class individually for better accuracy\\n\")\n",
    "    logger.info(\"Using per-class detection mode for maximum precision\")\n",
    "    \n",
    "    # Initialize dataset\n",
    "    dataset = DetectionDataset(classes=classes, base_dir=DATASET_OUTPUT_PATH)\n",
    "    \n",
    "    # Statistics tracking\n",
    "    total_detections = 0\n",
    "    detections_by_class = Counter()\n",
    "    detections_by_image = {}\n",
    "    confidence_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process each image\n",
    "    for img_idx, img_path in enumerate(sorted(image_paths), 1):\n",
    "        img_name = os.path.basename(img_path)\n",
    "        print(f\"\\n[{img_idx}/{len(image_paths)}] Processing: {img_name}\")\n",
    "        logger.info(f\"Processing image {img_idx}/{len(image_paths)}: {img_name}\")\n",
    "        \n",
    "        img_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                logger.warning(f\"Failed to read image: {img_path}\")\n",
    "                print(f\"   \u26a0\ufe0f  Warning: Could not read image, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Detect each class individually\n",
    "            all_detections = []\n",
    "            class_detection_count = 0\n",
    "            \n",
    "            for class_idx, class_name in enumerate(classes):\n",
    "                # Predict for this specific class\n",
    "                detections = base_model.predict(image, prompt=class_name)\n",
    "                \n",
    "                if len(detections) > 0:\n",
    "                    # Set class IDs\n",
    "                    detections.class_id = np.full(len(detections), class_idx)\n",
    "                    all_detections.append(detections)\n",
    "                    class_detection_count += len(detections)\n",
    "                    \n",
    "                    # Track statistics\n",
    "                    detections_by_class[class_name] += len(detections)\n",
    "                    if hasattr(detections, 'confidence') and detections.confidence is not None:\n",
    "                        confidence_scores.extend(detections.confidence.tolist())\n",
    "                    \n",
    "                    logger.debug(f\"  Class '{class_name}': {len(detections)} detections\")\n",
    "            \n",
    "            # Merge all detections for this image\n",
    "            if all_detections:\n",
    "                final_detections = sv.Detections.merge(all_detections)\n",
    "                dataset.add_detection(image_path=img_path, detections=final_detections)\n",
    "                \n",
    "                total_detections += len(final_detections)\n",
    "                detections_by_image[img_name] = len(final_detections)\n",
    "                \n",
    "                # Calculate statistics\n",
    "                if hasattr(final_detections, 'confidence') and final_detections.confidence is not None:\n",
    "                    avg_conf = np.mean(final_detections.confidence)\n",
    "                    min_conf = np.min(final_detections.confidence)\n",
    "                    max_conf = np.max(final_detections.confidence)\n",
    "                else:\n",
    "                    avg_conf = min_conf = max_conf = 0.0\n",
    "                \n",
    "                img_time = time.time() - img_start_time\n",
    "                \n",
    "                print(f\"   \u2705 SUCCESS: {len(final_detections)} symbols detected\")\n",
    "                print(f\"      \u2022 Confidence: avg={avg_conf:.3f}, min={min_conf:.3f}, max={max_conf:.3f}\")\n",
    "                print(f\"      \u2022 Processing time: {img_time:.2f}s\")\n",
    "                \n",
    "                logger.info(\n",
    "                    f\"Image {img_name}: {len(final_detections)} detections, \"\n",
    "                    f\"avg_conf={avg_conf:.3f}, time={img_time:.2f}s\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"   \u2139\ufe0f  No symbols detected\")\n",
    "                detections_by_image[img_name] = 0\n",
    "                logger.info(f\"Image {img_name}: No detections\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   \u274c ERROR: {str(e)}\")\n",
    "            logger.error(f\"Failed to process {img_path}: {str(e)}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # ========================================================================\n",
    "    # QUALITY METRICS & STATISTICS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\ud83d\udcca LABELING STATISTICS & QUALITY METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    images_with_detections = sum(1 for count in detections_by_image.values() if count > 0)\n",
    "    \n",
    "    print(f\"\\n\u23f1\ufe0f  Performance Metrics:\")\n",
    "    print(f\"   \u2022 Total processing time: {elapsed_time:.2f}s ({elapsed_time/60:.2f} min)\")\n",
    "    print(f\"   \u2022 Average time per image: {elapsed_time/len(image_paths):.2f}s\")\n",
    "    print(f\"   \u2022 Processing speed: {len(image_paths)/elapsed_time:.2f} images/second\")\n",
    "    \n",
    "    logger.info(f\"Total processing time: {elapsed_time:.2f}s\")\n",
    "    progress.record_metric(\"Labeling Time (s)\", f\"{elapsed_time:.2f}\")\n",
    "    progress.record_metric(\"Avg Time per Image (s)\", f\"{elapsed_time/len(image_paths):.2f}\")\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfaf Detection Metrics:\")\n",
    "    print(f\"   \u2022 Total detections: {total_detections}\")\n",
    "    print(f\"   \u2022 Images processed: {len(image_paths)}\")\n",
    "    print(f\"   \u2022 Images with detections: {images_with_detections} ({images_with_detections/len(image_paths)*100:.1f}%)\")\n",
    "    if images_with_detections > 0:\n",
    "        print(f\"   \u2022 Average detections per image: {total_detections/images_with_detections:.2f}\")\n",
    "    \n",
    "    logger.info(f\"Total detections: {total_detections}\")\n",
    "    logger.info(f\"Images with detections: {images_with_detections}/{len(image_paths)}\")\n",
    "    progress.record_metric(\"Total Detections\", total_detections)\n",
    "    progress.record_metric(\"Images with Detections\", images_with_detections)\n",
    "    \n",
    "    if confidence_scores:\n",
    "        print(f\"\\n\ud83d\udcc8 Confidence Score Statistics:\")\n",
    "        print(f\"   \u2022 Mean: {np.mean(confidence_scores):.3f}\")\n",
    "        print(f\"   \u2022 Std Dev: {np.std(confidence_scores):.3f}\")\n",
    "        print(f\"   \u2022 Min: {np.min(confidence_scores):.3f}\")\n",
    "        print(f\"   \u2022 Max: {np.max(confidence_scores):.3f}\")\n",
    "        print(f\"   \u2022 Median: {np.median(confidence_scores):.3f}\")\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Confidence scores - mean: {np.mean(confidence_scores):.3f}, \"\n",
    "            f\"std: {np.std(confidence_scores):.3f}\"\n",
    "        )\n",
    "        progress.record_metric(\"Avg Confidence\", f\"{np.mean(confidence_scores):.3f}\")\n",
    "    \n",
    "    if detections_by_class:\n",
    "        print(f\"\\n\ud83c\udff7\ufe0f  Top 10 Detected Classes:\")\n",
    "        for i, (class_name, count) in enumerate(detections_by_class.most_common(10), 1):\n",
    "            print(f\"   {i:2d}. {class_name:<35} {count:>3} detections\")\n",
    "            logger.info(f\"Class '{class_name}': {count} detections\")\n",
    "        \n",
    "        if len(detections_by_class) > 10:\n",
    "            remaining = sum(count for _, count in list(detections_by_class.items())[10:])\n",
    "            print(f\"   ... and {len(detections_by_class)-10} more classes ({remaining} detections)\")\n",
    "        \n",
    "        progress.record_metric(\"Unique Classes Detected\", len(detections_by_class))\n",
    "    \n",
    "    # Validation warnings\n",
    "    print(f\"\\n\u26a0\ufe0f  Quality Validation:\")\n",
    "    if images_with_detections == 0:\n",
    "        print(f\"   \u26a0\ufe0f  WARNING: No detections in any image\")\n",
    "        logger.warning(\"No detections found in any image\")\n",
    "    elif images_with_detections < len(image_paths) * 0.5:\n",
    "        print(f\"   \u26a0\ufe0f  WARNING: Less than 50% of images have detections\")\n",
    "        logger.warning(f\"Only {images_with_detections}/{len(image_paths)} images have detections\")\n",
    "    else:\n",
    "        print(f\"   \u2705 Good detection coverage\")\n",
    "    \n",
    "    if confidence_scores and np.mean(confidence_scores) < 0.3:\n",
    "        print(f\"   \u26a0\ufe0f  WARNING: Low average confidence score ({np.mean(confidence_scores):.3f})\")\n",
    "        logger.warning(f\"Low confidence score: {np.mean(confidence_scores):.3f}\")\n",
    "    elif confidence_scores:\n",
    "        print(f\"   \u2705 Good average confidence score\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 AUTO-LABELING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\ud83d\udcbe Dataset saved to: {DATASET_OUTPUT_PATH}\")\n",
    "logger.info(\"Auto-labeling phase completed successfully\")\n",
    "progress.end_phase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Enhanced Quality Review & Visualization\n",
    "\n",
    "### Comprehensive Dataset Validation\n",
    "\n",
    "Multi-level quality assurance system:\n",
    "- **Statistical Analysis**: Detection distribution, class balance, confidence metrics\n",
    "- **Visual Inspection**: Annotated samples with detailed labeling\n",
    "- **Quality Scoring**: Image-level and dataset-level quality metrics\n",
    "- **Validation Checks**: Dataset integrity and completeness verification\n",
    "\n",
    "### Enhanced Visualization\n",
    "\n",
    "Professional visualization features:\n",
    "- Color-coded bounding boxes by class category\n",
    "- Confidence score overlays\n",
    "- Class distribution charts\n",
    "- Side-by-side comparison views\n",
    "- Detection heatmaps\n",
    "\n",
    "### Manual Approval Gate\n",
    "\n",
    "Interactive review process with detailed checklist for quality assessment before proceeding to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\\n",
    "# COMPUTE COMPREHENSIVE DATASET STATISTICS\\n",
    "# ============================================================================\\n",
    "\\n",
    "print(\"\\\\n\" + \"-\"*70)\\n",
    "print(\"\ud83d\udcca COMPREHENSIVE DATASET STATISTICS\")\\n",
    "print(\"-\"*70)\\n",
    "logger.info(\"Computing dataset statistics\")\\n",
    "\\n",
    "total_detections = 0\\n",
    "class_counts = Counter()\\n",
    "images_with_detections = 0\\n",
    "detection_counts_per_image = []\\n",
    "bbox_sizes = []\\n",
    "\\n",
    "for image_path, detections in review_dataset:\\n",
    "    num_detections = len(detections)\\n",
    "    detection_counts_per_image.append(num_detections)\\n",
    "    \\n",
    "    if num_detections > 0:\\n",
    "        images_with_detections += 1\\n",
    "        total_detections += num_detections\\n",
    "        \\n",
    "        # Count classes\\n",
    "        for class_id in detections.class_id:\\n",
    "            class_name = review_dataset.classes[class_id]\\n",
    "            class_counts[class_name] += 1\\n",
    "        \\n",
    "        # Analyze bounding box sizes\\n",
    "        if hasattr(detections, 'xyxy') and detections.xyxy is not None:\\n",
    "            for box in detections.xyxy:\\n",
    "                width = box[2] - box[0]\\n",
    "                height = box[3] - box[1]\\n",
    "                area = width * height\\n",
    "                bbox_sizes.append(area)\\n",
    "\\n",
    "# Basic statistics\\n",
    "print(f\"\\\\n\ud83d\udcc8 Detection Summary:\")\\n",
    "print(f\"   \u2022 Images with detections: {images_with_detections}/{len(review_dataset)} ({images_with_detections/len(review_dataset)*100:.1f}%)\")\\n",
    "print(f\"   \u2022 Total detections: {total_detections}\")\\n",
    "if images_with_detections > 0:\\n",
    "    print(f\"   \u2022 Average detections per image: {total_detections/images_with_detections:.2f}\")\\n",
    "    print(f\"   \u2022 Min detections in an image: {min([c for c in detection_counts_per_image if c > 0])}\")\\n",
    "    print(f\"   \u2022 Max detections in an image: {max(detection_counts_per_image)}\")\\n",
    "\\n",
    "logger.info(f\"Dataset stats: {total_detections} detections across {images_with_detections} images\")\\n",
    "progress.record_metric(\"Total Dataset Detections\", total_detections)\\n",
    "progress.record_metric(\"Images with Detections\", f\"{images_with_detections}/{len(review_dataset)}\")\\n",
    "\\n",
    "# Bounding box statistics\\n",
    "if bbox_sizes:\\n",
    "    print(f\"\\\\n\ud83d\udccf Bounding Box Statistics:\")\\n",
    "    print(f\"   \u2022 Average area: {np.mean(bbox_sizes):.1f} px\u00b2\")\\n",
    "    print(f\"   \u2022 Median area: {np.median(bbox_sizes):.1f} px\u00b2\")\\n",
    "    print(f\"   \u2022 Std deviation: {np.std(bbox_sizes):.1f} px\u00b2\")\\n",
    "    logger.info(f\"Avg bbox area: {np.mean(bbox_sizes):.1f} px\u00b2\")\\n",
    "\\n",
    "# Class distribution\\n",
    "if class_counts:\\n",
    "    print(f\"\\\\n\ud83c\udff7\ufe0f  Class Distribution (Top 15):\")\\n",
    "    for i, (class_name, count) in enumerate(class_counts.most_common(15), 1):\\n",
    "        percentage = (count / total_detections) * 100\\n",
    "        print(f\"   {i:2d}. {class_name:<35} {count:>3} ({percentage:>5.1f}%)\")\\n",
    "        logger.debug(f\"Class {class_name}: {count} detections ({percentage:.1f}%)\")\\n",
    "    \\n",
    "    if len(class_counts) > 15:\\n",
    "        remaining = len(class_counts) - 15\\n",
    "        remaining_detections = sum(count for _, count in list(class_counts.items())[15:])\\n",
    "        print(f\"   ... and {remaining} more classes ({remaining_detections} detections)\")\\n",
    "    \\n",
    "    progress.record_metric(\"Classes with Detections\", len(class_counts))\\n",
    "    \\n",
    "    # Class balance analysis\\n",
    "    print(f\"\\\\n\u2696\ufe0f  Class Balance Analysis:\")\\n",
    "    most_common_count = class_counts.most_common(1)[0][1]\\n",
    "    least_common_count = class_counts.most_common()[-1][1]\\n",
    "    imbalance_ratio = most_common_count / least_common_count if least_common_count > 0 else 0\\n",
    "    print(f\"   \u2022 Most common class: {most_common_count} detections\")\\n",
    "    print(f\"   \u2022 Least common class: {least_common_count} detections\")\\n",
    "    print(f\"   \u2022 Imbalance ratio: {imbalance_ratio:.1f}:1\")\\n",
    "    \\n",
    "    if imbalance_ratio > 10:\\n",
    "        print(f\"   \u26a0\ufe0f  WARNING: High class imbalance detected\")\\n",
    "        logger.warning(f\"High class imbalance: {imbalance_ratio:.1f}:1\")\\n",
    "    else:\\n",
    "        print(f\"   \u2705 Reasonable class balance\")\\n",
    "    \\n",
    "    logger.info(f\"Class balance ratio: {imbalance_ratio:.1f}:1\")\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Train YOLOv8 Model\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "Following autodistill best practices, we use the YOLOv8 target model for deployment-ready inference.\n",
    "\n",
    "### Security Context\n",
    "\n",
    "We use PyTorch's `safe_globals` context manager to securely load model checkpoints, protecting against arbitrary code execution vulnerabilities.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "- Load YOLOv8 nano model (optimized for small datasets)\n",
    "- Train on auto-labeled dataset\n",
    "- Monitor training progress\n",
    "- Save best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCEED_TO_TRAINING:\n",
    "    from autodistill_yolov8 import YOLOv8\n",
    "    import torch\n",
    "    import locale\n",
    "    import time\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"\ud83c\udfcb\ufe0f  TRAINING YOLOV8 MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Set locale for proper encoding (prevents some training warnings)\n",
    "    locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "    \n",
    "    TRAIN_DATASET_PATH = os.path.join(DATASET_OUTPUT_PATH, \"data.yaml\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccb Training Configuration:\")\n",
    "    print(f\"   \u2022 Dataset: {TRAIN_DATASET_PATH}\")\n",
    "    print(f\"   \u2022 Model: {YOLO_MODEL_SIZE}\")\n",
    "    print(f\"   \u2022 Epochs: {TRAINING_EPOCHS}\")\n",
    "    print(f\"   \u2022 Output: {TRAINING_OUTPUT_PATH}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # INITIALIZE YOLOV8 MODEL WITH SECURITY CONTEXT\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"\ud83c\udfd7\ufe0f  INITIALIZING YOLOV8 MODEL\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Import required model components for safe loading\n",
    "    from ultralytics.nn.modules import (\n",
    "        C2f, Detect, Bottleneck, Conv, ConvTranspose, DFL\n",
    "    )\n",
    "    \n",
    "    SAFE_GLOBALS = [\n",
    "        C2f, Detect, Bottleneck, Conv, ConvTranspose, DFL,\n",
    "        torch.nn.ModuleList\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Use security context for safe model loading\n",
    "        with torch.serialization.safe_globals(SAFE_GLOBALS):\n",
    "            target_model = YOLOv8(YOLO_MODEL_SIZE)\n",
    "        \n",
    "        print(f\"\u2705 YOLOv8 model initialized successfully\")\n",
    "        print(f\"   \u2022 Architecture: {YOLO_MODEL_SIZE}\")\n",
    "        print(f\"   \u2022 Secure loading: Enabled\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"\u274c FATAL ERROR: Failed to initialize YOLOv8 model\\n\"\n",
    "            f\"   Error: {str(e)}\\n\"\n",
    "            f\"   Please ensure YOLOv8 is installed correctly.\"\n",
    "        )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # START TRAINING\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\ud83d\ude80 STARTING TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n\u2139\ufe0f  This may take several minutes depending on dataset size and hardware.\")\n",
    "    print(\"   Training progress will be displayed below...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        target_model.train(\n",
    "            data_path=TRAIN_DATASET_PATH,\n",
    "            epochs=TRAINING_EPOCHS,\n",
    "            project=TRAINING_OUTPUT_PATH\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\u2705 TRAINING COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\u23f1\ufe0f  Total training time: {elapsed_time/60:.2f} minutes\")\n",
    "        print(f\"\ud83d\udcbe Model saved to: {TRAINING_OUTPUT_PATH}\")\n",
    "        print(\"\\n\ud83d\udcca Check the training output directory for:\")\n",
    "        print(\"   \u2022 weights/best.pt - Best model checkpoint\")\n",
    "        print(\"   \u2022 weights/last.pt - Last epoch checkpoint\")\n",
    "        print(\"   \u2022 Training curves and metrics\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\u274c TRAINING FAILED\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  \u2022 Insufficient GPU memory\")\n",
    "        print(\"  \u2022 Invalid dataset format\")\n",
    "        print(\"  \u2022 Corrupted data.yaml file\")\n",
    "        print(\"  \u2022 No training images in dataset\")\n",
    "        PROCEED_TO_TRAINING = False\n",
    "        raise\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\u23ed\ufe0f  TRAINING SKIPPED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nReason: Dataset was not approved for training.\")\n",
    "    print(\"To train the model, re-run the quality review cell and approve the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Inference with Trained Model\n",
    "\n",
    "### Model Loading\n",
    "\n",
    "Load the best checkpoint from training for inference on new images.\n",
    "\n",
    "### Inference Process\n",
    "\n",
    "- Load trained model\n",
    "- Run inference on test image\n",
    "- Visualize predictions\n",
    "- Save annotated results\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "- Detection count\n",
    "- Inference time\n",
    "- Confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCEED_TO_TRAINING:\n",
    "    from ultralytics import YOLO\n",
    "    import glob\n",
    "    import os\n",
    "    import cv2\n",
    "    from IPython.display import Image, display\n",
    "    import time\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"\ud83d\udd2e INFERENCE WITH TRAINED MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LOCATE TRAINED MODEL\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"\ud83d\udd0d LOCATING TRAINED MODEL\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Find the latest training run\n",
    "    run_folders = sorted(glob.glob(os.path.join(TRAINING_OUTPUT_PATH, 'train*')))\n",
    "    \n",
    "    if not run_folders:\n",
    "        raise FileNotFoundError(\n",
    "            f\"\u274c FATAL ERROR: No training runs found in {TRAINING_OUTPUT_PATH}\\n\"\n",
    "            f\"   Please ensure training completed successfully.\"\n",
    "        )\n",
    "    \n",
    "    latest_run_folder = run_folders[-1]\n",
    "    TRAINED_MODEL_PATH = os.path.join(latest_run_folder, 'weights/best.pt')\n",
    "    \n",
    "    if not os.path.exists(TRAINED_MODEL_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"\u274c FATAL ERROR: Model checkpoint not found at {TRAINED_MODEL_PATH}\\n\"\n",
    "            f\"   Please verify training completed successfully.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\u2705 Found trained model: {TRAINED_MODEL_PATH}\")\n",
    "    print(f\"   \u2022 Run folder: {os.path.basename(latest_run_folder)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LOAD MODEL\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"\ud83d\udce5 LOADING TRAINED MODEL\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    try:\n",
    "        model = YOLO(TRAINED_MODEL_PATH)\n",
    "        print(\"\u2705 Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"\u274c FATAL ERROR: Failed to load model\\n\"\n",
    "            f\"   Error: {str(e)}\\n\"\n",
    "            f\"   The checkpoint file may be corrupted.\"\n",
    "        )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SELECT TEST IMAGE\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"\ud83d\uddbc\ufe0f  SELECTING TEST IMAGE\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Get all image paths again\n",
    "    test_image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        test_image_paths.extend(glob.glob(os.path.join(UNLABELED_IMAGES_PATH, ext)))\n",
    "    \n",
    "    if not test_image_paths:\n",
    "        print(\"\u26a0\ufe0f  No images found for inference\")\n",
    "    else:\n",
    "        # Use the first image for demonstration\n",
    "        inference_image_path = test_image_paths[0]\n",
    "        print(f\"\ud83d\udcf8 Test image: {os.path.basename(inference_image_path)}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # RUN INFERENCE\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"\ud83d\ude80 RUNNING INFERENCE\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            results = model(inference_image_path)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Get annotated frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            \n",
    "            # Create output directory\n",
    "            os.makedirs(INFERENCE_OUTPUT_PATH, exist_ok=True)\n",
    "            \n",
    "            # Save result\n",
    "            output_filename = f\"inference_result_{os.path.basename(inference_image_path)}\"\n",
    "            output_path = os.path.join(INFERENCE_OUTPUT_PATH, output_filename)\n",
    "            cv2.imwrite(output_path, annotated_frame)\n",
    "            \n",
    "            # Display results\n",
    "            num_detections = len(results[0].boxes)\n",
    "            \n",
    "            print(\"\\n\u2705 Inference complete\")\n",
    "            print(f\"   \u2022 Detections found: {num_detections}\")\n",
    "            print(f\"   \u2022 Inference time: {inference_time:.3f} seconds\")\n",
    "            print(f\"   \u2022 Result saved to: {output_path}\")\n",
    "            \n",
    "            # Show detection details\n",
    "            if num_detections > 0:\n",
    "                print(\"\\n\ud83d\udccb Detection Details:\")\n",
    "                for i, box in enumerate(results[0].boxes[:10], 1):  # Show first 10\n",
    "                    class_id = int(box.cls[0])\n",
    "                    confidence = float(box.conf[0])\n",
    "                    class_name = model.names[class_id]\n",
    "                    print(f\"   {i:2d}. {class_name:<30} (confidence: {confidence:.3f})\")\n",
    "                \n",
    "                if num_detections > 10:\n",
    "                    print(f\"   ... and {num_detections - 10} more detections\")\n",
    "            \n",
    "            # Display image in notebook\n",
    "            print(\"\\n\ud83d\uddbc\ufe0f  Displaying annotated result...\\n\")\n",
    "            display(Image(filename=output_path, width=800))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n\u274c Inference failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\u2705 INFERENCE COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\u23ed\ufe0f  INFERENCE SKIPPED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nReason: Training was not completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Complete! \ud83c\udf89\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook successfully implemented a production-grade auto-labeling pipeline using:\n",
    "\n",
    "1. \u2705 **Grounded-SAM-2** - Latest version with Florence-2 + SAM 2\n",
    "2. \u2705 **Official Best Practices** - Following autodistill documentation\n",
    "3. \u2705 **Quality Assurance** - Visual review and approval gates\n",
    "4. \u2705 **YOLOv8 Training** - Fast, deployment-ready model\n",
    "5. \u2705 **Comprehensive Error Handling** - Robust and production-ready\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Iterate on Parameters**: Adjust BOX_THRESHOLD and TEXT_THRESHOLD for better results\n",
    "- **Expand Dataset**: Add more diverse HVAC blueprint images\n",
    "- **Fine-tune Model**: Train for more epochs or with different architectures\n",
    "- **Deploy Model**: Export to ONNX or TensorRT for production use\n",
    "- **Validate Performance**: Test on held-out validation set\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Autodistill Documentation](https://docs.autodistill.com/)\n",
    "- [Grounded-SAM-2 GitHub](https://github.com/autodistill/autodistill-grounded-sam-2)\n",
    "- [YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [Roboflow Tutorials](https://blog.roboflow.com/label-data-with-grounded-sam-2/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}